
---
title: "ONrates12022023"
author: "Hammond"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
echo: false  # Make sure this line is present and set to false
header-includes:
  - \usepackage{booktabs}
  - \usepackage{float}
---

```{r setup, include=FALSE}
# Load required libraries
library(ggplot2)
library(dplyr)
library(tidyverse)
library(knitr)
library(xtable)
library(tinytex)
library(gridExtra)
library(reshape2)
library(bookdown)
library(zoo)
# Set global knitr options
knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(echo = TRUE)
```

```{r, clear environment, echo=TRUE}
# Clear the environment
#rm(list = ls())
#rmarkdown::render("C:/Users/Owner/Documents/Research/OvernightRates/ONrates03072024v3.Rmd",envir= my_envmp)
#rmarkdown::render("C:/Users/Owner/Documents/Research/OvernightRates/ONrates03142024draft.Rmd",envir= #my_envmp)
```
<!--
Last chat on rmd error
It seems like the error persists even after resaving the environment file. This indicates that the issue might be related to the code itself or how it interacts with the data/environment.

Here are a few additional suggestions to help you tackle the problem:

Review the code: Take a close look at lines 1778-1790 of your R Markdown document (ONrates03142024draft.Rmd). This section likely contains the code that's causing the error. Check for any operations involving the sdate column or any other data frame manipulation that might be causing the issue.

Debugging: Consider adding print statements or using a debugger to trace the execution of your code and identify the specific line or operation that triggers the error. This can help you narrow down the problem and identify potential solutions.

Verify the data: Double-check that the data loaded from the environment file (my_environmentmp.RDS) is correct and complete. Ensure that the sdate column and any other relevant columns are present and formatted as expected.

Check for dependencies: Make sure that all required packages and dependencies are loaded and available in your R Markdown document. Sometimes errors can occur due to missing or incompatible packages.

Try running the code in chunks: If your R Markdown document contains multiple code chunks, try running them individually to isolate the source of the error. This can help you identify which part of the code is causing the issue.

-->
#Load the environment from the RDS file
```{r, load environment mp, echo=FALSE}
my_envmp <- readRDS("C:/Users/Owner/Documents/Research/OvernightRates/my_environmentmp.RDS")
#saveRDS(my_envmp, file = "C:/Users/Owner/Documents/Research/OvernightRates/my_environmentmp.RDS")
#  rmarkdown::render("C:/Users/Owner/Documents/Research/OvernightRates/ONrates03142024draft.Rmd",envir= my_envmp)
```


```{r, load environment episodes, echo=FALSE, cache=FALSE}
my_envepisodes <- readRDS("C:/Users/Owner/Documents/Research/OvernightRates/my_envepisodes.RDS")
# from directory
#saveRDS(my_envepisodes, file = "C:/Users/Owner/Documents/Research/OvernightRates/my_envepisodes.RDS")
```
<!--
#```{r, cache = FALSE}
# Your code here
or here for a specific rds
#```{r, load_environment_episodes, echo=FALSE, cache=FALSE}
Still rmd file cant find my_envepisodes$sampleE:

Quitting from lines 1866-1922 [sampletable_characteristics] (ONrates03212024draft_new.Rmd)
Error:
! object 'sampleE' not found
Backtrace:
 1. utils::str(sampleE)
Execution halted

Given this information, it seems that the issue may not be with accessing sampleE itself, as it appears to be present and properly structured within the my_envepisodes environment.

The error message "! object 'sampleE' not found" suggests that there might be a problem elsewhere in your R Markdown document, possibly related to the scope or execution order of the code chunks. Here are a few additional steps you can take to troubleshoot:

Double-Check Chunk Names: Ensure that the chunk names in your R Markdown document match the names referenced in your error message.

Check Chunk Options: Review the chunk options (such as eval, include, etc.) to ensure that the chunk where sampleE is referenced has the appropriate options set to execute the code.

Review Entire Document: Look through your entire R Markdown document to ensure that there are no conflicting or overriding assignments to sampleE that might be causing it to be undefined in certain contexts.
-->

```{r, load environment box, echo=FALSE}
my_environmentbox <- readRDS("C:/Users/Owner/Documents/Research/OvernightRates/my_environmentbox.RDS")
```

'''{r, load environment volatile, echo=FALSE}
my_envvolatile<- readRDS("C:/Users/Owner/Documents/Research/OvernightRates/my_envvolatile.RDS")

```
<!--
https://rstudio.github.io/visual-markdown-editing/technical.html#:~:text=In%20raw%20markdown%2C%20you%20would,to%20escape%20the%20%40%20character).


Structure of article: IMRAD:
Introduction
Data
Methodology
Results and
Discussion

Conclusion
Acknowledgements
References
Supporting Materials

Discussing figures in a scientific paper is an important aspect of effectively communicating your research findings. Figures, such as graphs, charts, and images, help illustrate your points and make your paper more accessible to readers. Here's a step-by-step guide on how to discuss figures in a scientific paper:
1. Introduction:
Start by introducing the figure and its purpose. Briefly explain what the figure represents and why it's important in the context of your study.
2. Figure Description:
Provide a clear and concise description of the figure. Include relevant details about the data, axes, labels, units, and any key components.
3. Interpretation:
Discuss the main findings or trends depicted in the figure. Highlight significant patterns, relationships, or differences. Use specific data points to support your interpretations.
4. Connection to Hypotheses or Research Questions:
Relate the figure's content to the research questions or hypotheses you're addressing in your study. Explain how the figure's information contributes to answering these questions or testing these hypotheses.
5. Comparison with Previous Studies:
If applicable, compare your findings in the figure with results from previous studies. Highlight similarities or differences and discuss potential reasons for these discrepancies.
6. Limitations:
Address any limitations or uncertainties associated with the figure. Discuss potential sources of error, assumptions made, or aspects that could impact the interpretation of the data.
7. Implications:
Discuss the broader implications of the figure's findings. How do the results depicted in the figure contribute to the field's understanding of the topic? How might these findings impact theory, practice, or future research?
8. Integration with Text:
Make sure to integrate your discussion of the figure smoothly within the overall flow of your paper. Refer to the figure by its number and provide context as needed.
9. Clarity and Precision:
Use clear and precise language. Avoid jargon that might confuse readers unfamiliar with your specific field.
10. Visual Aids:
As you discuss the figure, consider referring to specific elements within the figure to guide the reader's understanding. For example, you can mention specific data points, lines, or sections of the graph.
11. Use of Citations:
If you're comparing your figure's results with those from other studies, cite those studies appropriately. This adds credibility to your discussion and allows readers to explore the referenced work.
12. Repetition and Synthesis:
Ensure that the discussion of the figure complements the narrative you've established in the rest of the paper. Avoid repeating information but rather synthesize the figure's insights into the overall story you're telling.
Remember that your goal is to help readers understand the figure's content, its significance, and its implications in the context of your research. By following these steps and maintaining a clear, organized writing style, you can effectively discuss figures in your scientific paper.
-->


```{r, main data spread, echo=FALSE}
#Access the data frame stored in the environment
my_data <- my_envmp$spread_no_na
str(my_data)
``` 

```{r, sdate,echo=FALSE}
sdate<-as.Date(my_data$Date,"%m/%d/%Y"); 
```

```{r, select sample,echo=FALSE}
begs <-which(sdate == as.Date("2016-03-04"))[1] # sample start date 3/04/2016
ends <-which(sdate == as.Date("2023-12-14"))[1] # sample end  date  12/27/2022
print(begs)
print(ends)
my_data=my_data[begs:ends,]
spread_no_na=my_data[begs:ends,]
```

<!--
quantiles are in my_envepisodes   str(my_envepisodes$quantilesE)

```{r, daily quantile dataframes, echo=FALSE,  cache=FALSE,results='hide'}
quantiles<-as.data.frame(as.list(my_data))
quantilesE <- quantiles[c("EFFR", "VolumeEFFR", "TargetUe", "TargetDe", "Percentile01_EFFR", "Percentile25_EFFR", "Percentile75_EFFR", "Percentile99_EFFR","sdate")]
quantilesO <- quantiles[c("sdate", "OBFR", "VolumeOBFR", "Percentile01_OBFR", "Percentile25_OBFR", "Percentile75_OBFR", "Percentile99_OBFR")]
quantilesT <- quantiles[c("sdate", "TGCR", "VolumeTGCR", "Percentile01_TGCR", "Percentile25_TGCR", "Percentile75_TGCR", "Percentile99_TGCR")]
quantilesB <- quantiles[c("sdate", "BGCR", "VolumeBGCR", "Percentile01_BGCR", "Percentile25_BGCR", "Percentile75_BGCR", "Percentile99_BGCR")]
quantilesS <- quantiles[c("sdate", "SOFR", "VolumeSOFR", "Percentile01_SOFR", "Percentile25_SOFR", "Percentile75_SOFR", "Percentile99_SOFR")]
```
-->

```{r, Load rate data frame rrbp, echo=FALSE}
rrbp <- data.frame(
  subset(my_data, select = c("EFFR", "OBFR","TGCR", "BGCR", "SOFR")
))
```


```{r, Load volume data frame vold, echo=FALSE}
vold <- data.frame(
  subset(my_data, select = c("VolumeEFFR", "VolumeTGCR","VolumeBGCR","VolumeSOFR"))
)
```

<!--
  Yes, in the code snippet you provided, you're creating a data frame `rrbp` by subsetting columns from  another data frame `my_data`. This approach is perfectly valid if you only need a subset of columns for your analysis, and it can be a convenient way to work with specific variables.
 
 However, keep in mind that `rrbp` is a new data frame, and any changes or manipulations you perform on it won't affect the original `my_data` data frame. If you intend to modify or analyze the subset of columns, it's a suitable approach.
-->



Make sure to replace `"EFFR", "OBFR", "TGCR", "BGCR", "SOFR"` with the actual column names you want to include in `rrbp`.

```{r,initialize list the lists measure_list1 and measurelist_2 to store matrices, echo=FALSE}
# Define parameters
nmat <- 6 # number of matrices
ncl <- 5# number of columns
begn<- c(1, 859, 923,  1014, 1519, 1)
endn<- c(858, 922, 1013, 1518, 1957, 1957)
sdate[endn] # "2019-07-31" "2019-10-31" "2020-03-16" "2022-03-16" "2023-12-14" "2023-12-14"
sdate[begn] # "2016-03-04" "2019-08-01" "2019-11-01" "2020-03-17" "2022-03-17" "2016-03-04"

# Start dates
start_dates <- sdate[begn]
start_dates
# End dates
end_dates <- sdate[endn]
end_dates

# Define your plot
#plot(x, y, main = "My Plot", xlab = "X-axis", ylab = "Y-axis")

# Access start dates
start_dates <- sdate[begn]

# Convert dates to strings
start_dates_strings <- as.character(start_dates)

# Add a label with the first start date to the plot
#text(x = x_coordinate, y = y_coordinate, labels = paste("Start Date:", start_dates_strings[1]), pos = 1)
# Repeat the above steps for the other start dates

# Access end dates
end_dates <- sdate[endn]

# Convert dates to strings
end_dates_strings <- as.character(end_dates)

# Add a label with the first end date to the plot
#text(x = x_coordinate, y = y_coordinate, labels = paste("End Date:", end_dates_strings[1]), pos = 3)
# Add a label with the second end date to the plot
#text(x = x_coordinate, y = y_coordinate, labels = paste("End Date:", end_dates_strings[2]), pos = 3)
# Repeat the above steps for the other end dates
# Display the plot


# Initialize lists
measure_list1 <- vector("list", length = nmat)
measure_list2 <- vector("list", length = nmat)
```


```{r, populate list measure_list1[[1]] <- norm_measure1 with matrices based on the provided calculations, echo=FALSE}
nmat <- 6 # number of matrices
ncl <- 5 # number of columns
begn <- c(1, 859, 923, 1014, 1519, 1)
endn <- c(858, 922, 1013, 1518, 1957, 1957)

k<-1
#for (k in 1:nmat) {
  bgn <- begn[k]
  edn <- endn[k]
  
  if (bgn >= 1 & bgn < edn) {
    rows_in_range <- (bgn + 1):edn
    #print(paste("Rows in range for k =", k, ":", rows_in_range))
    
    # Debugging: Print dimensions of subsets
    #print(dim(rrbp[rows_in_range, 1:ncl]))
    #print(dim(rrbp[bgn:(edn - 1), 1:ncl]))
    
    # Calculate measures for each k
     norm_measure1 <- log(rrbp[rows_in_range, 1:ncl]) - log(rrbp[bgn:(edn - 1), 1:ncl])
    # norm_measure2 <- abs(rrbp[rows_in_range, 1:ncl] - rrbp[bgn:(edn - 1), 1:ncl])
    norm_measure1[is.na( norm_measure1)] <- 0
    # Assign matrices to the lists
    measure_list1[[k]] <- norm_measure1
    #measure_list2[[k]] <- norm_measure2
  } else {
    print(paste("Invalid indices for k =", k))
}
```


```{r, populate list measure_list1[[2]] <- adjust_measure1 with matrices based on the provided calculations, echo=FALSE}
nmat <- 6 # number of matrices
ncl <- 5 # number of columns
begn <- c(1, 859, 923, 1014, 1519, 1)
endn <- c(858, 922, 1013, 1518, 1957, 1957)

k<-2
#for (k in 1:nmat) {
  bgn <- begn[k]
  edn <- endn[k]
  
  if (bgn >= 1 & bgn < edn) {
    rows_in_range <- (bgn + 1):edn
    #print(paste("Rows in range for k =", k, ":", rows_in_range))
    
    # Debugging: Print dimensions of subsets
    #print(dim(rrbp[rows_in_range, 1:ncl]))
    #print(dim(rrbp[bgn:(edn - 1), 1:ncl]))
    
    # Calculate measures for each k
     adjust_measure1 <- log(rrbp[rows_in_range, 1:ncl]) - log(rrbp[bgn:(edn - 1), 1:ncl])
    # adjust_measure2 <- abs(rrbp[rows_in_range, 1:ncl] - rrbp[bgn:(edn - 1), 1:ncl])
    adjust_measure1[is.na( adjust_measure1)] <- 0
    # Assign matrices to the lists
    measure_list1[[k]] <- adjust_measure1
    #measure_list2[[k]] <- adjust_measure2
  } else {
    print(paste("Invalid indices for k =", k))
}
```



```{r, populate list measure_list1[[3]] <- covid_measure1 with matrices based on the provided calculations, echo=FALSE}
nmat <- 6 # number of matrices
ncl <- 5 # number of columns
begn <- c(1, 859, 923, 1014, 1519, 1)
endn <- c(858, 922, 1013, 1518, 1957, 1957)

k<-3
#for (k in 1:nmat) {
  bgn <- begn[k]
  edn <- endn[k]
  
  if (bgn >= 1 & bgn < edn) {
    rows_in_range <- (bgn + 1):edn
    #print(paste("Rows in range for k =", k, ":", rows_in_range))
    
    # Debugging: Print dimensions of subsets
    #print(dim(rrbp[rows_in_range, 1:ncl]))
    #print(dim(rrbp[bgn:(edn - 1), 1:ncl]))
    
    # Calculate measures for each k
     covid_measure1 <- log(rrbp[rows_in_range, 1:ncl]) - log(rrbp[bgn:(edn - 1), 1:ncl])
    # covid_measure2 <- abs(rrbp[rows_in_range, 1:ncl] - rrbp[bgn:(edn - 1), 1:ncl])
    covid_measure1[is.na( covid_measure1)] <- 0
    # Assign matrices to the lists
    measure_list1[[k]] <- covid_measure1
    #measure_list2[[k]] <- covid_measure2
  } else {
    print(paste("Invalid indices for k =", k))
}
```



```{r, populate list measure_list1[[4]] <- zlb_measure1 with matrices based on the provided calculations, echo=FALSE}
nmat <- 6 # number of matrices
ncl <- 5 # number of columns
begn <- c(1, 859, 923, 1014, 1519, 1)
endn <- c(858, 922, 1013, 1518, 1957, 1957)

k<-4
#for (k in 1:nmat) {
  bgn <- begn[k]
  edn <- endn[k]
  
  if (bgn >= 1 & bgn < edn) {
    rows_in_range <- (bgn + 1):edn
    #print(paste("Rows in range for k =", k, ":", rows_in_range))
    
    # Debugging: Print dimensions of subsets
    #print(dim(rrbp[rows_in_range, 1:ncl]))
    #print(dim(rrbp[bgn:(edn - 1), 1:ncl]))
    
    # Calculate measures for each k
     zlb_measure1 <- log(rrbp[rows_in_range, 1:ncl]) - log(rrbp[bgn:(edn - 1), 1:ncl])
    # zlb_measure2 <- abs(rrbp[rows_in_range, 1:ncl] - rrbp[bgn:(edn - 1), 1:ncl])
    zlb_measure1[is.na( zlb_measure1)] <- 0
    # Assign matrices to the lists
    measure_list1[[k]] <- zlb_measure1
    #measure_list2[[k]] <- zlb_measure2
  } else {
    print(paste("Invalid indices for k =", k))
}
```



```{r, populate list measure_list1[[5]] <- inflation_measure1 with matrices based on the provided calculations, echo=FALSE}
nmat <- 6 # number of matrices
ncl <- 5 # number of columns
begn <- c(1, 859, 923, 1014, 1519, 1)
endn <- c(858, 922, 1013, 1518, 1957, 1957)

k<-5
#for (k in 1:nmat) {
  bgn <- begn[k]
  edn <- endn[k]
  
  if (bgn >= 1 & bgn < edn) {
    rows_in_range <- (bgn + 1):edn
    #print(paste("Rows in range for k =", k, ":", rows_in_range))
    
    # Debugging: Print dimensions of subsets
    #print(dim(rrbp[rows_in_range, 1:ncl]))
    #print(dim(rrbp[bgn:(edn - 1), 1:ncl]))
    
    # Calculate measures for each k
     inflation_measure1 <- log(rrbp[rows_in_range, 1:ncl]) - log(rrbp[bgn:(edn - 1), 1:ncl])
    # inflation_measure2 <- abs(rrbp[rows_in_range, 1:ncl] - rrbp[bgn:(edn - 1), 1:ncl])
    inflation_measure1[is.na( inflation_measure1)] <- 0
    # Assign matrices to the lists
    measure_list1[[k]] <- inflation_measure1
    #measure_list2[[k]] <- inflation_measure2
  } else {
    print(paste("Invalid indices for k =", k))
}
```



```{r, populate list measure_list1[[6]] <- sample_measure1 with matrices based on the provided calculations, echo=FALSE}
nmat <- 6 # number of matrices
ncl <- 5 # number of columns
begn <- c(1, 859, 923, 1014, 1519, 1)
endn <- c(858, 922, 1013, 1518, 1957, 1957)

k<-6
#for (k in 1:nmat) {
  bgn <- begn[k]
  edn <- endn[k]
  
  if (bgn >= 1 & bgn < edn) {
    rows_in_range <- (bgn + 1):edn
    #print(paste("Rows in range for k =", k, ":", rows_in_range))
    
    # Debugging: Print dimensions of subsets
    #print(dim(rrbp[rows_in_range, 1:ncl]))
    #print(dim(rrbp[bgn:(edn - 1), 1:ncl]))
    
    # Calculate measures for each k
     sample_measure1 <- log(rrbp[rows_in_range, 1:ncl]) - log(rrbp[bgn:(edn - 1), 1:ncl])
    # sample_measure2 <- abs(rrbp[rows_in_range, 1:ncl] - rrbp[bgn:(edn - 1), 1:ncl])
    sample_measure1[is.na( sample_measure1)] <- 0
    # Assign matrices to the lists
    measure_list1[[k]] <- sample_measure1
    #measure_list2[[k]] <- sample_measure2
  } else {
    print(paste("Invalid indices for k =", k))
}
```





# Introduction
How the Federal Reserve Bank (Fed) adjustment of reserves affects volatility of the Federal Funds rate (FFR) and wholesale overnight money market rates. What determines the Fed preferences  for offset volatility in these rates in managing the FFR within the FOMC target range. The The FFR is interest rate at which depository institutions or ”banks” lend funds to each other on an overnight basis. 
Daily overnight rates include the unsecured rates The effective federal funds rate (EFFR), The Federal funds market consists of domestic unsecured borrowings by depository institutions from other depository institutions and certain other entities, primarily government-sponsored enterprises.
The EFFR is calculated as a volume-weighted median of overnight federal funds transactions. The OBFR is a measure of wholesale, unsecured, overnight bank funding costs, calculated from federal funds transactions, Eurodollar transactions, and domestic deposit transactions. The federal funds rate is the Fed's policy rate. The effective federal funds rate (EFFR) tracks transactions in the federal funds market. The overnight  bank funding rate (OBFR), similar to the EFFR, is a broad measure of U.S. dollar funding costs for U.S. based banks. The secured overnight funding rate (SOFR) captures transactions in  overnight wholesale funding markets, the tri-party general collateral rate (TGCR) and over the counter broad general collateral rate (BGCR). 

[INSERT CONCODA REPO MARKET NOTES]
n the 1950s, Wall Street dealers originated the non deposit deposit, a repurchase transaction or repo, secured borrowing backed by collateral.The borrower issues a cash equivalent liability to the lender. In a reverse repo an entity lends cash against secured collateral. Since the 1970s, private parties have created deposit like instruments,  repos and money market funds ,intermediating credit outside the traditional banking system. This allows them to evade regulatory hurdles. The Tri-Party General Collateral Rate (TGCR), the base layer of repos, is a measure of rates on overnight, specific-counterparty tri-party general collateral repurchase agreement (repo) transactions secured by Treasury securities. 
Repo markets trade $4 trillions of dollars daily. Participants in  this tri repo market include primary dealers, broker-dealers, [get figure, note from graph]. Primary dealers intermediate US Treasury issuance to their clients by borrowing cash overnight from the Federal reserve, secured by Treasuries as collateral, then repurchasing the next day. In the 1990s, Section 13 (3) of the Federal Reserve act was amended to backstop dealers. Lenders include asset managers, money market  funds, hedge funds, and security lenders including banks

Transactions in the tri party market TGCR is centrally cleared. The BGCR is an over the counter  transactions, serving agents looking for specific cusip numbers.
The SOFR,  a broad measure of the cost of borrowing cash overnight collateralized by Treasury securities. replaces LIBOR as a benchmark overnight rate. The SOFR includes all trades in the Broad General Collateral market plus bilateral Treasury repurchase agreement (repo) transactions cleared through the Delivery-versus-Payment (DVP) service offered by the Fixed Income Clearing Corporation (FICC). The FUUCA trades are which filtered to remove a portion of transactions considered “specials”. 


Through repo and reverse repo transactons, the Fed uses the tri party market TGCR for temporary interventions to manage the FFR within its target range by using repos to adding reserves to the system or reverse repos to drain reserves.   (David Gibbs, Education Director Chicago Mercantile Exchange (CME). 

The level of reserves in the banking system can change either because
1) because funds are transferred between reserves and non-reserve accounts at the Federal
2) changes in the US Treasury account at the Fed
3) The Federal Reserve responds to volatility in the federal funds market by adjusting the reserve
supply to keep the federal funds rate within its target range through these repo and reverse repo operations each day

By trading securities with banks and other counterparites, ederal Reserve purchases or sales of assets from banks change the size of the Federal Reserve balance sheet. In a  repo transaction, Federal Reserve purchases securities, adding assets to its balance sheet and issues reserves by crediting the Federal Reserve accounts of the selling banks. Fed is lends reserves in a reverse repo transaction. When the Federal Reserve lends to counter parties  the Fed sale of securities lower aggregate reserves as funds are debited from depositor accounts. Fed lending constitutes a large volume of lending to RRP counterparties MMFs, liquidity.

[repo increases reserves, lowers FFR  reverse repo reverses reserves and raises FFR] [repo means dealer deposits (borrowed) reserves collateralized by UST securities?]. 


<! 
Put in rates section
The distributions of daily rates rates and transactions transactions, billions of dollars, change dramatically from year to year.[figure times series of rates] (Figure \ref{fig:volumesample1})Data provided by the NY Fed data download program summarize, weighted average median rates, summarize other quantiles of these  distributions (Figure \ref{fig:samplechar}).

There are also major difference within FOMC policy regimes (Table \ref{table:FOMCdailyratesstats} ). Here volatility  as measured by the annual standard deviation of the series.
-->

Reserves went through a full expansion-contraction cycle from 2010 to late 2019 and expanded again in early 2020, ranging from 8$\%$ (2010 and 2019) to 19$\%$ (2014 and 2021) of banks’ assets. These movements reflect the Federal Reserve balance-sheet expansions in response to the 2008 and 2020 crises, as well as the interim normalization period (2015-2019). 

To manage the build up in liquidity in reserves after the Great financial Crisis (GFC), the Fed initiated the policy regime of ample reserves to  maintain/manage the Fed Funds rate within its target range. Its administrative rates comprise a floor system where  interest on reserves (IOR) paid on bank reserves with the Fed are the ceiling and the overnight reverse repurchase rate (ONRRP rate) is the  floor. The Fed or FOMC also temporarily intervenes in repurchase (repo) and reverse repo to offset high frequency liquidity shocks to keep federal funds rate close to target.?stabilize FF around target rate..

<!--
Relationship to other work-----------------------------------------
Hamilton
Bertolini, Bertola, Prati ()
Gara
Benzoni
Cochrane and Piazzesi
Duffie Krishnamurthy


Ideas:
rate volatility
mean reversion or not, mean reversion around central tendency
rates quickly revert to new FF rate ann
rate dispersion as how well MP stance is communicated to economy
repo and volatility: PRTs basis trade
day effects
Fed preference for managing FFR with target, preference for offsetting volatlity
fat tails, skew, outliers
Time varying policy rules, how do innovations in rates or reserves behave
Rates vs reserve plots, a demand curve
Determine a regime change
-->
 
Work in this area provide evidence of key characteristics of the FFR - time varying volatility or autoregressive conditional heteroskedasticity, spectacular outliers, skew, fat tails. shifts around central tendencies, jumps.

Hamilton ------------------------------------------------
The importance of the overnight federal funds rate, the shortest-term actively traded security- the base of any term structure, is its centralilty  to finance and monetary policy. Hamilton (1996) examine the volatility in the Federal Funds market. adapting Nelson's (1991) EGARCH model to estimate the conditional mean and the conditional variance of the daily Federal Funds rate (FFR). He attributes the effect of an open-market purchase to smooth out small fluctuations in the FFR, rather than interday arbitration . A small temporary infusion of reserves through an overnight repurchase agreement by the Fed's trading desk lowers the federal funds rate by inducing movement along a schedule that represents lending banks' liquidity benefit from holding excess reserves Importannt day effect can produce theextreme volatility, large outliers of the FFR. These include the last day of the quarter or the last day of the year, when bank's report balance sheets of assets and liabilities. 

<!--
Transaction costs, rather minor institutional market detail, are central to the liquidity effect that enables the Federal Reserve to change the interest rate on .a daily basis.
-->

<!-- 
Monika Piazzesi -----------------------------------------
“Bond Yields and the Federal Reserve”, Journal of Political Economy, Volume 113, Issue 2, April 2005, pp. 311-344.  Additional results in the earlier 2001 working paper version: “An Econometric Model of the Yield Curve with Macroeconomic Jump Effects”, NBER Working paper no 8246: theoretical results for deterministic jump times and state-dependent jump size distributions, ; empirical results with 
-->

Piazzesi (2005) constructs a continuous-time model of the joint distribution of bond yields and the interest rate target set by the FOMC. the Fed's estimated policy rule reacts to information contained in the yield curve, especially  information in yields with of two year maturities, implying the Fed responds to some medium-run forecast of the economy. Both Federal Reserve and financial markets watch and depend on bond yields. Yield-based information may underlie the FOMC’s policy decisions and describes Fed policy better than Taylor rules.

high-frequency data in a linear-quadratic jump-diffusion model provides information about the exact timing of FOMC meetings. This can improve bond pricing and ability to identify monetary policy shocks. Decisions about target moves are result in a series of target values that looks like a pure jump process. Estimates reveals increased volatility of interest rates at all maturities in both FOMC meeting days and releases of key macroeconomic data. Macro news releases change the conditional distribution of a future Fed move. 
 
The short informational lag before Fed's policy decision, information available right before the FOMC meeting start provides a recursive identification scheme that turns the target forecast from right before the Federal Open Market Committee (FOMC) meeting into a high-frequency policy rule and the associated forecast errors into policy shocks. based on yield curve information and an arbitrage-free bond market.
 
Findings:
- 1) latent factors
- the target set by the Fed is an observable factor in the model and provides a clean measure of the short end of the yield curve. The use of target data avoids having to deal with calendar day effects in very short rates, which typically require lots of parameters
- 2) Second, the estimated response of yields to policy shocks is strong and slowly declines only with the maturity of the yield
- 3) The estimated policy rule describes the Fed as reacting to information contained in the yield curve. the most important information is contained in yields with maturities around two years, which suggests that the Fed reacts to some medium-run forecast of the economy. 

- The estimated policy rule displays interest rate smoothing: the target level is autocorrelated. 
- The rule also displays policy inertia: the Fed only partially adjusts the target to its desired rate. Inertia leads to positive autocorrelation in target changes, because one change is typically followed by additional changes in the same direction over a number of FOMC meetings

- yield data summarize market expectations of future target moves. These market expectations are
based on a host of variables that are omitted from other rules. 
yield data are available at higher frequencies and are less affected by
measurement errors than macroeconomic variables. 
The model demonstrated the policy inertia, the tendency to continue same policy changes. 

<!--
The “snake shaped ”volatility of yields:Volatility is high for very short maturities (the head of the snake), rapidly decreases until maturities of around three months (the neck of the snake), then increases until maturities of up to two years (the back of the snake), and finally decreases again (its tail)....explains with policy inertia, tendency to continue same policy changes

- impose no arbitrage and respects the timing of FOMC meetings. Decisions about target moves are made at points in time, resulting in a series of target values that looks like a pure jump process.
- The arrival intensity of target jumps depends on the FOMC meeting calendar and the state of the economy.
- estimation methods that exploit data on the entire cross section of yields as opposed to a single
short rate. Longer yields have the statistical advantage of providing important additional observations, especially in the context of rare policy events. Long yields also have an economic advantage, because they turn out to be inputs in the Fed’s policy rule—its systematic response to the state of the economy
- policy decision based on information available right before the FOMC starts its meeting. This
short informational lag provides a recursive identification scheme. The scheme turns the target forecast from right before the FOMC meeting into a high-frequency policy rule and the associated forecast errors into policy shocks.
- method of simulated maximum likelihood (Pedersen 1995; Santa-Clara 1995) extended
to jumps

- 1) latent factors
- the target set by the Fed is an observable factor in the model and provides a clean measure of the short end of the yield curve. The use of target data avoids having to deal with calendar day effects in very short rates, which typically require lots of parameters
- 2) Second, the estimated response of yields to policy shocks is strong and slowly declines only with the maturity of the yield
- 3) The estimated policy rule describes the Fed as reacting to information contained in the yield curve. the most important information is contained in yields with maturities around two years, which suggests that the Fed reacts to some medium-run forecast of the economy. 

- The estimated policy rule displays interest rate smoothing: the target level is autocorrelated. 
- The rule also displays policy inertia: the Fed only partially adjusts the target to its desired rate. Inertia leads to positive autocorrelation in target changes, because one change is typically followed by additional changes in the same direction over a number of FOMC meetings

- yield data summarize market expectations of future target moves. These market expectations are
based on a host of variables that are omitted from other rules. 
yield data are available at higher frequencies and are less affected by
measurement errors than macroeconomic variables.
- 4) snake shape of the volatility curve, the standard
deviation of yield changes as a function of maturity. Volatility is high
for very short maturities (the head of the snake), rapidly decreases until
maturities of around three months (the neck of the snake), then increases until maturities of up to two years (the back of the snake), and finally decreases again (its tail).
-->
 
 ISSUES
 Bertolini, Bertola, Prati () and Gara () and Benzoni observe that the effect of Fed interventions on the volatility in interbank rates 
\begin{itemize}
\item declines in high rate regimes
\item rises end of quarter, end of year
\item falls before holidays, rises day after
\item other observations:
TS properties.  Many rate changes of half percent or more (annualized), and outliers.  Volatility persistent. maintenace periods, calendar days, end of quarter, end of year, before and after holidays, large rate changes
\ Mean reverision versus dispersion?
\end{itemize}
Match to authors' observations
Summary of findings: Heteroskedascity, outliers, volatility, fat tails, skew
1. All. Spectacular outliers and autoregressive conditional heteroskedasticity (ARCH) effects
2. Piazzesi. “snake shaped ”volatility of yields:Volatility is high
for very short maturities (the head of the snake), rapidly decreases until
maturities of around three months (the neck of the snake), then increases until maturities of up to two years (the back of the snake), and finally decreases again (its tail).
3. Policy inertia (P)
4. 2 year UST yield economy medium terms, longer yields future economy (P)
5. Day effects, end of qtr, end of year (H) P says irrelevant if target is a state variable
6. Hamilton (1996) and others have concluded that the hypothesis that the FFR follows a martingale is inconsistent with observed daily changes in the federal funds rate
7. Benzoni identify mean reversion around a central tendency. The stochastic mean  allows a relatively fast mean-reversion of the short rate around a highly persistent time-varying central tendency process. Jumps are integral to the quality of fit and relieve the stochastic volatility factor from accommodating extreme outlier behavior. 
8. The mean drift may be indicative of slowly evolving inflationary expectations (Gara horiz?), time-variation in the required real interest rate, or both. (B)
9. Afonso. The slope of the demand function is negative as reserves decline. The sensitivity of rates to reserve shocks, becomes increasingly steeper reflecting the The slope is flat when reserves are abundant. increasing scarcity value of reserves. Reserve demand, realized rates against realized reserves, a nonlinear, downward-sloping relationship between prices and quantities, shifts outward over time
10. \emph{Vertical shift}
Policy changes shift the curve up and down by moving its lower bound. Increases (descreases) in the (FFR-IORB?) IORB rate shift the demand curve down (up by changing the banks’ opportunity cost of lending in the federal funds market 
11. \emph{Horizontal shift}
Low frequency horizontal shifts in the demand function reflect sensitivity of rates to shocks to reserves.
(1): for every level of the federal funds rate, they imply an increase in the quantity of aggregate
reserves demanded by the banking system. As a result of these shifts, the level of reserves at which
the curve stops being flat and start displaying a negative slope may have moved over time.
These structural changes [which ?] could be interpreted as the horizontal shifts $\ast(q)$ in the demand curve
12. (BBP) Bertolini, Bertola, Prati () focus is the high frequency patterns of FF rate volatility  that survive Fed's attempt to manage the FFR within the FOMC target range. 
13. Rate data provide evidence of Fed's difference preferences for offsetting volatility in the Federal Funds market
14. Quick response of other key short rates to monetary policy and other shocks. Overnight wholesale money market rates quickly revert to changes in the unsecured federal funds rate (FFR) or administered rates. 
15. Their model of FFR volatility seeks to isolate Fed preferences for offsetting volatility in the FFR market.[describe model]



Benzoni et all.-----------------------------
(Andersen, Benzoni,Lund, 2004) model the U.S. short-term interest rate 3 month Tbill with a three-factor jump-diffusion model. The model includes a time-varying mean reversion factor, a stochastic volatility factor, and a jump process.  The U.S. short-term interest rate is characterized by complex conditional heteroskedasticity, fat-tailed innovations, and pronounced autocorrelation patterns.   Stochastic volatility is critical for a good fit. Benzoni et al identify mean reversion of the short rate around a central tendency. The stochastic mean  allows a relatively fast mean-reversion of the short rate around a highly persistent time-varying central tendency process. Jumps are integral to the quality of fit and relieve the stochastic volatility factor from accommodating extreme outlier behavior. 

The mean drift may be indicative of slowly evolving inflationary expectations (Gara horiz?), time-variation in the required real interest rate, or both.


Gara Afonso ----------------------   ----------
Gara et al derive a function of reserve demand that measures banks’ demand for liquidity  as a function of aggregate reserves and the FFR. The FFR is price at which banks are willing to borrow and lend  reserve balances.The reserve demand function describes the price at which banks are willing to trade reserves as a function of the total amount of reserves in the banking system. model of banks' reserve demand sensitivity  to shocks to reserves is greater when reserves are scarce. The slopeof demand function is negative as reserves decline. When reserves are abundant [connect, the demand curve is flat around the the IORB rate. the interest rate paid by the Federal Reserve on reserve balances. The IORB representsthe banks’ opportunity cost of lending in the federal funds market. 

Reserve demand, portrayed in realized rates against realized reserves, is nonlinear, downward-sloping reflecting a negative relation ship between prices and quantities. The slope oof the demand curve, representing the sensitivity of rates to reserve shocks, becomes increasingly steeper the more scarce are reserves. The slope flattens when reserves are abundant. 

\emph{Vertical shift}
Policy changes shift the curve up and down by moving its lower bound. Increases (descreases) in the (FFR-IORB?) IORB rate shift the demand curve down (up by changing the banks’ opportunity cost of lending in the federal funds market 
\emph{Horizontal shift}
Low frequency horizontal shifts in the demand function reflect sensitivity of rates to shocks to reserves.
(1): for every level of the federal funds rate, they imply an increase in the quantity of aggregate
reserves demanded by the banking system. As a result of these shifts, the level of reserves at which
the curve stops being flat and start displaying a negative slope may have moved over time.

The visible negative correlation between quantities (reserves) and prices (federal funds rates) suggests  supply shocks tend to dominate demand shocks. 

? explains ONRRP: The segmentation of the federal funds market between banks and FHLBs suggests that, under our price normalization, the lower bound of the demand function (1), $\ast(p)$ , should range between the ONRRP-IORB spread and zero.

<!--
Gara plot: GaraAmpleReserves2022
\begin{comment}
\begin{figure}[ht]
\includegraphics[scale=1.0\textwidth]{GaraReserves.png}
% \includegraphics[scale=0.9, bb=108   268   473   567]{dailyratessample1brush.tex}
\caption{Reserves normalized by commercial deposits, Federal Fund rate EFFR-IOR spread and reserve demand curve}
\label{fig:Gara}
%\centering
\label{fig:gara}
\end{figure}
\end{comment}


Figure (Panel (b))  to control for changes in FOMC policy, they plot federal funds rate minus the IORB rate versus aggregate reserves. Policy changes shift the curve up and down by moving its lower bound. 
The visible negative correlation between quantities (reserves) and prices (federal funds rates) suggests  supply shocks tend to dominate demand shocks. Reserve demand, realized rates against realized reserves, is a nonlinear, downward-sloping relationship between prices and quantities that moves outward over time
-->


Bertolini, Bertola, Prati () model of FFR volatility seeks to isolate Fed preferences for offsetting volatility in the FFR market.[describe model]. This search was motivated by the puzzle that the high frequency patterns of FF rate volatility survives Fed's attempt to manage the FFR within the FOMC target range. 
They note the immediate response of other key short rates to monetary policy and other shocks. Overnight wholesale money market rates quickly revert to changes in the unsecured federal funds rate (FFR) or administered rates. 

\emph{Duffie, Krishnamurthy}
Krishnamurthy and Vissing Jorgensen find the implicit volatility of overnight reference rates is the results of Fed discretionary policy, quantitative easing (QE), and asset purchases. 
Ample reserves paper?

Krishnamuthy provide evidence that dispersion among secured overnight rates impede passthough of monetary policy to the economy. The  effectiveness of monetary policy to transmit the desired stance of monetary policy to financial markets and the real economy economy depends on efficient transmission of rate changes to other short rates in wholesale and consumer funding markets.





Cochrane and Piazzesi ------------------------------
regress policy shocks on rates

NOTES --------------------------------------------------------------------------------
<!--
Hamilton (1996) and others have concluded that the hypothesis that the FFR follows a martingale is inconsistent with observed daily changes in the federal funds rate. (Bertolini,Bertola, Prati ,) site frictions such as  transactions costs, interbank credit limits, overdraft penalties, obstacles to intra-period arbitrage preclude the FFR from following a martingale process.
-->

Hamilton ------------------------------------------------
Hamilton (1996)  adapts's Nelson's (1991) EGARCH model of daily stock return to the conditional mean and the conditional variance of daily FFR federal funds rates and the effect of an open-market purchase on the distribution of the FFR.  The importance of the overnight federal funds rate, the shortest-term actively traded securityc- the base of any term structure, is its centralilty  to finance and monetary policy. Hamilton, as other authors, recognizes the extreme volatility and the large outliers of short-rate data. Hamilton documents important day effects -the last day of the quarter or the last day of the year, when bank's reported balance sheets of assets and liabilities on .
 
He attributes the OMO rather interday arbitration the ability to smooth out small fluctuations in the FFR. A small temporary infusion of reserves through an overnight repurchase agreement by the Fed's trading desk, anticipated or not, lowers that day's federal funds rate by inducing movement along a schedule that represents lending banks' liquidity benefit from holding excess reserves. [repo increases reserves, lowers FFR reverse repo reverses reserves and raises FFR] [repo means dealer deposits (borrowed) reserves collateralized by UST securities?]. 

Transaction costs, rather minor institutional market detail, are central to the liquidity effect that enables the Federal Reserve to change the interest rate on .a daily basis.
 
Monika Piazzesi -----------------------------------------
“Bond Yields and the Federal Reserve”, Journal of Political Economy, Volume 113, Issue 2, April 2005, pp. 311-344.  Additional results in the earlier 2001 working paper version: “An Econometric Model of the Yield Curve with Macroeconomic Jump Effects”, NBER Working paper no 8246: theoretical results for deterministic jump times and state-dependent jump size distributions, linear-quadratic jump-diffusion model; empirical results with macro news releases that change the conditional distribution of a future Fed move.

Piazzesi (2005) reveals increased volatility of interest rates at all maturities ib both 
FOMC meeting days and around releases of key macroeconomic aggregates in a continuous-time model of the joint distribution of bond yields and the interest rate target set by the FOMC. 

Both Federal Reserve decisions and financial markets watch and depend on bond yields. The estimated policy rule describes the Fed as reacting to information contained in the yield curve. the most important information is contained in yields with maturities around two years, which suggests that the Fed reacts to some medium-run forecast of the economy. The policy rule crucially depends on thetwo-year yield and describes Fed policy better than Taylor rules. This yield-based information may underlie the FOMC’s policy decisions. 

 

The short informational lag before Fed's policy decision based on information available right before the FOMC starts its meeting provides a recursive identification scheme that turns the target forecast from right before the FOMC meeting into a high-frequency policy rule and the associated forecast errors into policy shocks. The Fed extracts information about the state of the economy from the current yield curve. 
The high-frequency policy rule based on yield curve information and an arbitrage-free bond market...

The “snake shaped ”volatility of yields:Volatility is high for very short maturities (the head of the snake), rapidly decreases until maturities of around three months (the neck of the snake), then increases until maturities of up to two years (the back of the snake), and finally decreases again (its tail)....explains with policy inertia, tendency to continue same policy changes

<!--
To learn about these responses,In continuous time, the Fed’s target is a pure jump
process. Jump intensities depend on the state of the economy and
the meeting calendar of the Federal Open Market Committee. The
model has closed-form solutions for yields as functions of a few state
variables. Introducing monetary policy helps to match the whole yield
curve, because the
Policy inertia: tendency to continue same policy changes
Volatility smile
2 year UST yield 

Not only do markets watch the Federal Reserve, but the reverse is also true. At its meetings, the FOMC .

r short maturities, because most studies avoid dealing with the extreme volatility and the large outliers at certain calendar days of short-rate data

- with high-frequency data, use information about the exact timing of FOMC meetings to improve bond pricing and to identify monetary policy shocks. 
- construct a continuous-time model of the joint distribution of bond yields and the interest rate target set by the FOMC.
- impose no arbitrage and respects the timing of FOMC meetings. Decisions about target moves are made at points in time, resulting in a series of target values that looks like a pure jump process.
- The arrival intensity of target jumps depends on the FOMC meeting calendar and the state of the economy.
- estimation methods that exploit data on the entire cross section of yields as opposed to a single
short rate. Longer yields have the statistical advantage of providing important additional observations, especially in the context of rare policy events. Long yields also have an economic advantage, because they turn out to be inputs in the Fed’s policy rule—its systematic response to the state of the economy
- policy decision based on information available right before the FOMC starts its meeting. This
short informational lag provides a recursive identification scheme. The scheme turns the target forecast from right before the FOMC meeting into a high-frequency policy rule and the associated forecast errors into policy shocks.
- method of simulated maximum likelihood (Pedersen 1995; Santa-Clara 1995) extended
to jumps

- 1) latent factors
- the target set by the Fed is an observable factor in the model and provides a clean measure of the short end of the yield curve. The use of target data avoids having to deal with calendar day effects in very short rates, which typically require lots of parameters
- 2) Second, the estimated response of yields to policy shocks is strong and slowly declines only with the maturity of the yield
- 3) The estimated policy rule describes the Fed as reacting to information contained in the yield curve. the most important information is contained in yields with maturities around two years, which suggests that the Fed reacts to some medium-run forecast of the economy. 

- The estimated policy rule displays interest rate smoothing: the target level is autocorrelated. 
- The rule also displays policy inertia: the Fed only partially adjusts the target to its desired rate. Inertia leads to positive autocorrelation in target changes, because one change is typically followed by additional changes in the same direction over a number of FOMC meetings

- yield data summarize market expectations of future target moves. These market expectations are
based on a host of variables that are omitted from other rules. 
yield data are available at higher frequencies and are less affected by
measurement errors than macroeconomic variables.
- 4) snake shape of the volatility curve, the standard
deviation of yield changes as a function of maturity. Volatility is high
for very short maturities (the head of the snake), rapidly decreases until
maturities of around three months (the neck of the snake), then increases until maturities of up to two years (the back of the snake), and finally decreases again (its tail).

Most empirical papers on monetary policy focus on the short-rate process alone. Then infer long rates by discredited expectations hypothesis
-->

Bertolini, Bertola, Prati () and Gara () and Benzoni observe that the effect of Fed interventions on the volatility in interbank rates 
\begin{itemize}
\item declines in high rate regimes
\item rises end of quarter, end of year
\item falls before holidays, rises day after
\item other observations:
TS properties.  Many rate changes of half percent or more (annualized), and outliers.  Volatility persistent. maintenace periods, calendar days, end of quarter, end of year, before and after holidays, large rate changes
\end{itemize}

Summary of findings: Heteroskedascity, outliers, volatility, fat tails, skew
1. All. Spectacular outliers and autoregressive conditional heteroskedasticity (ARCH) effects
2. Piazzesi. “snake shaped ”volatility of yields:Volatility is high
for very short maturities (the head of the snake), rapidly decreases until
maturities of around three months (the neck of the snake), then increases until maturities of up to two years (the back of the snake), and finally decreases again (its tail).
3. Policy inertia (P)
4. 2 year UST yield economy medium terms, longer yields future economy (P)
5. Day effects, end of qtr, end of year (H) P says irrelevant if target is a state variable
6. Hamilton (1996) and others have concluded that the hypothesis that the FFR follows a martingale is inconsistent with observed daily changes in the federal funds rate
7. Benzoni identify mean reversion around a central tendency. The stochastic mean  allows a relatively fast mean-reversion of the short rate around a highly persistent time-varying central tendency process. Jumps are integral to the quality of fit and relieve the stochastic volatility factor from accommodating extreme outlier behavior. 
8. The mean drift may be indicative of slowly evolving inflationary expectations (Gara horiz?), time-variation in the required real interest rate, or both. (B)
9. Afonso. The slope of the demand function is negative as reserves decline. The sensitivity of rates to reserve shocks, becomes increasingly steeper reflecting the The slope is flat when reserves are abundant. increasing scarcity value of reserves. Reserve demand, realized rates against realized reserves, a nonlinear, downward-sloping relationship between prices and quantities, shifts outward over time
10. \emph{Vertical shift}
Policy changes shift the curve up and down by moving its lower bound. Increases (descreases) in the (FFR-IORB?) IORB rate shift the demand curve down (up by changing the banks’ opportunity cost of lending in the federal funds market 
11. \emph{Horizontal shift}
Low frequency horizontal shifts in the demand function reflect sensitivity of rates to shocks to reserves.
(1): for every level of the fe
deral funds rate, they imply an increase in the quantity of aggregate
reserves demanded by the banking system. As a result of these shifts, the level of reserves at which
the curve stops being flat and start displaying a negative slope may have moved over time.
These structural changes [which ?] could be interpreted as the horizontal shifts $\ast(q)$ in the demand curve
12. (BBP) Bertolini, Bertola, Prati () focus is the high frequency patterns of FF rate volatility  that survive Fed's attempt to manage the FFR within the FOMC target range. 
13. Rate data provide evidence of Fed's difference preferences for offsetting volatility in the Federal Funds market
14. Quick response of other key short rates to monetary policy and other shocks. Overnight wholesale money market rates quickly revert to changes in the unsecured federal funds rate (FFR) or administered rates. 
15. Their model of FFR volatility seeks to isolate Fed preferences for offsetting volatility in the FFR market.[describe model]


 
Benzoni et all.-----------------------------
(Andersen, Benzoni,Lund, 2004) model the U.S. short-term interest rate 3 month Tbill with a three-factor jump-diffusion model that includes a time-varying mean reversion factor, a stochastic volatility factor, and a jump process.  The U.S. short-term interest rate is characterized by complex conditional heteroskedasticity, fat-tailed innovations, and pronounced autocorrelation patterns.   Stochastic volatility is critical for a good fit. Benzoni identify mean reversion around a central tendency. The stochastic mean  allows a relatively fast mean-reversion of the short rate around a highly persistent time-varying central tendency process. Jumps are integral to the quality of fit and relieve the stochastic volatility factor from accommodating extreme outlier behavior. 

The mean drift may be indicative of slowly evolving inflationary expectations (Gara horiz?), time-variation in the required real interest rate, or both.


<!--
Hamilton (1996) and others have concluded that the hypothesis that the FFR follows a martingale is inconsistent with observed daily changes in the federal funds rate. (Bertolini,Bertola, Prati ,) site frictions such as  transactions costs, interbank credit limits, overdraft penalties, obstacles to intra-period arbitrage preclude the FFR from following a martingale process.
-->



Gara Afonso --------------------------------
Gara et al model of banks' reserve demand response to shocks to reserves reveals greater sensitivity to rate changes when reserves are scarce. When reserves are ample, [connect idea?] The demand curve is flat around the interest rate paid by the Federal Reserve on reserve balances (the IORB rate), the banks’ opportunity cost of lending in the federal funds market. 

The slope of the demand function is negative as reserves decline. The sensitivity of rates to reserve shocks, becomes increasingly steeper reflecting the The slope is flat when reserves are abundant. increasing scarcity value of reserves. Reserve demand, realized rates against realized reserves, a nonlinear, downward-sloping relationship between prices and quantities, shifts outward over time

\emph{Vertical shift}
Policy changes shift the curve up and down by moving its lower bound. Increases (descreases) in the (FFR-IORB?) IORB rate shift the demand curve down (up by changing the banks’ opportunity cost of lending in the federal funds market 
\emph{Horizontal shift}
Low frequency horizontal shifts in the demand function reflect sensitivity of rates to shocks to reserves.
(1): for every level of the federal funds rate, they imply an increase in the quantity of aggregate
reserves demanded by the banking system. As a result of these shifts, the level of reserves at which
the curve stops being flat and start displaying a negative slope may have moved over time.

These structural changes [which ?] could be interpreted as the horizontal shifts $\ast(q)$ in the demand curve


? The visible negative correlation between quantities (reserves) and prices (federal funds rates) suggests  supply shocks tend to dominate demand shocks. 


? explains ONRRP: The segmentation of the federal funds market between banks and FHLBs suggests that, under our price normalization, the lower bound of the demand function (1), $\ast(p)$ , should range between the
ONRRP-IORB spread and zero.

derive a function of reserve demand that measures banks’ demand for liquidity  as a function of aggregate reserves and the FFR. The FFR is price at which banks are willing to borrow and lend  reserve balances.The reserve demand function describes the price at which banks are willing to trade reserves as a function of the total amount of reserves in the banking system.
Figure (Panel (b))  to control for changes in FOMC policy, they plot federal funds rate minus the IORB rate versus aggregate reserves. Policy changes shift the curve up and down by moving its lower bound. 
The visible negative correlation between quantities (reserves) and prices (federal funds rates) suggests  supply shocks tend to dominate demand shocks. Reserve demand, realized rates against realized reserves, is a nonlinear, downward-sloping relationship between prices and quantities that moves outward over time



Bertolini, Bertola, Prati () model of FFR volatility seeks to isolate Fed preferences for offsetting volatility in the FFR market.[describe model], the high frequency patterns of FF rate volatility  that survive Fed's attempt to manage the FFR within the FOMC target range. 
They note the immediate response of other key short rates to monetary policy and other shocks. Overnight wholesale money market rates quickly revert to changes in the unsecured federal funds rate (FFR) or administered rates. 

\emph{Duffie, Krishnamurthy}
Krishnamurthy and Vissing Jorgensen propose the implicit volatility of overnight reference rates results from  of Fed discretionary policy, quantitative easing (QE) and ..asset purchase. Ample reserves paper?

Duffie and Krishnamurthy.The  effectiveness of monetary policy to transmit the desired stance of monetary policy to financial markets and the real economy economy depends on efficient transmission of rate changes to other short rates in wholesale and consumer funding market.This paper examines the dispersion and volatility of short term rates. 

Krishnamuthy provide evidence that dispersion among these secured overnight rights impede passthough of monetary policy to the economy. Mean reverision versus dispersion hypothesis


Cochrane and Piazzesi ------------------------------
regress policy shocks on rates

<!--
NOTES --------------------------------------------------------------------------------

2. Affine Term Structure Models in the Handbook of Financial Econometrics, Elsevier
Monika Piazzesi
Department of Economics, Stanford University, Stanford, California

Several aspects of bond
yields, however, set them apart from other variables typically used inVAR studies. One
aspect is that bonds are assets, and that bonds with many different maturities are traded at
the same time. Bonds with long maturities are risky when held over short horizons, and
risk-averse investors demand compensation for bearing such risk.Arbitrage opportunities
in these markets exist unless long yields are risk-adjusted expectations of average future
short rates. Movements in the cross section of yields are therefore closely tied together.
These ties show up as cross-equation restrictions in a yield-VAR. Another aspect of yields is
that they are not normally distributed, at least not until recently. This makes it difficult
to compute the risk-adjusted expected value of future short rates.

Term structure models capture exactly these aspects of bond yields. They impose the
cross-equation restrictions implied by no-arbitrage and allow yields to be nonnormal.
The word “affine term structure model” is often used in different ways. I will use the
word to describe any arbitrage-free model in which bond yields are affine (constant plus linear) functions of some state vector x.
1 Affine models are thus a special class of
term structure models, which write the yield y(τ) of a τ-period bond as
y(τ) = A(τ) + B(τ)x
for coefficients A(τ) and B(τ) that depend on maturity τ. The functions A(τ) and B(τ)
make these yield equations consistent with each other for different values of τ. The
functions also make the yield equations consistent with the state dynamics.


The functional form of bond yields is obtained from computing risk-adjusted expectations of future short rates.Therefore, restrictive assumptions have to be made on the risk-adjusted dynamics of the state
vector. More concretely,the risk-adjusted process for the state vector needs to be an affine
diffusion, a process with affine instantaneous mean and variance.

Why Care About Bond Yields: 
forecasting
monetary policy
debt policy
derivative pricing and hedging 
computed from a given
model of the yield curve (see the references in Duffie et al.,2000). Banks need to manage
the risk of paying short-term interest rates on deposits while receiving long-term interest
rates on loans.

policy shocks on long-term bonds outside of a yield-curve model.
More patience is required to estimate a system of yield equations in a way that ensures
no-arbitrage. The cross-equation restrictions have to be derived from parameters that
describe the state dynamics and risk premia. Although the model is affine in the state
vector x,the functions A(τ) and B(τ) are nonlinear functions of the underlying parameters

Cross-equation restrictions have many advantages. First, these restrictions ensure that
the yield dynamics are consistent. A(τ) and B(τ) make yield equations consistent with
each other in the cross section and the time series. 

Second, term structure models allow us to separate risk premia from expectations
about future short rates.

EH under which expected excess bond returns are zero. Modified versions of the EH have been tested under which expected excess returns are constant. These tests compare, e.g., the ratio of the likelihood function with and without restrictions implied by the EH (for references, see Bekaert and Hodrick,
2001).The evidence suggests that expected returns on long bonds are on average higher
than on short bonds and that they are time-varying. Cross-equation restrictions are then
needed to model these risk premia.
Third, unrestricted regressions imply that the number of variables needed to describe
the yield curve equals the number of yields in the regression.

2. BASICS
2.1. Bond Pricing in Continuous Time
Term structure modeling determines the price of zero-coupon bonds
The pricing relation (2.2) shows that any yield-curve model consists of two ingredients:
(i) the change of measure from Q to Q∗ and
(ii) the dynamics of the short rate r under Q∗.
In so-called factor models of the yield curve,(ii) is replaced by the following assumption:
(ii)' the short rate r is a function R(x) of x and x ∈ RN is a time-homogeneous Markov process under Q∗.
This means that x is the relevant state vector, a vector of factors. This modified (ii)
assumption implies that the conditional expectation in (2.2) is some function F of time-to-maturity τ and the state $x_t$ at time t, or
$P(τ)_t = F(x_t, τ)$.
To capture certain features of yield data (e.g., seasonalities around macroeconomic
news releases), I will later consider functions R that also depend on time t and time-inhomogeneous Markov processes x, in which case $P(τ)_t = F(x_t, t, τ)$ separately depends on t and τ (in addition to $x_t$)


The payoff of zero-coupon bonds is 1 unit at maturity, so their price is
$P*{\tau}_t = E^∗_t\left exp \left -\int{t}^{t+\tau} r_u du \right\right $ (2.2)
where E∗ denotes expectation under Q∗. Standard results show that if there exists a rik-neutral probability measure Q∗, a system of asset prices is arbitrage free

The big advantage of pricing bonds (or any other assets) in continuous time is Ito’s
Lemma. The lemma says that smooth functions F of some Ito process x and time t are
again Ito processes (see Duffie, 2001, Chapter 5 for details). The lemma thus preserves
the Ito property even if F is nonlinear. Ito’s Lemma allows me to turn the problem
of solving the conditional expectation in (2.2) into the problem of solving a PDE for
the bond price F(x, τ). The trick of computing (2.2) by solving a PDE is called the
Feynman–Kac approach.

2.2. Local Expectations Hypothesis
The LEH states that the pricing relation (2.2) holds under the data-generating measure
Q. Bond yields are thus given by
LEH : $y^{\tau}_t = −log E_t [exp(−S)] /\tau $ (2.4)
where $S = \int t+τ t ru du$ (as above). The LEH therefore amounts to risk-neutral pricing: the data generating measure Q and the risk-neutral measure Q∗ coincide. This means that
expected excess returns on long bonds are zero.
The LEH is not the same as the more prominent EH, which states that bond yields
y(τ)
t are expected values of average future short rates, or
EH : $y^{\tau}_t = E_t[S]/\tau$. (2.5)

The difference between the two hypotheses (2.4) and (2.5) is due to Jensen’s inequality.
For example, suppose that the short rate is Gaussian under Q = Q∗, which implies that
Affine Term Structure Models S is also Gaussian (as the sum of Gaussians). With this normality assumption, Eq. (2.4) becomes   $y^{\tau}_t= E_t[S] /(tau− .5 var_t[S])tau$
which differs from (2.5) because of the variance term.

3. AFFINE MODELS
Affine term structure models make functional-form assumptions in step (ii)' of yield curve modeling, which lead to tractable pricing formulas.The functional-form assumptions are on the short-rate function R(x) and the process x for the state vector under the
risk-neutral measure. The functional form is affine in both cases:
• R(x) is affine
• x is an affine diffusion under Q∗:
• the drift $\mu_x(x)$  is affine
• the variance matrix $\sigma_x(x) \sigma_x(x)$is affine.

These functional forms are for coefficients under the risk-neutral measure. In particular,
the drift $\mu_x(x)$ is affine under the data-generating measure only when $\sigma_x(x) \sigma_\xi(x)$ is
affine, which can be seen from (2.12). The next sections make these assumptions more
precise and show that bond prices F(x,τ) are exponential-affine in x. In this setting,
yields are thus affine in x which explains the name of this class of models.

3.1. Affine Short Rate
The functional form of the short rate is made precise in the following assumption.
Assumption 1 The short rate is given by
$r = R(x) = \delta_0 +  \delta_1 x$
for $\delta_0 ∈ R$ and  \delta_1 ∈ R^N $.

The choice of short-rate parameters $\delta_0$ and $\delta_1$  depends on the number of factors in
the model.The short rate usually is the factor in one-factor models, which means $\delta_0=0$ 
and $\delta_1 =1 $. The short rate in one-factor models is Markov. In N-factor models, the
short rate alone is not Markov, but the short rate together with N − 1 yields is typically
Markov.The short rate often serves as one of the factors in multidimensional models. In
this case, we still have $\delta_0=0$ and $\delta_1=(1, 0_{N−1})'$. Long yields still depend on the other
factors because the expected future path of the short rate depends on the current state
x in (2.2), when the short rate covaries with these other factors under the risk-neutral
measure.

Bertolini, Bertola, Prati () and Gara () observe the effect of Fed interventions on the volatility in interbank rates. Add
(Schuhlhofer et all propose efficiency in the ample reserves regime adopted (date) address the tradeoff the Fed makes to minimize the frequency of operations, volatility of interest rates, and balance sheet costs 

that daily money market rates hover around Fed targets and quickly revert to them in times of shocks.Bertolini et al note the tendency of short term rates to hover around official targets and quickly revert to them in times of shocks.

Bertolini et al Time series methodology: 
volatility of interest rates - rises in advance of reserve
settlement days - declines in high rate regimes - biweekly periodicity when Fed is perceived as committed to keeping rates close to the target

Although overnight wholesale money market rates change in response to monetary policy and other shocks, they quick revert to changes in the unsecured federal funds rate (FFR) or administered rates.

 Hamilton ----------------------
 Section IV proposes a theoretical model of the federal funds market that
 could account for these features as a result of line limits, transaction
 costs, and weekend accounting conventions.
 
 These results suggest that there is little, if any, interday speculation
 that would smooth out small fluctuations in the federal funds rate. A
 small temporary infusion of reserves through an overnight re-
 purchase agreement by the Fed's trading desk lowers that day's fed-
 eral funds rate by inducing movement along a schedule that repre-
 sents lending banks' liquidity benefit from holding excess reserves.
 
 The ability of the repurchase agreement to affect the interest does
 not depend on whether the open-market operation is anticipated in
 advance. 
 
 A small temporary infusion of reserves through an overnight repurchase agreement by the Fed's trading desk, whether or not the open-market operation is anticipated in advance, lowers that day's federal funds rate by inducing movement along a schedule that represents lending banks' liquidity benefit from holding excess reserves. [repo means dealer deposits (borrowed) reserves collateralized by UST securities?]
 
Hamilton provides valuable evidence of  end-of-quarter and end-of-year effects since bank balance sheets are publically available, providing a snapshot of an institution's assets on the last day of the
quarter or the last day of the year. Fridays, the federal funds market is soft. A federal funds loan on a Friday has a 3-day term rather than a 1-day term, and it is not altogether surprising that if the 
martingale hypothesis fails, its failure might show up in part in the appearance of something different about Fridays. 

Hamilton (1996) adapts Nelson's (1991) model of daily stock return in a model of the conditional mean and the conditional variance of  the FFR federal funds rate. The volatility consequences of upward and downward moves in interest rates are asymmetric. There is controversy if the FFR and overight rates, follow a random walk. Hamilton (1996) and others have concluded that the hypothesis that the FFR follows a martingale is inconsistent with observed daily changes in the federal funds rate. (Bertolini,Bertola, Prati ,) site frictions such as  transactions costs, interbank credit limits, overdraft penalties, obstacles to intra-period arbitrage preclude the FFR from following a martingale process.

His empirical description of the time-series process that generates the
 daily federal funds rate, simultaneously modeling the conditional
 mean, variance, and overall shape of the probability distribution of
 daily changes in the federal funds rate. These results are then used
 to understand what happens to the federal funds rate when the Fed
 makes an open-market purchase
 
Mondays are independently estimated to be days of federal funds increases. If we accept the idea that the level of the  federal funds rate tends to be a little low on Fridays, the change from Friday to Monday by implication would be bigger than average.

 
 I conclude that transaction costs, rather than a minor institu-
 tional detail of this particular market, lie at the heart of the liquidity
 effect that enables the Federal Reserve to change the interest rate on
 a daily basis
 
end-of-quarter and end-of-year effects as well. Public scrutiny of
 many enterprises is based on their balance sheets, which typically
 represent a snapshot of an institution's assets on the last day of the
 quarter or the last day of the year. Some institutions appear willing
 to hold certain assets on December 30 that they are unwilling to hold
 on December 31. At the end of the year, there is a surge in asset
 turnover and in corporations drawing on bank lines of credit, which
 can generate a huge demand for short-term loans of good funds
 around that date; see Allen and Saunders (1992) for further discus-
 sion. Again at a minimum we want to allow for increased variability
 of the federal funds rate at the end of the quarter and possibly tempo-
 rary departures from the martingale hypothesis on these days as well
 
 
 The model  In modeling the conditional mean, recall that the martingale hy-
 pothesis does not restrict the process followed by it if t corresponds
 to the first Thursday of a maintenance period. The last day of a
 quarter may likewise reflect special circumstances, with limited substi-
 tutability with reserves held on the day before or after. Thus if t is
 the first day of a maintenance period or the first day of a quarter, I
 describe the conditional mean for the federal funds rate with a pth-
 order autoregression:
 
 The mean 
 \begin{align*}
 i_t=\mu_t +\sigma_t \nu_t
 \mu_t = i_{t-1} + \eta_{s_t} + \beta h_t
 #y¯_t = [(\sum_{i}^{}(v_{i,t}) times yhat_{i,t})]/(\sum_{i}^{}v_{i,t})
 \end{align*}
 where re $s_t$ E {1, 2, . . ., 10} indicates which day of the maintenance
 period is associated with observation t.
  
  Thus $\eta_2$ corresponds to the average change in the federal funds rate
 between the first Thursday and Friday of a maintenance period,
 whereas $\eta_10$ gives the average change of the federal funds rate on
 settlement Wednesday. Under the martingale hypothesis, $\eta_j$ = 0 for j=2, 3, .. ., 10.
 
 
To describe at', the conditional variance of the federal funds rate,
 I adapt Nelson's (1991) model of daily stock returns
 $log(\sigma^2_t)-\xi_{s_t}-\kappa h_t = (sum_{j=1}^{\tau}(log(\sigma^2_{t-j}) -\xi_{t-j}-\kappa h_{t-j} =
 (\alpha abs(\nu_{t-1})-E(abs(\nu_{t-1})) + \chi\nu_{t-1})$  or $hi or \zeta$
 
Empirical resultspp 38-44 
 He finds deviations of the FFR from the FOMC target tend to dissipate quickly. (same for other rates?).  S fraction - ) of the cumulative change over the preceding two days is reversed on
 day t:
 $\mu_t = \eta_1 + i_{t-l} + \fi(i_{t-1} - i_{t - 3} + I'h_t $ (12)
 
 the unconditional variance has the same value for days 2-7,
 $ \xi_2 = \xi_2= cdot \xi_7 $ (13)
 the generalized autoregressive conditional heteroskedasticity
 effects are integrated:
 $\delta_2 + \delta_2  + cdot + \delta_\tau= 1$
 
Two magnitudes most useful for describing the conditional variance are (1) the conditional variance on the previous day and (2) the average deviation during the maintenance period (eqn 12)
 
  The autoregressive coefficients that characterize the first day of a new maintenance pe-
 riod or new quarter take the following form: some fraction - ) of
 the cumulative change over the preceding two days is reversed on
 day t. That is, these observations are described with the following
 special case of (4):
 $\mu_t = 91 + i_{t-l} + 4k(i_{t-1} - i_{t - 3} + I'h_t $ (12)
 where I estimate + = - 0.81. This is clearly related to the conclusion
 by Rudebusch (1994) that deviations of the funds rate from the value
 targeted by the Fed tend to dissipate quickly. M
 
 \url{https://bookdown.org/dalzelnm/bookdown-demo/mathematical-notation-in-r.html}
 $Y_i \sim N( \mu, \sigma)$ makes  Yi∼N(μ,σ)
 gridExtra::grid.arrange(g1,g2, g1,g2, ncol = 2)
 .
 The vector $h_t$ is a collection of zero-one dummy variables incorporating calendar effects.
 Calendar effects nine elements of h_t :
 t holidays (as captured by the first four elements of ht) matter for
 the mean parameters (I) but not for the variance parameters (K). By
 contrast, the end-of-quarter and end-of-year effects on the variance
 were very dramati
  j 1 Kj Meaning
 1 -.028 {.00} t precedes a 1-day holiday
 (.020)
 2 -.031 {.00} t precedes a 3-day holiday
 (.011)
 3 .023 {.00} t follows a 1-day holiday
 (.020)
 4 .171 {.00} t follows a 3-day holiday
 (.017)
 5 {.00} .853 t is the last day of quarter 1, 2, 3, or 4
 (.379)
 6 {.00} .975 t is the last day of the year
 (.765)
 7 {.00} 1.550 t is the day before, on, or after the last day of quarter 1,
 (.216) 2, or 3
 8 {.00} {1.550} t is 2 days before, 1 day before, on, 1 day after, or 2 days
 after the end of the year
 9 {.185} {.00} t is day 9 but precedes a 1-day holida
 
  MAXIMUM LIKELIHOOD ESTIMATES OF OTHER PARAMETERS IN EQUATIONS
 (7), (12), AND (15)
 Parameter Value Meaning
 $\fi$- .811 Fraction of $i_{t-1} - i_{t-3}$ that is expected to be reversed if t is
 (.021) first day of a new period
  $\delta$- .793 Weight on the previous day's log variance in determining
 (.046) today's log variance
   $\alpha$-.316 Weight on the absolute value of the previous day's innova-
 (.036) tion in determining today's log variance
 $\x$ .341 Additional effect of a positive previous day's innovation in
 (.083) determining today's log variance
  $\rho$- .861 Fraction of innovations drawn from distribution 1
 (.026)
 j 1{1.00} Standard deviation of distribution 1
 T2 2.915 Standard deviation of distribution 2
 (.200


Fridays, the federal funds market is soft  the eta's
 A federal funds loan on a Friday has a 3-day term rather
 than a 1-day term, and it is not altogether surprising that if the mar-
 tingale hypothesis fails, its failure might show up in part in the ap-
 pearance of something different about Fridays. Likewise, days 3 and
 8 are both Mondays, and both are independently estimated to be days
 of federal funds increases. If we accept the idea that the level of the
 federal funds rate tends to be a little low on Fridays, the change from
 Friday to Monday by implication would be bigger than average

 proposes a complete
 empirical description of the time-series process that generates the
 daily federal funds rate, simultaneously modeling the conditional
 mean, variance, and overall shape of the probability distribution of
 daily changes in the federal funds rate. These results are then used
 to understand what happens to the federal funds rate when the Fed
 makes an open-market purchasThe federal funds market is a good place to start for an understand-
 ing of either finance or monetary policy. From the point of view of
 finance, overnight federal funds represent the shortest-term security
 that is actively traded and thus form the base of any term structure
 relation. Indeed, recent papers by Balduzzi, Bertola, and Foresi
 (1993), Roberds, Runkle, and Whiteman (1994), and Rudebusch
 (1994) suggest that several well-known puzzles in the term structure
 of interest rates can be resolved by tracking these relations back to
 daily changes in the federal funds rate.
 From the point of view of monetary policy, the daily operating
 procedures of the Federal Reserve System are also defined in terms
 of the federal funds market. An open-market purchase injects neThe federal funds market is a good place to start for an understand-
 ing of either finance or monetary policy. From the point of view of
 finance, overnight federal funds represent the shortest-term security
 that is actively traded and thus form the base of any term structure
 relation. Indeed, recent papers by Balduzzi, Bertola, and Foresi
 (1993), Roberds, Runkle, and Whiteman (1994), and Rudebusch
 (1994) suggest that several well-known puzzles in the term structure
 of interest rates can be resolved by tracking these relations back to
 daily changes in the federal funds rate.
 From the point of view of monetary policy, the daily operating
 procedures of the Federal Reserve System are also defined in terms
 of the federal funds market. An open-market purchase injects ne
a consensus as to the nature of predictable changes in
 the federal funds rate or the economic forces that produce them.
 Earlier researchers have agreed that this daily interest rate fails to
 follow a martingale, However,
 none of these studies made allowance for the spectacular outliers
 and autoregressive conditional heteroskedasticity (ARCH) effects that
 characterize these data, and apart from Campbell (1987), none of-
 fered a detailed description of how the findings could be reconciled
 with profit maximization by banks.

The reserve demand function Estimating 
Gara Alonso ---------------------------------------
The sensitivity of the federal funds rate FFR to shocks to the level of reserves is of paramount importance for the implementation of monetary policy.

time evolution of aggregate reserves normalized by banks’ total assets to control for the growth of the banking industry in our sample.plots the daily average  correlation between quantities (reserves) and prices (federal funds rates), which suggests that after
removing month-end data from our daily sample, supply shocks tend to dominate demand shocks.
By comparing panels (a) and (b), we can see a negative
correlation between quantities (reserves) and prices (federal funds rates), which suggests that after
removing month-end data from our daily sample, supply shocks tend to dominate demand shocks.
This is confirmed by panel (c), which plots realized rates against realized reserves and can be seen
as an approximate visualization of the reserve demand curve. It shows a clear nonlinear, downward-sloping relationship between prices and quantities that moves outward over time: the curve moved up and to the right after 2014 and further up after March 2020, at the onset of the Covid pandemic

The curve itself, however, also moves horizontally at a low frequency. In the earlier part of
our sample, the rate sensitivity to reserve shocks fades as reserves exceed 12$\%$  of banks’ assets; in
the latter part, instead, it reemerges around 13$\%$ , suggesting a moderate shift to the right. Below
these thresholds, the curve’s slope gets steeper as reserves decrease, consistent with the theory. A
one-percentage-point drop in normalized reserves increases the federal funds-IORB spread by 1.3
basis points (bp) in 2010 and by 1.1 bp in 2019, while having no effect in 2012-2017 and after March
3, 2020. These estimates are robust to controlling for spillovers from the repo and Treasury markets.
Results are qualitatively similar if we normalize reserves by GDP instead of banks’ assets

\emph{theory reserve demand}
bank demand:  regulatory requirements and liquidity needs, such as daily payments to other institutions.

Banks borrow and lend reserves in the federal funds market, typically overnight, at the federal funds rate. The reserve demand curve describes the price at which banks are willing to trade reserves as a function of the total amount of reserves in the banking system.

Demand curve slope is banks's reserve demand response to shocks:
Low reserves, negative slope, reduce demand as interest rates or shocks...
As reserves keep decreasing, the slope of the curve, i.e., the sensitivity of rates to reserve shocks, becomes increasingly steeper reflecting the
increasing scarcity value of reserves.

High reserves, 0 slope, no response
the demand curve is flat around the interest rate paid by the Federal Reserve on reserve balances (the IORB rate), which is the banks’ opportunity cost of lending in the federal funds market.
Gara plot: GaraAmpleReserves2022

Andersen, Benzoni, Lund -----------------------------------
- mean drift 
the mean drift may be indicative of slowly evolving inflationary expectations, time-variation in the required real interest rate, or both.Jumps to be integral to the quality of fit and to relieve the stochastic volatility factor from accommodating extreme outlier behavior.

stochastic volatility factor from accommodating extreme outlier behavior.
- continuous-time model provides an excellent characterization of the U.S. short-term interest rate
- three-factor jump-diffusion model
- simultaneous and efficient inference regarding all model components which include
    - a shock to the interest rate process itself
    - a time-varying mean reversion factor
    - a stochastic volatility factor 
    - a jump process.
    
short rate:
- governs the price of riskless fund
transfers and is thus a key determinant of the intertemporal consumption and investment decisions
of economic agents. 
-impacts the expected returns of primary assets whose
excess returns (over the short rate) are functions of systematic risk exposures and associated risk
premia. 
-direct input to pricing and hedging in the huge fixed income securities market and the associated trading of fixed-income derivatives

main features of the U.S. short-term interest rate
- complex conditional heteroskedasticity
- fat-tailed innovations
- pronounced autocorrelation patterns

- stochastic volatility
   - critical for a good fit
   - the stochastic mean offers a more modest, but still significant, improvement. 
     allow for a relatively fast mean-reversion of the short rate around a highly persistent         
     time-varying central tendency process.

- mean drift 
the mean drift may be indicative of slowly evolving inflationary expectations, time-variation in the required real interest rate, or both.

- jumps
 Furthermore, we find jumps to be integral to the quality of fit and to relieve the stochastic volatility factor from accommodating extreme outlier behavior. Without jumps, the estimated volatility specifications are unable to match the degree of persistence in the conditional variance process typically observed in short-term interest rate series. 

We find that an intuitively appealing and fairly manageable continuous-time model provides
an excellent characterization of the U.S. short-term interest rate over the post Second World
War period. Our three-factor jump-diffusion model consists of elements embodied in existing
specifications, but our approach appears to be the first to successfully accommodate all such
features jointly. Moreover, we conduct simultaneous and efficient inference regarding all model
components which include a shock to the interest rate process itself, a time-varying mean reversion factor, a stochastic volatility factor and a jump process. Most intriguingly, we find that
the restrictions implied by an affine representation of the jump-diffusion system are not rejected
by the U.S. short rate data. This allows for a tractable setting for associated asset pricing
applications.

Why the short rate(s)?
The short rate governs the price of riskless fund
transfers and is thus a key determinant of the intertemporal consumption and investment decisions
of economic agents. In addition, the short rate impacts the expected returns of primary assets whose
excess returns (over the short rate) are functions of systematic risk exposures and associated risk
premia. Finally, the short rate serves as a direct input to pricing and hedging in the huge fixed income securities market and the associated trading of fixed-income derivatives


main features of the U.S. short-term interest rate, i.e., complex
conditional heteroskedasticity, fat-tailed innovations and pronounced autocorrelation patterns, may
be captured within an intuitively appealing and manageable continuous-time jump-diffusion setting.
Relative to earlier contributions, we explore a more general parametric class of continuous-time models, allowing for both multiple latent factors, entering separately in the drift and diffusion coefficients,
and jumps in the interest rate level.

Both of our favored models contain three factors featuring stochastic volatility, mean drift and
jumps. The inclusion of the stochastic volatility factor is critical for a good fit, whereas the stochastic mean offers a more modest, but still significant, improvement. Specifically, we find it important to allow for a relatively fast mean-reversion of the short rate around a highly persistent time-varying central tendency process. Economically, Interestingly, we find little evidence of the so-called level effect. Moreover, our analysis suggests that correlation between the shocks to the short rate process and the other two latent factors is not critical in modeling the short rate series. Overall, there is no indication of misspecification in our preferred models. Along some relevant dimensions, our affine three-factor jump specification provides the superior fit, which lends support to jump-diffusion representations of the form suggested by Duffie, Pan, and Singleton (2000) and Chacko and Das (2002)

Other recent contributions that have investigated the short term interest rate include Das (2002); Eraker (2001);
Elerian, Chib, and Shephard (2001); Hamilton (1996); Johannes (2004); Li, Pearson, and Poteshman (2002), and more.
-->
NOTES --------------------------------------------------------------------------------

<!--
Citations
Cochrane and Piazzesi
Benzoni
Gara
Duffie Krishnamurthy

Adam Copeland | Darrell Duffie | Yilin (David) Yang.  Reserves Were Not So Ample After All. July 2021. Federal Reserve Bank of New York Staff Reports, no. 974
JEL classification: G14, D47, D8


James D. Hamilton. The Daily Market for Federal Funds. February 1996. Journal of Political Economy, Vol. 104, No. 1 
Stable URL: https://www.jstor.org/stable/2138958 
 Published by: The University of Chicago Press  pp. 26-56

Piazzesi, Monika. Bond Yields and the Federal Reserve. April 2005. Journal of Political Economy, Volume 113, Issue 2, pp. 311-344.

Gara Afonso, Kyungmin Kim, Antoine Martin, Ed Nosal, Simon Potter, and Sam Schulhofer-Wohl. Monetary policy implementation with an ample supply of reserves. January, 2020. Federal Reserve Bank Chicago.WP 2020-02
https://doi.org/10.21033/wp-2020-0

Bertolini.L, Bertola, Prati. Day-To-DaY_Monetary Policy and the Volatility of the Federal Funds Interest Rate. December 2000.  IMF WOrking Paper WP/00/206

Torben Gustav Andersen, Luca Benzoni,Jesper Lund. Stochastic volatility, mean drift, and jumps in the short-term interest rate. January 2004

@article{article,
author = {Andersen, Torben and Benzoni, Luca and Lund, Jesper},
year = {2004},
month = {01},
pages = {},
title = {Stochastic volatility, mean drift, and jumps in the short-term interest rate}
}
-->

<!--
# recover files rsrecovr::recovr(project_path = "all")
COMMENT on Bertolini,Bertola, Prati ()  observation that that volatility in the federal funds rate is lower in high rate regimes or when Fed is perceived as committed to kee
-->

!>--
USE this in paper outline:  Section II provides a brief review of the current operation of re-
 serve requirements and the federal funds market in the United
 States. Section III develops a time-series description of daily changes
 in the federal funds rate. 
 -->
 
<!--
Hypotheses 1 Volatility: Noteable is the high frequency of volatility of the Fed's policy rate EFFR as the FOMC manages the FF rate within target range. The volatility is especially high in the Fed October date ample reserves policy regime.  The EFFR and OBFR are respond to changes in FOMC target range for the Federal Funds rate (FFR) and the administered rates - interest on reserves (IOR) and the overnight reverse repurchase rate (ONRRP). But the distributions of secured rates TGCR, BGCR, SOFR share similar characteristics of the distribution of these unsecured rates.The volume of trade in interbank markets even more volatile than rates (Note mean reversion, no change in distribution. Dispersion, changes in distribution coef of variation standard deviation devided by the mean.)

Hypotheses 2 Although overnight wholesale money market rates change in response to monetary policy and other shocks, they quick revert to changes in the unsecured federal funds rate (FFR) or administered rates. Duffie and Krishnamuthy provide evident that dispersion among these secured overnight rights impede passthough of monetary policy to the economy. Mean reverision versus dispersion hypothesis

Hypothesis 3 The rate data provide evidence of Fed's difference preferences for offsetting volatility in the Federal Funds market when managing the FFR within its target range, The Fed may change target and administered rates. Temporary changes in reserves (FF market, TGCR) can manage the FFR through repo markets. A  reverse repo increase in reserves through Fed purchase of reserves with securities, lowers the FFR, a a repo with dealers or () decreases in reserves through  increases the FFR.
 
Hypothesis 4: Puzzle: The demand for each instrument, price versus volume reveals,large changes in demand but relatively stable rates A puzzle: price stable, demand fluctuates? Daily rates versus dollar volumes of transactions illustrate demand for these overnight money market instruments. Volumes more volatile than rates

Hypothesis 5 The volatility response of overnight rates to Fed interventions is greater:  the lower reserves, the magnitude of the shock of the monetary policy interventions, the type of  change in policy intervention: the Fed Funds targets, or bounds of the channel for the EFFR rate or the administered rates, the IOR and ONRRP. 
  
Other: 
6. volatiity of transactions
7. spreads, indicators of reserves, arbitrage
8. another research question/hypothesie
-->
               


# Monetary policy during the sample period 2016-2023
* my shocks (FFR changes under episodes)
* Romer and Romer shocks
* other external shocks to these markets

In section (VOlatility? ) We examine how these shocks, changes in monetary policy (Table \ref{table:FOMCratechanges}).affect rate volatility in different monetary policy regimes.

Before the financial crisis of 2008, policy was typically based on adjusting the scarcity value of a limited supply of central bank deposits (reserves), but the substantial increase in liquidity resulting from asset purchases in response to the financial crisis of 2008, the Fed experimented with different policy regimes to manage the FFR. Policy continued to evolve with events that occurred during the six year period of our sample 2016-2023. The QE1, QE2, and QE3 asset purchases [PURPOSE] challenged the bank’s ability to control short-term interest rates. Asset purchases under QE resulted in central banks changing their overall frameworks for controlling short-term interest rates.

Policy changes include  FOMC reference rate changes from Libor sot SOFR, paying depositing banks interest on reserves (the administered rates the IORB), creation of standing repo facility, repo and the overnight reverse repo facility (ONRRP)  
 FOMC rate changes  occurred during the six year period of our sample 2016-2022 (Table \ref{table:FOMCratechanges}), monetary regimes, and events in Table. 

In 2018 the FOMC adopted target rates, In 2019, the FOMC abandoned active management of scarce reserves and other short term interest rates and adopted a policy of ample reserves.

On October 1, 2008, Congress gave the Fed the power to pay interest on reserves (IOR) to help control the fed funds rate. The IOR rate is the interest rate that banks earn from the Fed on the funds deposited in their reserve accounts. In 2013, the Federal Reserve introduced the ONRRP facility to set a floor under the EFFR.The overnight reverse repo rate (ONRRP) facility offering rate is the Fed’s supplementary tool for interest rate control. In the recently established repo facility, July 2021, the Standing Repo Facility (SRF), the Desk purchases securities from a counterparty subject to an agreement to repurchase the securities at a later date,  temporarily increases the supply of reserve balances in the banking system and lowers the FFR. 


The IOR is the Fed’s principal tool for interest rate control. It sets a ceiling on the FFR. In 2013, the Federal Reserve introduced the ONRRP facility to set a floor under the EFFR.The overnight reverse repo rate (ONRRP) facility offering rate is the Fed’s supplementary tool for interest rate control.

Each repo transaction is economically similar to a loan collateralized by securities,interest” paid on that loan, is the difference between the original price and the second, higher price. In a reverse repo transaction, the Desk sells securities to a counterparty subject to an agreement to repurchase the securities at a later date.  temporarily decreases the supply of reserve balances in the banking system and raising the FFR.

The reverse repo facility, the ON RRP facility, is available to a wide range of money market lenders. This facility is particularly important for monetary policy implementation in periods when reserves are elevated. As reserves grow, banks’ willingness to take on additional reserves diminishes, and they reduce the rates they pay for deposits and other funding. In this environment, market rates trade below the IORB rate because nonbank lenders are willing to lend at such rates. For example, the Federal Home Loan Banks (FHLBs), which are important lenders in the fed funds market and not eligible to earn IORB, are willing to lend at rates below the IORB rate rather than leave funds unremunerated in their accounts at the Fed. To provide a floor under the fed funds rate, the FOMC introduced the ON RRP facility.
Under this ample reserves regime The Federal Reserve manages the the policy rate, the Federal Funds rate (FFR) within its target range. 

In the ample reserves regime introduced in 2019, the Fed adjusts administrative rates, the IOR and the ONRPP, to manage the FFR between the upper and lower target rates set by the Federal Open Market Committee (FOMC).The Fed sets the interest on reserve balances (IOR) paid on deposits held at the bank, and the overnight reverse repurchase facility (ON RRP).  The ability to set the IOR and the ONRRP are key for the Fed’s implementation of monetary policy. 

In this  corridor system, the interest on reserves (IOR) is the ceiling, the overnight reverse repo purchase rare (ONRRP) the floor. Banks have little or no incentive to lend their reserves to other banks at rates lower than the IOR rate they can receive from the Fed. The IOR is, in effect, a reservation rate for banks’ intermediation activities.

Similarly, no bank should be borrow at a higher rate than the ONRRP. No bank should lend at a lower rate than the ONRPP. Banks and non-bank financial institutions should be unwilling to invest funds in private markets below the ONRRP. Competition among borrowers should drive rates to the discount window (DW) rate. 

,!--
scan for tidbits
In the ample reserves regime. the administered rates constitute a “floor” system to manages the FFR within its target range, the upper and lower targets set by the Federal Open Market Committee (FOMC). The ability to set the IOR and the ONRRP are key for the Fed’s implementation of monetary policy. The interest on reserves rate (IOR or IORB) is the Fed’s principal tool for interest rate control.the Fed adjusts administrative rates to manages the FFR between the FOMC target upper and lower bounds.The IOR. 

The administered rates, the IOR and the ON RPP,  along with an ample supply of reserves, created a corridor system for managing the Fed Funds rate. The IOR is the ceiling, the ON RRP the floor. No bank would lend at rates on overnight cash lower than the IOR, the rate they can receive from the Fed. Thus Banks have little or no incentive to lend their reserves to other banks at rates lower than the IOR rate. In this sense, the IOR rate is in effect a reservation rate for banks’ intermediation activities,
No bank should be borrow at a higher rate than the ONRRP. Banks and non-bank financial institutions should be unwilling to invest funds in private markets below the ONRRP. No bank should lend at a lower rate than the ONRPP. Competition among borrowers should drive rates to the discount window (DW) rate. 

Under the Ample Reserves policy, the Federal Reserve manages the FFR within its desired range in a “floor” system through Temporary open market operations (TOMO), short-term repurchase and reverse repurchase agreements. These Short-term repurchase and reverse repurchase agreements are designed to temporarily add or drain reserves available to the banking system and influence day-to-day trading in the Federal Funds market.
-->


\url{https://libertystreeteconomics.newyorkfed.org/2022/01/how-the-feds-overnight-reverse-repo-facility-works/}
How to cite this post:
Gara Afonso, Lorie Logan, Antoine Martin, William Riordan, and Patricia Zobel, “How the Fed’s Overnight Reverse Repo Facility Works,” Federal Reserve Bank of New York Liberty Street Economics, January 11, 2022, \url{https://libertystreeteconomics.newyorkfed.org/2022/1/how-the-feds-overnight-reverse-repo-facility-works/}.

a (Table \ref{table:FOMCratechanges}).
<!--
Additional posts in this series:
How the Federal Reserve’s Monetary Policy Implementation Framework Has Evolved
How the Fed Adjusts the Fed Funds Rate within Its Target Range
The Fed’s Latest Tool: A Standing Repo Facility

Related reading:
Stressed Outflows and the Supply of Central Bank Reserves (February 2019)
Divorcing Money from Monetary Policy (September 2008)
Federal Reserve Board  | Let’s Close the Gap: Revising Teaching Materials to Reflect How the Federal Reserve Implements Monetary Policy (October 2020)
-->


We examine how these shocks, changes in monetary policy.affect rate volatility in different monetary policy regimes.
FOMC rates changes, monetary regimes, and events over the four year period, 2018-2023 (Table \ref{table:FOMCratechanges}) include changes in the Fed funds rate and the administered rates IOR and the reward rate ON RRP

\begin{table}[h!]
\centering
\begin{tabular}{c c c} 
\hline
FOMC rates changes \\
%Date & Change &(bps)	$\&$ Federal Funds Rate (pct) \\ [0.5ex] 
 \hline\hline
\textbf{2015 to 2018: Returning to Normalcy} & \\
Date & Change &(bps)	$\&$ Federal Funds Rate (pct) \\ [0.5ex]
\hline
20-Dec-18 &	 25 &		2.25 to 2.50\\
Sept. 27, 2018 &		25 &		2.0 to 2.25\\
Jun. 14, 2018&		25 &		1.75 to 2.0\\	
22-Mar-18 &		25&		1.50 to 1.75\\	
Dec. 14, 2017 &		2&	5	1.25 to 1.50\\	
15-Jun-17&		25&		1.00 to 1.25\\	
16-Mar-17&		25 &		0.75 to 1.00\\	
Dec. 15, 2016 &		25 &		0.5 to 0.75\\
Dec. 17, 2015 &		25&		0.25 to 0.50\\
\hline
\textbf{2019  Mid-Cycle Adjustment} & \\
Date & Change &(bps)	$\&$ Federal Funds Rate (pct) \\ [0.5ex]
\hline
31-Oct-19 &		-25	&	1.50 to 1.75\\	
Sept. 19, 2019&		-25&		1.75 to 2.0\\	
Aug. 1, 2019&		-25	& 2.0 to 2.25\\	
\hline			
\textbf{2020 Coping with Covid} & \\
Date & Change &(bps)	$\&$ Federal Funds Rate (pct) \\ [0.5ex]
\hline
16-Mar-20&		-100&		0 to 0.25\\	
3-Mar-20&		-50&		1.0 to 1.25\\	
\hline		
\textbf{2022 Taming Inflation} & \\
Date & Change &(bps)	$\&$ Federal Funds Rate (pct) \\ [0.5ex]
\hline
14-Dec-22&		50&		4.25 to 4.50\\	
2-Nov-22&		75&		3.75 to 4.00\\	
21-Sep-22&		75&		3.00 to 3.25\\	
27-Jul-22&		75&		2.25 to 2.5\\	
16-Jun-22&		75&		1.5 to 1.75\\	
5-May-22&		50&		0.75 to 1.00\\	
17-Mar-22&		25&		0.25 to 0.50\\
\end{tabular}
\caption{FOMC rates changes 2018 to 2022}
\label{table:FOMCratechanges}
\end{table}

add 2023


Monetary policy shocks, events (Table \ref{table:monetary shocks}) 
QE date for analysis
FIND DATES, link to table and figure above
TABLE of monetary policy decisions and events
CHECK:UPDATE TABLE DO!
\emph{Which epoch, Sep 2019, Mar 2020, series of QE}
2019
March 15, 2020
March 23, 2020
June 2020
December 2020 slow purchase
November 2021
December 2021

1) 96 9/16/2019 Repo spike SOHR 2.42 +13 over 9/15, EFFR 2.23 +11
   97 9/17/2019 Repo spike    SOFR 5+                  EFFR 2.3   
\url{https://www.federalreserve.gov/econres/notes/feds-notes/what-happened-in-money-markets-in-september-2019-20200227.html}
On Monday, September 16, SOFR printed at 2.43 percent, 13 basis points higher than the previous business day. With pressures in the repo market spilling over into the fed funds market, the EFFR printed at 2.25 percent, 11 basis points above the Friday print and at the top of the FOMC's target range. On September 17, the EFFR moved above the top of the target range to 2.3 percent and the SOFR increased to above 5 percent.


2) Mar 10-18 2020  Dash for cash 
121 3/8/2020 0:00 subtract 5 from coordinate
122 3/15/2020 0:00
The COVID-19 Pandemic Caused Market Disruptions across Sovereign Bond Markets
At the start of the COVID-19 pandemic in late February 2020, and in response to the economic repercussions of impending lockdown measures, investors began to demand higher-quality, safe assets. In particular, they shifted their portfolios toward sovereign bonds, and the resulting buying pressure drove sovereign yields to decline broadly. As the crisis intensified in March 2020, however, investors’ demand for cash surged, leading to selling pressure on sovereign bonds and therefore increases in their yields. This down-and-up pattern in yields is illustrated for ten-year U.S., German, U.K., and Japanese bonds in the chart below.
\url{https://libertystreeteconomics.newyorkfed.org/2022/07/the-global-dash-for-cash-in-march-2020/#:~:text=The$\%$20economic$\%$20disruptions$\%$20associated$\%$20with,number$\%$20of$\%$20central$\%$20bank$\%$20actions.}


-122 March 15, 2020 On March 15, 2020, the Fed shifted the objective of QE to supporting the economy. It said that it would buy at least $\$$500 billion in Treasury securities and  $\$$200 billion in government-guaranteed mortgage-backed securities over “the coming months.” 
 - 123 3/22/2020  March 23, 2020, it made the purchases open-ended, saying it would buy securities “in the amounts needed to support smooth market functioning and effective transmission of monetary policy to broader financial conditions,” expanding the stated purpose of the bond buying to include bolstering the economy. 
 


- June 2020 - In June 2020, the Fed set its rate of purchases to at least $\$$80 billion a month in Treasuries and  $\$$40 billion in residential and commercial mortgage-backed securities until further notice. 
 - December 2020 slow: The Fed updated its guidance in December 2020 to indicate it would slow these purchases once the economy had made “substantial further progress” toward the Fed’s goals of maximum employment and price stability. 
 - November 2021 taper: In November 2021, judging that test had been met, the Fed began tapering its pace of asset purchases by  $\$$10 billion in Treasuries and  $\$$5 billion in MBS each month. 
 - December 2021 double taper: At the subsequent FOMC meeting in December 2021, the Fed doubled its speed of tapering, reducing its bond purchases by  $\$$20 billion in Treasuries and  $\$$10 billion in MBS each month

December 2020 slow purchase
November 2021
December 2021
  Table and figures display changes in the Fed funds rate and the administered rates IOR and the reward rate ON RRP
Table \ref{table:FOMCratechanges} lists FOMC rates changes, monetary regimes, and events over the four year period, 2018-2022.





 
# Daily overnight rates ---------------------------------------------- 
Intro
Observations issues
Overnight rates are outliers, heteroskedastic, outliers present, and Extreme or high volatility of short rates. volatility survives rate the Federal Reserve Bank's (Fed) management of of the Federal Funds rate.
Volatility relative to to prior periods (here daily) 
Outliers
Rate volatility survives 

# Rates
## Time series properties

Time series propertie of overnight policy, interbank rates, transactions, and reserves held at the Federal Reserve
We examine the time series properties of overnight reference rates and their correspondence with the Federal Funds rate, the FOMC policy rate from 2016 through 2023.Transactions, reserves.

For the sample from 2016 through 2023, (Figure \@ref(fig:Daily rates 2016-2022-plot), you can see how closely all overnight rates closely track the EFFR. Fed policy changes are visible in Vertical shifts of the series.I've identifies these shifts with different episodes of Fed policy regimes: derived from visual inspection of the data and the corresponsing policy changes (TABLE?) during each period.  A more rigorous approach to identifying change in regimes is offered by (Valeria Gargiulo, Christian Matthes, and Katerina Petrova,2024) 

Introduce overnight rates, EFFR, TGCR, BGCR, SOFR. sources NYFed, concoda David Gibbs
Figure 1. Visible in sample figure 1(and stats?)
Features of overnight rates closely tracking the EFFR
Vertical shifts from policy changes
Horizontal shifts, sensitivity
Policy inertia
Tendency to move together
Mean reverions (around a central tendency Benzoni) Martingale or not: Hamilton (1996) and others have concluded that the hypothesis that the FFR follows a martingale is inconsistent with observed daily changes in the federal funds rate

Monetary Policy across Inflation Regimes
Valeria Gargiulo, Christ ian Matthes, and Katerina Petrova
Federal Reserve Bank of New York Staff Reports, no. 1083
January 2024
\URL{https://doi.org/10.59576/sr.1083}

Introduce episodes: It helps to look at the behavior of data over regimes
1. normalcy   3/4/2016		7/31/2019      1  858 
2. mid cycle adjustment 8/1/2019 - 10/31/2019 737660  859 - 922
3. covid 11/1/2019	    3/16/2020   923  1013
4. zlb         3/17/2020- 3/16/2022     1014-1518
4. Taming inflation 03/17/2022 - 12/14/2023 1519-1957

Rates increase steadily from _ basis points during normalcy from March 2016 through July 2019.In 2019 rates began a three month period  from August to October 2019 (\@ref(fig:fig1).  With the onset of covid, rates  plunge from _ basis points in November 2019 to March 2020. Rates then hover near zero from Mar 2020 to March 2022 during the zero lower bound. Then rates rose in steady 25 or 50 basis point increments from March 2022 to_ in December 2023 as  the Fed fights inflation. Policy inertia is evident in all periods, as the Fed sequentially changes rates in ?? basis points .

Chat: In the sample, the average weighted median for the Effective Federal Funds Rate (EFFR) stands at 157 basis points (pb) (\@ref(tab:tab1). Comparatively, other money market rates show a clustering around the median EFFR, with the Tri-Party General Collateral Rate (TGCR), Brokered General Collateral Rate (BGCR), and Secured Overnight Financing Rate (SOFR) ranging from 123 to 134 bp. Daily transaction volumes also reflect this pattern, with SOFR leading at approximately $754 billion, followed by TGCR and BGCR at around $300 billion each. EFFR has a notably lower daily trade volume of $79 billion. The percentiles of rates further demonstrate this clustering around EFFR and SOFR.

 Figure \@ref(fig:sampleplot), the relationship is evident. 
```{r sampleplot, fig.cap="Overnight rates 2016-2023", echo=FALSE}
#rrbp <- subset(rrbp, select = -OBFR)
#rrbp$sdate <- as.Date(my_data$sdate)
rrbp <- cbind(sdate = as.Date(my_data$Date, "%m/%d/%Y"), rrbp)
str(rrbp)
maxrate<-max(rrbp[,2:5])
meltrrbp <- melt(rrbp,id="sdate")
 rates <- ggplot(meltrrbp,aes(x=sdate,y=value,colour=variable,group=variable)) + 
   geom_point(size = 1) +
   labs(x="",  y = "Basis Points (bp)", color = "Rate", shape = "Rate") +  
   scale_y_continuous(breaks = seq(0, maxrate, by = 50), limits = c(0, maxrate)) + 
   theme_minimal() + 
  guides(shape = guide_legend(title = "Rate"))
   print(rates)
```

?? hERE?


 Figure \@ref(fig:FOMC targets and EFFR)
```{r FOMC targets and EFFR, fig.cap= "EFFR, FOMC targets 2016-2023",echo=FALSE}
quantilesE2 = subset( quantilesE, select = -c(VolumeEFFR,TargetDe, TargetUe,Percentile01_EFFR,Percentile99_EFFR))
 melteffr <- melt(quantilesE2,id="sdate")
 quantileseffr <- ggplot(melteffr,aes(x=sdate,y=value,colour=variable,group=variable)) + 
   geom_point(shape = 16, size = 1) +
   labs(x="Date",  y = "Basis Points (bp)", color = "Rate", shape = "Rate") +  
   scale_y_continuous(breaks = seq(0, 500, by = 50), limits = c(0, 500)) + 
   theme_minimal() + guides(shape = guide_legend(title = "Rate"))
 print(quantileseffr)
```



## Characteristics of the sample distribution

DELETE CHUNK
As shown in Figure \@ref(fig:sampleplot_characteristics), the relationship is evident. Table 
```{r sampleplot_characteristics, tab.cap="Rate characteristics 2016-2023", echo=FALSE, results='asis'}
# boxplots and or quantiles
# Discuss quantiles in (\@ref(fig:fig1),  (\@ref(tab:tab1). box plots (summarizes a data set visually using a five-number summary) or quantile plots, histograms, densities
```


\@ref(tab:sampletable_characteristics) provides a summary of the data.
\```{r sampletable_characteristics, tab.cap="Rate characteristics 2016-2023", echo=FALSE, results='asis'}
# Sample characterisics quantiles and more --------------------------------------------------
k=6 # sample period maybe convert to begn later etc
bgn<-1
edn<-nrow(rrbp)
print(bgn)
print(edn)
sample<-rrbp[bgn:edn,]  # All
sdate<-spread_no_na$sdate
sdatesample<-sdate[bgn:edn]
qsampleE=quantilesE[bgn:edn,] # rate specific metrics 
#str(quantilesE[bgn:edn,])
qsampleT=quantilesT[bgn:edn,]
qsampleB=quantilesB[bgn:edn,]
qsampleS=quantilesS[bgn:edn,]


# Store your data frame in the environment
my_envepisodes$sdate <- sdate
my_envepisodes$sampleE <-qsampleE
my_envepisodes$sampleT <-qsampleT
my_envepisodes$sampleB <-qsampleB
my_envepisodes$sampleS <-qsampleS

Estats <- colMeans(qsampleE[bgn:edn,2:ncol(qsampleE)], na.rm = TRUE)
#Ostats <- colMeans(qsampleO[bgn:edn,,2:ncol(qsampleO)], na.rm = TRUE)
Tstats <- colMeans(qsampleT[bgn:edn,2:ncol(qsampleT)], na.rm = TRUE)
Bstats <- colMeans(qsampleB[bgn:edn,2:ncol(qsampleB)], na.rm = TRUE)
Sstatssample <- colMeans(qsampleS[bgn:edn,2:ncol(qsampleS)], na.rm = TRUE)

# Add NA for targets 
Tstats2<-c(Tstats[1],Tstats[2], TargetUe=NA, TargetDe=NA,Tstats[3],Tstats[4],Tstats[5],Tstats[6])
Bstats2<-c(Bstats[1],Bstats[2], TargetUe=NA, TargetDe=NA,Bstats[3],Bstats[4],Bstats[5],Bstats[6])
Sstats2<-c(Sstats[1],Sstats[2], TargetUe=NA, TargetDe=NA,Sstats[3],Sstats[4],Sstats[5],Sstats[6])

Estats_mat <- matrix(Estats, nrow = 8, ncol = 1)
Tstats_mat <- matrix(Tstats2, nrow = 8, ncol = 1)
Bstats_mat <- matrix(Bstats2, nrow = 8, ncol = 1)
Sstats_mat <- matrix(Sstats2, nrow = 8, ncol = 1)

# Combine the matrices into one matrix
stats_sample <- cbind(Estats_mat, Tstats_mat, Bstats_mat, Sstats_mat)

# Print the combined matrix
print(stats)
# Set row names
rownames(stats_sample) <- c("Rate", "Volume", "Upper target", "Lower target", 
                     "Percentile_01", "Percentile_25", "Percentile_75", "Percentile_99")

# Set column names
colnames(stats_sample) <- c("EFFR", "TGCR", "BGCR",  "SOFR")

# Print the combined matrix
print(stats_sample)
```



(see Figure \@ref(fig:Overnight rates quantiles 2016-2023)
\```{r Overnight rates quantiles, daily sample quantile dataframes, echo=FALSE,  cache=FALSE,results='hide'}
boxplot_sample <- data.frame(
  Group = structure(1:4, .Label = c("EFFR", "TGCR", "BGCR", "SOFR"), class = "factor"),
  y0 = c(min(quantilesE$EFFR), min(quantilesT$TGCR), min(quantilesB$BGCR), min(quantilesS$SOFR)),
  y25 = c(quantile(quantilesE$Percentile25_EFFR, 0.25), quantile(quantilesT$Percentile25_TGCR, 0.25), quantile(quantilesB$Percentile25_BGCR, 0.25), quantile(quantilesS$Percentile25_SOFR, 0.25)),
  y50 = c(median(quantilesE$EFFR), median(quantilesT$TGCR), median(quantilesB$BGCR), median(quantilesS$SOFR)),
  y75 = c(quantile(quantilesE$Percentile75_EFFR, 0.75), quantile(quantilesT$Percentile75_TGCR, 0.75), quantile(quantilesB$Percentile75_BGCR, 0.75), quantile(quantilesS$Percentile75_SOFR, 0.75)),
  y100 = c(max(quantilesE$EFFR), max(quantilesT$TGCR), max(quantilesB$BGCR), max(quantilesS$SOFR)))
```


(see Figure \@ref(fig:Overnight rates boxplot)
\```{r overnight rates boxplot, fig.cap= "Daily rates 2016-2023", echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=8, eval=TRUE, cache=FALSE, dev='png', out.width='80%', , results='hide', fig.pos='H'} 
 bsample<-ggplot(boxplot_sample , aes(x=Group,
                   ymin = y0,
                   ymax = y100,
                   lower = y25,
                   middle = y50,
                   upper = y75,
                   fill = Group)) +
    geom_boxplot(stat = "identity")   
  print(bsample)
```


## Stats and dispersion
The interquartile range (IQR), a useful measure of variability for skewed distributions. As the 75th minus the 25thpercentile, the IQR demonstrate where 50 percent of the data lie, showing how clustered are the values.Use the IQR to assess the variability where most of your values lie. Because it’s based on the middle half of the distribution, it’s less influenced by extreme values. Larger values indicate that the central portion of your data spread out further. Conversely, smaller values show that the middle values cluster more tightly. 

When measuring variability, statisticians prefer using the interquartile range instead of the full data range because extreme values and outliers affect it less. Typically, use the IQR with a measure of central tendency, such as the median, to understand your data’s center and spread. This combination creates a fuller picture of your data’s distribution.

Unlike the more familiar mean and standard deviation, the interquartile range and the median are robust measures. Outliers do not strongly influence either statistic because they don’t depend on every value. Additionally, like the median, the interquartile range is superb for skewed distributions. For normal distributions, you can use the standard deviation to determine the percentage of observations that fall specific distances from the mean. However, that doesn’t work for skewed distributions, and the IQR is an excellent alternative.

The range of the data, here the 99th minus the first percentile illustrate outliers, showing where 98 percent of the data lie. The range is the spread of your data from the lowest to the highest value in the distribution. It is a commonly used measure of variability. While a large range means high variability, a small range means low variability in a distribution.
\url{https://statisticsbyjim.com/basics/interquartile-range/}

\@ref(tab:sampletable_metrics) provides a summary of the data.
\```{r sampletable_metrics, tab.cap="Simple statistics 2016-2023", echo=FALSE, results='asis'}
# Sample data metrics --------------------------------------------------------
k=6 # sample period maybe convert to begn later etc
bgn<-1
edn<-nrow(rrbp)
print(bgn)
print(edn)
sample<-rrbp[bgn:edn,]  # All
sdate<-spread_no_na$sdate
sdatesample<-sdate[bgn:edn]
qsampleE=quantilesE[bgn:edn,] # rate specific metrics 
qsampleT=quantilesT[bgn:edn,]
qsampleB=quantilesB[bgn:edn,]
qsampleS=quantilesS[bgn:edn,]

categories = c('EFFR', 'TGCR', 'BGCR', 'SOFR')
median_values = c(Estats[1], Tstats[1], Bstats[1], Sstats[1])
iqr_values = c(Estats[7] - Estats[6], Tstats[5] - Tstats[4], Bstats[5] - Bstats[4], Sstats[5] - Sstats[4]) 
range_values = c(Estats[8] - Estats[5],  Tstats[6] - Tstats[3], Bstats[6] - Bstats[3], Sstats[6] - Sstats[3])
volume=c(Estats[2],Tstats[2],Bstats[2],Sstats[2])
ratesamplestats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE=range_values, VOLUME=volume)
print( xtable(ratesamplestats), include.rownames = FALSE )
```

<!--
In this setup:

- The `data_processing_chunk` is a code chunk that contains all the assignments and data processing steps.
- It ensures that the variables (`sdate`, `qsampleE`, `qsampleT`, `qsampleB`, `qsampleS`) are properly assigned within the R Markdown document.
- You can include any necessary print statements or inspections to verify the data.
- Finally, it updates the `my_envepisodes` environment with the processed data.

Placing the assignments within a code chunk ensures that they are executed in the correct sequence and context within your R Markdown document.
-->

The average weighted median for the EFFR during the sample is 157 basis points (pb). The average weighted medians of other money market rates cluster around the median EFFR - TGCR,BGCR and SOFR 123-134 bp. Daily transaction volume in SOFR (billions of dollars ) is around 754 billion. The money market volumes in TGCR and BGCR around 300 billion.  Daily trade volume in EFFR is a low 79 billion.  The percentiles of rates reflect the same clustering around the EFFR and SOFR as do their medians.

The interquartile range (IQR) indicated half the data are clustered tightly around median rates. Half the the EFFR, TGCR, and BGCR rates are within or little more than one basis point. The SOFR, 4 basis points.

Estimating moments of the data show overnight rates are highly skewed with fatter tails.

The interquartile range for the money market rates TGCR and BGCR show 50 percent of the data are closely ? around around the mean.  The TGCR and BGCR are under 1 bp, smaller than the 1.56 bp of the policy rate, EFFR. The SOFR has the largest IQR, some 4.2 bp. The OBFR,designed to track the policy rate. is similar to the EFFR.

The range of the data, the 99th minus the first percentile, is more sensitive to outliers. The range of the EFFR and the money market rates, TGCR, BGCR, and SOFR are some 14 to 16 basis points.


## volatility
Conditional heteroskedasticity (\url{https://www.investopedia.com/terms/h/heteroskedasticity.asp})
Heteroskedasticity often arises in two forms: conditional and unconditional. Conditional . 
? Do my plots of log percent changes, and standard deviation of log percent changes serve

I describe volatility as log percent changes in rates since data are average weighted median values. the median is the best measure of central tendency because it’s the value in the middle of values ordered from low to high. The rate data from the Federal Reserve Data Download Program are average volume weighted medians and select percentiles. The median rather than the mean better represent daily rates since the data have outliers or extreme values. The median and interquartile range, the 75 percentile minus the 25 percentile, summarize a typical value and the variability in the sample.  The range of the data, the 99th ,minus the first percentile, is more sensitive to outliers.

<!--
Mean reversion
Benzoni identify mean reversion around a central tendency. The stochastic mean  allows a relatively fast mean-reversion of the short rate around a highly persistent time-varying central tendency process. 
Jumps are integral to the quality of fit and relieve the stochastic volatility factor from accommodating extreme outlier behavior. 
-->

Volatility in the sample, the standard deviation of log percent change
2 plots: log percent change, and stadard deviation of log percent change over arbitrary periods, i.e number of days.

(see Figure \@ref(fig:volatility log percent change)
```{r volatility log percent change, fig.cap= "Percent change Daily rates 2016-2023", echo=FALSE}
df <- as.data.frame(sample_measure1)
begn<- c(1, 859, 923,  1014, 1519, 1)
endn<- c(858, 922, 1013, 1518, 1957, 1957)
bgn<-begn[6]
edn<-endn[6]-1
df$sdate <- sdate[bgn:edn]
```

This measure of volatility needs work since str(sdrates) 1956 observations five day rolling average. But Rolling every five days so maybe ok

(see Figure \@ref(fig:stdev volatility 5day)
```{r stdev volatility 5day, fig.cap= "Rollign standard deviation of percent change daily rates 2016-2023", echo=FALSE}
# multivariate rolling stdev-------------------
library(zoo)
nday <- 5
#sdate <- df$sdate  # Assuming sdate is the column containing dates

# Select the first five columns of measure1 for calculation, excluding sdate
#measure_subset <- measure1[, 1:5]
# Apply rolling standard deviation directly on the data frame
sdrates <- rollapply(sample_measure1, width = nday, FUN = sd, align = "right", fill = NA)
sdrates[is.na(sdrates)]<-0

# Convert the result to a data frame
sdrates <- as.data.frame(sdrates)
sdrates$sdate <-sdate[1:1956]

mxq = max(sdrates[,1:5]) # 1.089607
mnq = min(sdrates[,1:5]) # 0


meltsdrates <- melt(sdrates,id="sdate")
plotsdrates <- ggplot(meltsdrates,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_point(size = 1, shape = 16) +
  labs(x="Date",  y = "Pct Change Basis Points (bp)", color = "Std dev pct change", shape = "Std dev pct change") +  
  scale_y_continuous(breaks = seq(0, mxq, by = .05), limits = c(0, mxq)) + 
  theme_minimal() + guides(shape = guide_legend(title = "Pct change"))
# + theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5))
print(plotsdrates)
```

## Episodes that suggest different policy regimes
#Discuss plots and statistics of episodes daily epoch rates

## Episodes 
The pattern in rate data from 2016 t0 2023 suggests the following episodes that may correspond with differing policy regimes or external shocks (Figure \@ref(fig:EFFR, FOMC targets 2016-2023-plot):


normalcy
adjust 
covid 
zlb 
inflation 


```{r, episode parameters, echo=FALSE}
# normalcy <-rrbp %>% slice(1:858)
# adjust <-rrbp %>% slice(859:922)
# covid <-rrbp %>% slice(923:1013)
# zlb <-rrbp %>% slice(1014:1518)
# inflation <-rrbp %>% slice(1519:1957)

#1. normalcy   3/4/2016		7/31/2019      4  859
#2. mid cycle adjustment 8/1/2019 - 10/31/2019 737660 
#860 - 923
#3. covid 11/1/2019	    3/16/2020   924  1032
#4. zlb         3/17/2020- 3/16/2022     1032-1516
#5. Taming inflation 03/17/2022 - 12/14/2023 1517-1957
#NO! inflation   5/5/2022		12/14/2023 1517  1714
# Redo -3 for each position for nrow=1957

 
# ------------------------ episode stats
# 1. average rates and volumes
# 2. IQR
# 3. all quantiles
# 3. Distance from target ?? or average:  below target, above target
begn<- c(1, 859, 923,  1014, 1519, 1)
endn<- c(858, 922, 1013, 1518, 1957, 1957)


# Define your plot
start_dates <-sdate[begn[k]]
end_dates <-sdate[endn[k]]
start_dates_strings <- as.character(start_dates)
end_dates_strings <- as.character(end_dates)
#text(x = x_coordinate, y = y_coordinate, labels = paste("Start Date:", start_dates_strings[1]), pos = 1)
#text(x = x_coordinate, y = y_coordinate, labels = paste("End Date:", end_dates_strings[1]), pos = 1)
```

## episode characteristics
<!--
'data.frame':	1957 obs. of  9 variables:
 $ sdate            : Date, format: "2016-03-04" "2016-03-07" "2016-03-08" "2016-03-09" ...
 $ EFFR             : num  36 36 36 36 36 36 36 37 37 37 ...
 $ VolumeEFFR       : num  75 72 72 75 72 68 67 67 63 63 ...
 $ TargetUe         : num  50 50 50 50 50 50 50 50 50 50 ...
 $ TargetDe         : num  25 25 25 25 25 25 25 25 25 25 ...
 $ Percentile01_EFFR: num  34 34 32 34 35 35 35 35 35 36 ...
 $ Percentile25_EFFR: num  36 36 36 36 36 36 36 36 36 36 ...
 $ Percentile75_EFFR: num  37 37 37 37 37 37 37 37 37 37 ...
 $ Percentile99_EFFR: num  52 50 50 52 75 50 50 50 50 50 ...)
Table \@ref(tab:norm_characteristics) provides a summary of the data.
-->



\@ref(tab:normtable_characteristics) provides a summary of the data.
\```{r normtable_characteristics, tab.cap="Rate characteristics normalcy 3/4/2016-7/31/2019", echo=FALSE, results='asis'}
k=1 # normalcy period

bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
norm<-rrbp[bgn:edn,]  # All
sdatenorm<-sdate[bgn:edn]

#qnormE=quantilesE[bgn:edn,] # rate specific metrics  -->
#qnormT=quantilesT[bgn:edn,] -->
#qnormB=quantilesB[bgn:edn,] -->
#qnormS=quantilesS[bgn:edn,] -->
str(my_envepisodes$normE)

# Store your data frame in the environment
my_envepisodes$sdatenorm <-sdatenorm
my_envepisodes$normE <-qnormE
my_envepisodes$normT <-qnormT
my_envepisodes$normB <-qnormB
my_envepisodes$normS <-qnormS

my_env <-my_envepisodes

Estatsnorm <- colMeans(normE[bgn:edn,2:ncol(qnormE)], na.rm = TRUE)
#Ostatsnorm <- colMeans(normO[bgn:edn,,2:ncol(qnormO)], na.rm = TRUE)
Tstatsnorm <- colMeans(normT[bgn:edn,2:ncol(qnormT)], na.rm = TRUE)
Bstatsnorm <- colMeans(normB[bgn:edn,2:ncol(qnormB)], na.rm = TRUE)
Sstatsnorm <- colMeans(normS[bgn:edn,2:ncol(qnormS)], na.rm = TRUE)

# Add NA for targets 
Tstatsnorm2<-c(Tstatsnorm[1],Tstatsnorm[2], TargetUe=NA, TargetDe=NA,Tstatsnorm[3],Tstatsnorm[4],Tstatsnorm[5],Tstatsnorm[6])
Bstatsnorm2<-c(Bstatsnorm[1],Bstatsnorm[2], TargetUe=NA, TargetDe=NA,Bstatsnorm[3],Bstatsnorm[4],Bstatsnorm[5],Bstatsnorm[6])
Sstatsnorm2<-c(Sstatsnorm[1],Sstatsnorm[2], TargetUe=NA, TargetDe=NA,Sstatsnorm[3],Sstatsnorm[4],Sstatsnorm[5],Sstatsnorm[6])


# episode characteristics NEW---------------------------------
Estatsnorm_mat <- matrix(Estatsnorm, nrow = 8, ncol = 1)
Tstatsnorm_mat <- matrix(Tstatsnorm2, nrow = 8, ncol = 1)
Bstatsnorm_mat <- matrix(Bstatsnorm2, nrow = 8, ncol = 1)
Sstatsnorm_mat <- matrix(Sstatsnorm2, nrow = 8, ncol = 1)

# Combine the matrices into one matrix
charnorm <- cbind(Estatsnorm_mat, Tstatsnorm_mat, Bstatsnorm_mat, Sstatsnorm_mat)
print(charnorm)
# Set row names
rownames(charnorm) <- c("Rate", "Volume", "Upper target", "Lower target", 
                     "Percentile_01", "Percentile_25", "Percentile_75", "Percentile_99")

colnames(charnorm) <- c("EFFR", "TGCR", "BGCR",  "SOFR")
print(charnorm)
```

## episode stats --------------------------------------------------------
Table \@ref(tab:normtable_metrics) provides a summary of the data.
\```{r normtable_metrics, tab.cap="Rate statistics normalcy 3/04/2016- 7/31/2019", echo=FALSE, results='asis'}
k <- 1 # normalcy period
bgn <- begn[k]
edn <- endn[k]
print(bgn)
print(edn)
norm <- rrbp[bgn:edn,]  # All
sdatenorm <- sdate[bgn:edn]

qnormE <- quantilesE[bgn:edn,] # rate specific metrics 
qnormT <- quantilesT[bgn:edn,]
qnormB <- quantilesB[bgn:edn,] 
qnormS <- quantilesS[bgn:edn,] 

normE <- qnormE
normT <- qnormT
normB <- qnormB
normS <- qnormS

# Store your data frame in the environment
my_envepisodes$sdatenorm <- sdatenorm
my_envepisodes$normE <- normE
my_envepisodes$normT <- normT
my_envepisodes$normB <- normB
my_envepisodes$normS <- normS

my_env <- my_envepisodes

Estatsnorm <- colMeans(normE[bgn:edn,2:ncol(qnormE)], na.rm = TRUE)
#Ostatsnorm <- colMeans(normO[bgn:edn,,2:ncol(qnormO)], na.rm = TRUE)
Tstatsnorm <- colMeans(normT[bgn:edn,2:ncol(qnormT)], na.rm = TRUE)
Bstatsnorm <- colMeans(normB[bgn:edn,2:ncol(qnormB)], na.rm = TRUE)
Sstatsnorm <- colMeans(normS[bgn:edn,2:ncol(qnormS)], na.rm = TRUE)

# Assuming categories is defined somewhere in your code
categories <- c("EFFR", "TGCR", "BGCR", "SOFR")
# Assuming Estatsnorm, Tstatsnorm, Bstatsnorm, and Sstatsnorm are correctly calculated
median_values <- c(Estatsnorm[1], Tstatsnorm[1], Bstatsnorm[1], Sstatsnorm[1])
iqr_values <- c(Estatsnorm[7] - Estatsnorm[6], Tstatsnorm[5] - Tstatsnorm[4], Bstatsnorm[5] - Bstatsnorm[4], Sstatsnorm[5] - Sstatsnorm[4])
range_values <- c(Estatsnorm[8] - Estatsnorm[5], Tstatsnorm[6] - Tstatsnorm[3], Bstatsnorm[6] - Bstatsnorm[3], Sstatsnorm[6] - Sstatsnorm[3])
volume_values <- c(Estatsnorm[2], Tstatsnorm[2], Bstatsnorm[2], Sstatsnorm[2])
# Create the data frame
normstats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE = range_values, VOLUME = volume_values)

# Print the table using xtable
print(xtable(normstats), include.rownames = FALSE)

#Implement proper error handling mechanisms in your RMD file. For example, you can use `tryCatch()` to #handle potential errors during the creation of `normstats` and print informative messages if any error #occurs.

# Attempt to create normstats, and handle any errors
tryCatch({
  # Your existing code to create normstats
  normstats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE = range_values, VOLUME = volume_values)
}, error = function(e) {
  # Print error message
  cat("Error creating normstats:", e$message, "\n")
})
```


see Figure \@ref(fig:EFFR during normalcy period  3/4/2016-7/31/2019-plot)
\```{r, fig.cap= "EFFR during normalcy period  3/4/2016-7/31/2019"}
sdate<-sdatenorm
maxr-max(norm[,2:ncol(norm)]) #224
maxr=250
minr-min(norm[,2:ncol(norm)]) #0
meltrates_norm <- melt(norm,id="sdate")
rates_norm <- ggplot(meltrates_norm,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_point(shape = 16, size = 1) +
  labs(x="", y = "Basis Points (bp)", color = "Rate", shape = "Rate") +  
  scale_y_continuous(breaks = seq(minr, maxr, by = 25), limits = c(minr, maxr)) + 
  theme_minimal()+ guides(shape = guide_legend(title = "Rate"))
print(rates_norm)
#ggsave("C:/Users/Owner/Documents/Research/OvernightRates/Figures/rates_norm.pdf")
#ggsave("C:/Users/Owner/Documents/Research/OvernightRates/Figures/rates_norm.png")
```

The average weighted median for the EFFR during the normalcy period is 124 basis points (pb). The average weighted medians of other money market rates cluster around the median EFFR - TGCR,BGCR and SOFR 84-85 bp. Daily transaction volume in SOFR (billions of dollars ) is around 350 billion. The money market volumes in TGCR and BGCR around 158-166 billion.  Daily trade volume in EFFR is a low 76 billion.  The percentiles of rates reflect the same clustering around the EFFR and SOFR as do their medians.

The interquartile range for the money market rates TGCR and BGCR are 0.22-0,23 bp, smaller than the 1.22 bp of the policy rate, EFFR. The SOFR has the largest IQR, some 3.3 bp. The OBFR,designed to track the policy rate. is similar to the EFFR.

The range of the data, the 9th ,minus the first percentile, is more sensitive to outliers/ Some 17.65 basis points for the EFFR and 5.62-8.53the money market rates, TGCR, BGCR, and SOFR. 


The average weighted median for the EFFR during the nomrmalcy period is 157 basis points (pb). The average weighted medians of other money market rates cluster around the median EFFR - TGCR,BGCR and SOFR 123-134 bp. Daily transaction volume in SOFR (billions of dollars ) is around 754 billion. The money market volumes in TGCR and BGCR around 300 billion.  Daily trade volume in EFFR is a low 79 billion.  The percentiles of rates reflect the same clustering around the EFFR and SOFR as do their medians.

The interquartile range for the money market rates TGCR and BGCR are under 1 bp, smaller than the 1.56 bp of the policy rate, EFFR. The SOFR has the largest IQR, some 4.2 bp. The OBFR,designed to track the policy rate. is similar to the EFFR.

The range of the data, the 9th ,minus the first percentile, is more sensitive to outliers/ Some 14 to 16 basis points for the EFFR and the money market rates, TGCR, BGCR, and SOFR. 


Adjust--------------------------------
Table \@ref(tab:adjust_characteristics) provides a summary of the data.
\```{r adjust_characteristics, tab.cap="Rate characteristics  mid cycle adjustment 8/1/2019-10/31/2019", echo=FALSE, results='asis'}
k=2  
bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
adjust<-rrbp[bgn:edn,]  # rates plot
#sdate<-spread_no_na$sdate  I had to do this
sdateadj<-sdate[bgn:edn]

qadjE=quantilesE[bgn:edn,]
qadjT=quantilesT[bgn:edn,]
qadjB=quantilesB[bgn:edn,]
qadjS=quantilesS[bgn:edn,]
adjust<-rrbp[bgn:edn,]  # All 
my_envepisodes$adjustE <-qadjE
my_envepisodes$adjustT <-qadjT
my_envepisodes$adjustB <-qadjB
my_envepisodes$adjustS <-qadjS


# Store your data frame in the environment
my_envepisodes$sdateadj <-sdateadj
my_envepisodes$adjE <-qadjE
my_envepisodes$adjT <-qadjT
my_envepisodes$adjB <-qadjB
my_envepisodes$adjS <-qadjS

# stats_adjust<-colMeans(quantilesE[bgn_edn,]     #adjust
Estatsadj <- colMeans(qadjE[bgn:edn,2:ncol(qadjE)], na.rm = TRUE)
#Ostatsadj <- colMeans(qadjO[bgn:edn,,2:ncol(qadjO)], na.rm = TRUE)
Tstatsadj <- colMeans(qadjT[bgn:edn,2:ncol(qadjT)], na.rm = TRUE)
Bstatsadj <- colMeans(qadjB[bgn:edn,2:ncol(qadjB)], na.rm = TRUE)
Sstatsadj <- colMeans(qadjS[bgn:edn,2:ncol(qadjS)], na.rm = TRUE)

# Add NA for targets 
Tstatsadj2<-c(Tstatsadj[1],Tstatsadj[2], TargetUe=NA, TargetDe=NA,Tstatsadj[3],Tstatsadj[4],Tstatsadj[5],Tstatsadj[6])
Bstatsadj2<-c(Bstatsadj[1],Bstatsadj[2], TargetUe=NA, TargetDe=NA,Bstatsadj[3],Bstatsadj[4],Bstatsadj[5],Bstatsadj[6])
Sstatsadj2<-c(Sstatsadj[1],Sstatsadj[2], TargetUe=NA, TargetDe=NA,Sstatsadj[3],Sstatsadj[4],Sstatsadj[5],Sstatsadj[6])


# episode characteristics NEW---------------------------------
Estatsadj_mat <- matrix(Estatsadj, nrow = 8, ncol = 1)
Tstatsadj_mat <- matrix(Tstatsadj2, nrow = 8, ncol = 1)
Bstatsadj_mat <- matrix(Bstatsadj2, nrow = 8, ncol = 1)
Sstatsadj_mat <- matrix(Sstatsadj2, nrow = 8, ncol = 1)

# Combine the matrices into one matrix
charadj <- cbind(Estatsadj_mat, Tstatsadj_mat, Bstatsadj_mat, Sstatsadj_mat)
print(charadj)
# Set row names
rownames(charadj) <- c("Rate", "Volume", "Upper target", "Lower target", 
                     "Percentile_01", "Percentile_25", "Percentile_75", "Percentile_99")

colnames(charadj) <- c("EFFR", "TGCR", "BGCR",  "SOFR")
print(charadj)
```

Table \@ref(tab:adjusttable_metrics) provides a summary of the data.
\```{r adjusttable_metrics, tab.cap="Rate statistics  mid cycle adjustment 8/1/2019-10/31/2019", echo=FALSE, results='asis'}
median_values = c(Estatsadj[1], Tstatsadj[1], Bstatsadj[1], Sstatsadj[1])
iqr_values = c(Estatsadj[7] - Estatsadj[6],  Tstatsadj[5] - Tstatsadj[4], Bstatsadj[5] - Bstatsadj[4], Sstatsadj[5] - Sstatsadj[4])
range_values = c(Estatsadj[8] - Estatsadj[5], Tstatsadj[6] - Tstatsadj[3], Bstatsadj[6] - Bstatsadj[3], Sstatsadj[6] - Sstatsadj[3])
volume_values = c(Estatsadj[2], Tstatsadj[2], Bstatsadj[2], Sstatsadj[2])
adjstats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE=range_values, VOLUME = volume_values)
print(xtable(adjstats),include.rownames = FALSE)

```

 (see Figure \@ref(fig:EFFR during mid cycle adjustment 8/1/2019-10/31/2019)
\```{r, fig.cap= "EFFR during mid cycle adjustment 8/1/2019-10/31/2019}
sdate<-sdateadj
maxr=max(adjust[,2:ncol(adjust)]) # drop outlier 525
maxr=260
minr=min(adjust[,2:ncol(adjust)])
meltrates_adjust <- melt(adjust,id="sdate")
rates_adjust <- ggplot(meltrates_adjust,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_point(shape=16,size=1) +
  labs(x="", y = "Basis Points (bp)", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(minr, maxr, by = 25), limits = c(minr, maxr)) + 
  theme_minimal() + guides(shape = guide_legend(title = "Rate"))
print(rates_adjust)
```

The average weighted median for all over night rates are tightly clustered around 148.64-151.18c during covid. The average weighted medians of other money market rates cluster around the median EFFR - TGCR,BGCR and SOFR 123-134 bp. Daily transaction volume in SOFR (billions of dollars ) is around 1086 billion. The money market volumes in TGCR and BGCR around 402-423  billion.  Daily trade volume in EFFR is a low 71.7 billion.  The percentiles of rates reflect the same clustering around the EFFR and SOFR as do their medians.

The interquartile range for the money market rates TGCR and BGCR are under 1 bp, smaller than the 1.56 bp of the policy rate, EFFR. The SOFR has the largest IQR, some 4.2 bp. The OBFR,designed to track the policy rate. is similar to the EFFR.

The range of the data, the 9th ,minus the first percentile, is more sensitive to outliers/ Some 14 to 16 basis points for the EFFR and the money market rates, TGCR, BGCR, and SOFR. 
  

Table \@ref(tab:covidtable_characteristics) provides a summary of the data.
```{r covidtable_characteristics, tab.cap="Rate characteristic covid 11/1/2019-3/16/2020 ", echo=FALSE, cache=FALSE, results='asis'}
# Covid period --------------------------------------
k=3  
begn<-c(1,859,923,1014,1519,1)
endn<-c(858,922,1013,1518,1957,1957)
bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
covid<-rrbp[bgn:edn,]
sdatecovid<-sdate[bgn:edn]

qcovidE=quantilesE[bgn:edn,]
qcovidT=quantilesT[bgn:edn,]
qcovidB=quantilesB[bgn:edn,]
qcovidS=quantilesS[bgn:edn,]


# Store your data frame in the environment
my_envepisodes$sdatecovid <-sdatecovid
my_envepisodes$covidE <-qcovidE
my_envepisodes$covidT <-qcovidT
my_envepisodes$covidB <-qcovidB
my_envepisodes$covidS <-qcovidS

covidE<- qcovidE
#covidO<- qcovidO
covidT<- qcovidT
covidB<- qcovidB
covidS<- qcovidS

 EC1<-mean(covidE[,2]) # 150.4615
 EC2<-mean(covidE[,3]) # 71.7033
 EC3<-mean(covidE[,4]) # 168.956
 EC4<-mean(covidE[,5]) # 143.956
 EC5<-mean(covidE[,6]) # 145.8022
 EC6<-mean(covidE[,7]) # 149.5275
 EC7<-mean(covidE[,8]) # 151.8791
 EC8<-mean(covidE[,9]) #  157.5165
covidE[, 2:ncol(qcovidE)]

Estatscovid <- colMeans(covidE[,1:ncol(qcovidE)-1], na.rm = TRUE)
Tstatscovid <- colMeans(covidT[,2:ncol(qcovidT)], na.rm = TRUE)
Bstatscovid <- colMeans(covidB[,2:ncol(qcovidB)], na.rm = TRUE)
Sstatscovid <- colMeans(covidS[,2:ncol(qcovidS)], na.rm = TRUE)

# Add NA for targets 
Estatscovid2<-c(Estatscovid[1],Estatscovid[2], TargetUe=NA, TargetDe=NA,Estatscovid[3],Estatscovid[4],Estatscovid[5],Estatscovid[6])
Tstatscovid2<-c(Tstatscovid[1],Tstatscovid[2], TargetUe=NA, TargetDe=NA,Tstatscovid[3],Tstatscovid[4],Tstatscovid[5],Tstatscovid[6])
Bstatscovid2<-c(Bstatscovid[1],Bstatscovid[2], TargetUe=NA, TargetDe=NA,Bstatscovid[3],Bstatscovid[4],Bstatscovid[5],Bstatscovid[6])
Sstatscovid2<-c(Sstatscovid[1],Sstatscovid[2], TargetUe=NA, TargetDe=NA,Sstatscovid[3],Sstatscovid[4],Sstatscovid[5],Sstatscovid[6])


# episode characteristics NEW---------------------------------
Estatscovid_mat <- matrix(Estatscovid, nrow = 8, ncol = 1)
Tstatscovid_mat <- matrix(Tstatscovid2, nrow = 8, ncol = 1)
Bstatscovid_mat <- matrix(Bstatscovid2, nrow = 8, ncol = 1)
Sstatscovid_mat <- matrix(Sstatscovid2, nrow = 8, ncol = 1)

# Combine the matrices into one matrix
charcovid <- cbind(Estatscovid_mat, Tstatscovid_mat, Bstatscovid_mat, Sstatscovid_mat)
print(charcovid)
# Set row names
rownames(charcovid) <- c("Rate", "Volume", "Upper target", "Lower target", 
                     "Percentile_01", "Percentile_25", "Percentile_75", "Percentile_99")

colnames(charcovid) <- c("EFFR", "TGCR", "BGCR",  "SOFR")
print(charcovid)
```

Table \@ref(tab:covid_stats) provides a summary of the data.
```{r covid_stats, tab.cap="Rate statistics covid 11/1/2019-3/16/2020" , echo=FALSE,cache=FALSE, results='asis'}
# Define categories
categories <- c("Estatscovid", "Tstatscovid", "Bstatscovid", "Sstatscovid")

median_values = c(Estatscovid[1], Tstatscovid[1], Bstatscovid[1], Sstatscovid[1])
iqr_values = c(Estatscovid[7] - Estatscovid[6], Tstatscovid[5] - Tstatscovid[4], Bstatscovid[5] - Bstatscovid[4], Sstatscovid[5] - Sstatscovid[4])
range_values = c(Estatscovid[8] - Estatscovid[5], Tstatscovid[6] - Tstatscovid[3], Bstatscovid[6] - Bstatscovid[3], Sstatscovid[6] - Sstatscovid[3])
volume_values = c(Estatscovid[2], Tstatscovid[2], Bstatscovid[2], Sstatscovid[2])
covidstats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE=range_values, VOLUME = volume_values)
print(xtable(covidstats), include.rownames = FALSE)
```


 (see Figure \@ref(fig:EFFR during covid 11/1/2019-3/16/2020)
\```{r, fig.cap= "EFFR during covid 11/1/2019-3/16/2020"}
sdate<-sdatecovid
maxr=max(covid[,2:ncol(covid)]) # drop outlier 165
maxr=maxr+5
minr=min(covid[,2:ncol(covid)]) #23
minr=minr-3
meltrates_covid <- melt(covid,id="sdate")
rates_covid <- ggplot(meltrates_covid,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_point(shape=16,size=1) +
  labs(x="",  y = "Basis Points (bp)", color = "Rate", shape = "Rate") +  
  scale_y_continuous(breaks = seq(minr, maxr, by = 25), limits = c(minr, maxr)) + 
  theme_minimal() + guides(shape = guide_legend(title = "Rate"))
print(rates_covid)
```


  
  The average weighted median for the EFFR during the zero lower bound is 8 basis points (pb). The average weighted medians of other money market rates cluster around the median EFFR - TGCR,BGCR and SOFR? 4,6 bp. Daily transaction volume in SOFR (billions of dollars ) is around 754 billion. The money market volumes in TGCR and BGCR around 353-376 billion.  Daily trade volume in EFFR is a low 69 billion.  The percentiles of rates reflect the same clustering around the EFFR and SOFR as do their medians.

The interquartile range for the money market rates TGCR and BGCR are .12-.15 bp, smaller than the 1.54 bp of the policy rate, EFFR. The SOFR has the largest IQR, some 4.2 bp. The OBFR,designed to track the policy rate. is similar to the EFFR.

The range of the data, the 9th ,minus the first percentile, is more sensitive to outliers, Some 11 to 12 basis points for  the money market rates, TGCR, BGCR 7.33 for the EEFR and 14.44 for SOFR. 

<!--
# Clear all objects from the global environment
rm(list = ls())

# Trigger garbage collection
gc()
-->


zlb-----------------------------------
Table \@ref(tab:zlbtable_characteristics) provides a summary of the data.
\```{r zlb_characteristics, tab.cap="Rate characteristics zero lower bound 3/17/2020-3/16/2022", echo=FALSE, eval=TRUE, cache=FALSE<results='asis'}
# zero lower bound period --------------------------------------
k=4  
begn<-c(1,859,923,1014,1519,1)
endn<-c(858,922,1013,1518,1957,1957)
bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
zlb<-rrbp[bgn:edn,]
sdatezlb<-sdate[bgn:edn]

qzlbE=quantilesE[bgn:edn,]
qzlbT=quantilesT[bgn:edn,]
qzlbB=quantilesB[bgn:edn,]
qzlbS=quantilesS[bgn:edn,]


# Store your data frame in the environment
my_envepisodes$sdatezlb <-sdatezlb
my_envepisodes$zlbE <-qzlbE
my_envepisodes$zlbT <-qzlbT
my_envepisodes$zlbB <-qzlbB
my_envepisodes$zlbS <-qzlbS

# my_envepisodes$zlbE

# stats_zlbust<-colMeans(quantilesE[bgn_edn,]     #zlbust
Estatszlb <- colMeans(qzlbE[,2:ncol(qzlbE)], na.rm = TRUE)
#Ostatszlb <- colMeans(qzlbO[,2:ncol(qzlbO)], na.rm = TRUE)
Tstatszlb <- colMeans(qzlbT[,2:ncol(qzlbT)], na.rm = TRUE)
Bstatszlb <- colMeans(qzlbB[,2:ncol(qzlbB)], na.rm = TRUE)
Sstatszlb <- colMeans(qzlbS[,2:ncol(qzlbS)], na.rm = TRUE)

# Add NA for targets 
Tstatszlb2<-c(Tstatszlb[1],Tstatszlb[2], TargetUe=NA, TargetDe=NA,Tstatszlb[3],Tstatszlb[4],Tstatszlb[5],Tstatszlb[6])
Bstatszlb2<-c(Bstatszlb[1],Bstatszlb[2], TargetUe=NA, TargetDe=NA,Bstatszlb[3],Bstatszlb[4],Bstatszlb[5],Bstatszlb[6])
Sstatszlb2<-c(Sstatszlb[1],Sstatszlb[2], TargetUe=NA, TargetDe=NA,Sstatszlb[3],Sstatszlb[4],Sstatszlb[5],Sstatszlb[6])


# episode characteristics NEW---------------------------------
Estatszlb_mat <- matrix(Estatszlb, nrow = 8, ncol = 1)
Tstatszlb_mat <- matrix(Tstatszlb2, nrow = 8, ncol = 1)
Bstatszlb_mat <- matrix(Bstatszlb2, nrow = 8, ncol = 1)
Sstatszlb_mat <- matrix(Sstatszlb2, nrow = 8, ncol = 1)

# Combine the matrices into one matrix
charzlb <- cbind(Estatszlb_mat, Tstatszlb_mat, Bstatszlb_mat, Sstatszlb_mat)
print(charzlb)
# Set row names
rownames(charzlb) <- c("Rate", "Volume", "Upper target", "Lower target", 
                     "Percentile_01", "Percentile_25", "Percentile_75", "Percentile_99")

colnames(charzlb) <- c("EFFR", "TGCR", "BGCR",  "SOFR")
print(charzlb)
```


Table \@ref(tab:zlbtable_metrics) provides a summary of the data.
```{r zlb_metrics, tab.cap="Rate statistics zero lower bound 3/17/2020-3/16/2022", echo=FALSE, results='asis'}
# Define categories
categories <- c("Estatszlb", "Tstatszlb", "Bstatszlb", "Sstatszlb")
k=4  
begn<-c(1,859,923,1014,1519,1)
endn<-c(858,922,1013,1518,1957,1957)
bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
zlb<-rrbp[bgn:edn,]
sdatezlb<-sdate[bgn:edn]

qzlbE=quantilesE[bgn:edn,]
qzlbT=quantilesT[bgn:edn,]
qzlbB=quantilesB[bgn:edn,]
qzlbS=quantilesS[bgn:edn,]


# Store your data frame in the environment
my_envepisodes$sdatezlb <-sdatezlb
my_envepisodes$zlbE <-qzlbE
my_envepisodes$zlbT <-qzlbT
my_envepisodes$zlbB <-qzlbB
my_envepisodes$zlbS <-qzlbS

# my_envepisodes$zlbE

# stats_zlbust<-colMeans(quantilesE[bgn_edn,]     #zlbust
Estatszlb <- colMeans(qzlbE[,1:ncol(qzlbE)-1], na.rm = TRUE)
#Ostatszlb <- colMeans(qzlbO[,2:ncol(qzlbO)], na.rm = TRUE)
Tstatszlb <- colMeans(qzlbT[,2:ncol(qzlbT)], na.rm = TRUE)
Bstatszlb <- colMeans(qzlbB[,2:ncol(qzlbB)], na.rm = TRUE)
Sstatszlb <- colMeans(qzlbS[,2:ncol(qzlbS)], na.rm = TRUE)

median_values = c(Estatszlb[1], Tstatszlb[1], Bstatszlb[1], Sstatszlb[1])
iqr_values = c(Estatszlb[7] - Estatszlb[6], Tstatszlb[5] - Tstatszlb[4], Bstatszlb[5] - Bstatszlb[4], Sstatszlb[5] - Sstatszlb[4])
range_values = c(Estatszlb[8] - Estatszlb[5], Tstatszlb[6] - Tstatszlb[3], Bstatszlb[6] - Bstatszlb[3], Sstatszlb[6] - Sstatszlb[3])
volume_values = c(Estatszlb[2], Tstatszlb[2], Bstatszlb[2], Sstatszlb[2])
zlbstats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE=range_values, VOLUME = volume_values)
print(xtable(zlbstats),include.rownames = FALSE)
```

 (see Figure \@ref(fig:EFFR during zero lower bound 3/17/2020-3/16/2022")
\```{r, fig.cap= "EFFR during zero lower bound 3/17/2020-3/16/2022}
sdate<-sdatezlb
maxr=max(zlb[,2:ncol(zlb)]) # drop outlier 54
maxr=maxr+6
minr=min(zlb[,2:ncol(zlb)]) #0

meltrates_zlb <- melt(zlb,id="sdate")
rates_zlb <- ggplot(meltrates_zlb,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_point(shape=16,size=1) +
  labs(x="",  y = "Basis Points (bp)", color = "Rate", shape = "Rate") +  
  scale_y_continuous(breaks = seq(0, 15, by = 5), limits = c(0, 15)) + 
  theme_minimal() + guides(shape = guide_legend(title = "Rate"))
print(rates_zlb)
```
  
The average weighted median for the EFFR during the inflation targeting regime is 368 basis points (pb). The average weighted medians of other money market rates cluster around the median EFFR - TGCR,BGCR and SOFR 362-364 bp. Daily transaction volume in SOFR (billions of dollars ) is around 1200 billion. The money market volumes in TGCR and BGCR around 456to 470  billion.  Daily trade volume in EFFR is a low 100 billion.  The percentiles of rates reflect the same clustering around the EFFR and SOFR as do their medians.

The interquartile range for the money market rates TGCR and BGCR are 3.26-3.46 bp, smaller than the 1.62 bp of the policy rate, EFFR. The SOFR has the largest IQR, some 5.68 bp. The OBFR,designed to track the policy rate. is similar to the EFFR.

The range of the data, the 9th ,minus the first percentile, is more sensitive to outliers,  Some 23 basis points for the EFFR and 21 for SOFR and the money market rates, TGCR, BGCR, 32-33. 

Inflation------------------------------
Table \@ref(tab:inflationtable_characteristics) provides a summary of the data.
\```{r nflationtable_characteristics, tab.cap="Rate characteristics Taming inflation 03/17/2022-12/14/2023", echo=FALSE, results='asis'}
sdate<-sdateinflation
k=5 
begn<-c(1,859,923,1014,1519,1)
endn<-c(858,922,1013,1518,1957,1957)
bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
inflation<-rrbp[bgn:edn,]
sdateinflation<-sdate[bgn:edn]

qinflationE=quantilesE[bgn:edn,]
qinflationT=quantilesT[bgn:edn,]
qinflationB=quantilesB[bgn:edn,]
qinflationS=quantilesS[bgn:edn,]


# Store your data frame in the environment
my_envepisodes$sdateinflation <-sdateinflation
my_envepisodes$inflationE <-qinflationE
my_envepisodes$inflationT <-qinflationT
my_envepisodes$inflationB <-qinflationB
my_envepisodes$inflationS <-qinflationS

# stats_inflationust<-colMeans(quantilesE[bgn_edn,]     #inflationust
Estatsinflation <- colMeans(qinflationE[,2:ncol(qinflationE)], na.rm = TRUE)
#Ostatsinflation <- colMeans(qinflationO[,2:ncol(qinflationO)], na.rm = TRUE)
Tstatsinflation <- colMeans(qinflationT[,2:ncol(qinflationT)], na.rm = TRUE)
Bstatsinflation <- colMeans(qinflationB[,2:ncol(qinflationB)], na.rm = TRUE)
Sstatsinflation <- colMeans(qinflationS[,2:ncol(qinflationS)], na.rm = TRUE)

# Add NA for targets 
Tstatsinflation2<-c(Tstatsinflation[1],Tstatsinflation[2], TargetUe=NA, TargetDe=NA,Tstatsinflation[3],Tstatsinflation[4],Tstatsinflation[5],Tstatsinflation[6])
Bstatsinflation2<-c(Bstatsinflation[1],Bstatsinflation[2], TargetUe=NA, TargetDe=NA,Bstatsinflation[3],Bstatsinflation[4],Bstatsinflation[5],Bstatsinflation[6])
Sstatsinflation2<-c(Sstatsinflation[1],Sstatsinflation[2], TargetUe=NA, TargetDe=NA,Sstatsinflation[3],Sstatsinflation[4],Sstatsinflation[5],Sstatsinflation[6])


# episode characteristics NEW---------------------------------
Estatsinflation_mat <- matrix(Estatsinflation, nrow = 8, ncol = 1)
Tstatsinflation_mat <- matrix(Tstatsinflation2, nrow = 8, ncol = 1)
Bstatsinflation_mat <- matrix(Bstatsinflation2, nrow = 8, ncol = 1)
Sstatsinflation_mat <- matrix(Sstatsinflation2, nrow = 8, ncol = 1)

# Combine the matrices into one matrix
charinflation <- cbind(Estatsinflation_mat, Tstatsinflation_mat, Bstatsinflation_mat, Sstatsinflation_mat)
print(charinflation)
# Set row names
rownames(charinflation) <- c("Rate", "Volume", "Upper target", "Lower target", 
                     "Percentile_01", "Percentile_25", "Percentile_75", "Percentile_99")

colnames(charinflation) <- c("EFFR", "TGCR", "BGCR",  "SOFR")
print(charinflation)
```

Table \@ref(tab:inflationtable_statistics) provides a summary of the data.
\```{r inflationtable_statistics, tab.cap="Rate characteristics Taming inflation 03/17/2022-12/14/2023", 
categories <- c("Estatsinflation", "Tstatsinflation", "Bstatsinflation", "Sstatsinflation")

sdate<-sdateinflation
k=5 
bgn<-begn[k]
edn<-endn[k]
print(bgn)
print(edn)
inflation<-rrbp[bgn:edn,]
sdateinflation<-sdate[bgn:edn]

qinflationE=quantilesE[bgn:edn,]
qinflationT=quantilesT[bgn:edn,]
qinflationB=quantilesB[bgn:edn,]
qinflationS=quantilesS[bgn:edn,]


# Store your data frame in the environment
my_envepisodes$sdateinflation <-sdateinflation
my_envepisodes$inflationE <-qinflationE
my_envepisodes$inflationT <-qinflationT
my_envepisodes$inflationB <-qinflationB
my_envepisodes$inflationS <-qinflationS

# stats_inflationust<-colMeans(quantilesE[bgn_edn,]     #inflationust
Estatsinflation <- colMeans(qinflationE[,2:ncol(qinflationE)], na.rm = TRUE)
#Ostatsinflation <- colMeans(qinflationO[,2:ncol(qinflationO)], na.rm = TRUE)
Tstatsinflation <- colMeans(qinflationT[,2:ncol(qinflationT)], na.rm = TRUE)
Bstatsinflation <- colMeans(qinflationB[,2:ncol(qinflationB)], na.rm = TRUE)
Sstatsinflation <- colMeans(qinflationS[,2:ncol(qinflationS)], na.rm = TRUE)

# Add NA for targets 
Tstatsinflation2<-c(Tstatsinflation[1],Tstatsinflation[2], TargetUe=NA, TargetDe=NA,Tstatsinflation[3],Tstatsinflation[4],Tstatsinflation[5],Tstatsinflation[6])
Bstatsinflation2<-c(Bstatsinflation[1],Bstatsinflation[2], TargetUe=NA, TargetDe=NA,Bstatsinflation[3],Bstatsinflation[4],Bstatsinflation[5],Bstatsinflation[6])
Sstatsinflation2<-c(Sstatsinflation[1],Sstatsinflation[2], TargetUe=NA, TargetDe=NA,Sstatsinflation[3],Sstatsinflation[4],Sstatsinflation[5],Sstatsinflation[6])


IS THIS TO BE A SEPARATE CODECHUNK?
median_values = c(Estatsinflation[1], Tstatsinflation[1], Bstatsinflation[1], Sstatsinflation[1])
iqr_values = c(Estatsinflation[7] - Estatsinflation[6], Tstatsinflation[5] - Tstatsinflation[4], Bstatsinflation[5] - Bstatsinflation[4], Sstatsinflation[5] - Sstatsinflation[4])
range_values = c(Estatsinflation[8] - Estatsinflation[5], Tstatsinflation[6] - Tstatsinflation[3], Bstatsinflation[6] - Bstatsinflation[3], Sstatsinflation[6] - Sstatsinflation[3])
volume_values = c(Estatsinflation[2], Tstatsinflation[2], Bstatsinflation[2], Sstatsinflation[2])
inflationstats <- data.frame(Category = categories, Median = median_values, IQR = iqr_values, RANGE=range_values, VOLUME = volume_values)
print(xtable(inflationstats),include.rownames = FALSE)
```

 (see Figure \@ref(fig:EFFR during Taming inflation 03/17/2022-12/14/2023")
\```{r, fig.cap= "EFFR during Taming inflation 03/17/2022-12/14/2023"}
sdate<-sdateinflation
maxr=max(inflation[,2:ncol(inflation)]) # 539
minr=min(inflation[,2:ncol(inflation)]) #0

meltrates_inflation <- melt(inflation,id="sdate")
rates_inflation <- ggplot(meltrates_inflation,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_point(shape=16,size=1) +
  labs(x="",  y = "Basis Points (bp)", color = "Rate", shape = "Rate") +  
  scale_y_continuous(breaks = seq(0, 500, by = 50), limits = c(0, 500)) + 
  theme_minimal() + guides(shape = guide_legend(title = "Rate"))
print(rates_inflation)
ggsave("C:/Users/Owner/Documents/Research/OvernightRates/Figures/rates_inflation.pdf")
ggsave("C:/Users/Owner/Documents/Research/OvernightRates/Figures/rates_inflation.png")
# Warning Removed 620 rows containing missing values (`geom_point()`) so plot ends in July 2023
```

DECIDE WHETHER TO PRESENT CHAR AND STATS FOR EACH EPISODE ALONE OR JOINTLY AS BELOW
```{r Combine episode dataframes, include=FALSE}
# Extract data frames from my_envepisodes and add them to the global environment
episodesstats <- my_envepisodes$episodesstats
episodeschar <- my_envepisodes$episodeschar


#```{r access_data}
# Access data frames from the global environment
#combined_stats <- rbind(episodesstats)
#combined_char <- rbind(episodeschar)
#OLD
#episodeschar <-rbind(charnorm, charadj,charcovid, charzlb,charinflation)
#episodesstats <-rbind( normstats, adjstats,covidstats, zlbstats,inflationstats) # 

#CHAT
# Define variable names
variable_names <- c("Normalcy", "Adjustment", "Covid", "Zero lower bound", "Inflation")

# Print timestamp
cat("Thu Mar 21 21:23:31 2024\n")

# Print LaTeX table environment
cat("\\begin{table}[ht]\n")
cat("\\centering\n")
cat("\\begin{tabular}{rlrrrr}\n")
cat("  \\hline\n")
cat(" & Category & Median & IQR & RANGE & VOLUME \\\\ \n")
cat("  \\hline\n")

# Print table contents (example)
# Replace this with your actual table content
for (i in 1:nrow(episodesstats)) {
  if (grepl("^EFFR", episodesstats[i, "Category"])) {
    if ((i %% 4) == 1 && i != 1) {
      cat(variable_names[(i - 1) / 4 + 1], "\n")  # Insert variable name every 4 rows above "EFFR"
    }
  }
  cat(episodesstats[i, "Category"], " & ",
      episodesstats[i, "Category"], " & ",
      episodesstats[i, "Median"], " & ",
      episodesstats[i, "IQR"], " & ",
      episodesstats[i, "RANGE"], " & ",
      episodesstats[i, "VOLUME"], " \\\\ \n")
}

# Print end of LaTeX table environment
cat("   \\hline\n")
cat("\\end{tabular}\n")
cat("\\end{table}\n")
```



This works for episodechar table
```{r combined tables} 
# Define variable names
variable_names <- c("EFFR", "TGCR", "BGCR", "SOFR")

# Define labels
labels <- c("Normalcy", "Adjust", "Covid", "ZLB", "Inflation")

# Print timestamp
cat("Thu Mar 21 21:23:31 2024\n")

# Print LaTeX table environment
cat("\\begin{table}[ht]\n")
cat("\\centering\n")
cat("\\begin{tabular}{rlrrrrrrr}\n")
cat("  \\hline\n")
cat(" & Category & ")
for (var_name in variable_names) {
  cat(var_name, " & ")
}
cat("\\\\ \n")
cat("  \\hline\n")

# Print table contents
for (i in 1:nrow(episodeschar)) {
  if ((i - 1) %% 8 == 0 && i != 1) {
    cat(labels[((i - 1) / 8) %% length(labels) + 1], " & ")
  }
  cat("Rate & ")  # Insert 'Rate' label every 8 rows
  for (var_name in variable_names) {
    cat(episodeschar[i, var_name], " & ")
  }
  cat("\\\\ \n")
}

# Print end of LaTeX table environment
cat("   \\hline\n")
cat("\\end{tabular}\n")
cat("\\end{table}\n")
```

<!--
But when I add a text line every 8 rows I get error

 # Define variable names
 variable_names <- c("EFFR", "TGCR", "BGCR", "SOFR")
 
 # Print timestamp
 cat("Thu Mar 21 21:23:31 2024\n")
Thu Mar 21 21:23:31 2024
 
 # Print LaTeX table environment
 cat("\\begin{table}[ht]\n")
\begin{table}[ht]
 cat("\\centering\n")
\centering
 cat("\\begin{tabular}{rlrrrr}\n")
\begin{tabular}{rlrrrr}
 cat("  \\hline\n")
  \hline
 cat(" & Category & Rate & Volume & Upper target & Lower target & Percentile_01 & Percentile_25 & Percentile_75 & Percentile_99\\\\ \n")
 & Category & Rate & Volume & Upper target & Lower target & Percentile_01 & Percentile_25 & Percentile_75 & Percentile_99\\ 
 cat("  \\hline\n")
  \hline
 
 # Print table contents
 for (var_name in variable_names) {
     cat("\\multicolumn{10}{l}{", var_name, "} \\\\ \\hline\n")  # Insert variable name as a header
     for (i in which(episodeschar$Category == var_name)) {
         cat(episodeschar[i, "Category"], " & ",
             episodeschar[i, "Rate"], " & ",
             episodeschar[i, "Volume"], " & ",
             episodeschar[i, "Upper target"], " & ",
             episodeschar[i, "Lower target"], " & ",
             episodeschar[i, "Percentile_01"], " & ",
             episodeschar[i, "Percentile_25"], " & ",
             episodeschar[i, "Percentile_75"], " & ",
             episodeschar[i, "Percentile_99"],  " \\\\ \n")
     }
 }
\multicolumn{10}{l}{ EFFR } \\ \hline
Error in episodeschar$Category : $ operator is invalid for atomic vectors
--> 
 
pisodeschar[i, "Percentile_25"], " & ",
      episodeschar[i, "Percentile_75"], " & ",
      episodeschar[i, "Percentile_99"],  " \\\\ \n")
}

# Print end of LaTeX table environment
cat("   \\hline\n")
cat("\\end{tabular}\n")
cat("\\end{table}\n")

```}

# Define variable names
variable_names <- c("EFFR", "TGCR", "BGCR", "SOFR")

# Print timestamp
cat("Thu Mar 21 21:23:31 2024\n")

# Print LaTeX table environment
cat("\\begin{table}[ht]\n")
cat("\\centering\n")
cat("\\begin{tabular}{rlrrrrrrr}\n")
cat("  \\hline\n")
cat(" & Category & EFFR & TGCR & BGCR & SOFR\\\\ \n")
cat("  \\hline\n")

# Print table contents
for (row_name in rownames(episodeschar)) {
  cat(row_name, " & ")
  for (var_name in variable_names) {
    cat(episodeschar[row_name, var_name], " & ")
  }
  cat("\\\\ \n")
}

# Print end of LaTeX table environment
cat("   \\hline\n")
cat("\\end{tabular}\n")
cat("\\end{table}\n")



Episode plots:------------------------------------------------OLD??

```{r, normalcy, echo=FALSE}
#1. normalcy   3/4/2016		7/31/2019      4  859
ratecol=2
vol=3
targetup = 3
targetdown = 4
p1=6
p1=7
p1=8
p1=9  # correct
my_episode<-my_envepisodes$norm
# dim(norm) 858, 9
# dim(normE) 1957, 9
#my_episode[,rates]  
#my_episode<-my_episode[,ratecol]
max(my_episode[,ratecol]) #245
min(my_episode[,ratecol]) # 26
#WRING
#max(norm[,rates]) #533
#min(norm[,rates]) #4
```


 (see Figure \@ref(fig:EFFR during normalcy period  3/4/2016-7/31/2019-plot)
```{r, fig.cap= "EFFR during normalcy period  3/4/2016-7/31/2019"}
# Assuming 'sdate' is a vector and 'qnormE[,2]' is a data frame with multiple columns
normE<-my_envepisodes$norm
sdate <- normE[, 1]
data_without_sdate <- normE[, -1]

# Create a data frame including 'sdate' and melt it
meltrates_norm <- melt(data.frame(sdate, data_without_sdate), id.vars = "sdate")

# Create the plot
rates_norm <- ggplot(meltrates_norm, aes(x = sdate, y = value, colour = variable, group = variable)) + 
  geom_line() +
  labs(x = "Date", y = "rates:Basis Points (bp), volume $billion", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 350, by = 25), limits = c(0, 350)) + 
  theme_minimal()

# Print the plot
print(rates_norm)
```


```{r, adjust , echo=FALSE}
#2. adjust   8/1/2019 - 10/31/2019 737660  860 - 923
rates=2
vol=3
targetup = 3
targetdown = 4
p1=6
p1=7
p1=8
p1=9
qadjE<-my_envepisodes$adjust
max(qadjE[,rates]) #230
min(qadjE[,rates]) #158
```

#(see Figure \@ref(fig:EFFR during adjustment period  8/1/2019 - 10/31/2019-plot)
```{r, fig.cap= "EFFR during adjustment period  8/1/2019 - 10/31/2019"}
library(ggplot2)

# Assuming 'sdate' is your Date variable
sdate <- as.Date(qadjE[, 1])
data_without_sdate<- qadjE[, -1]

# Create a data frame including 'sdate' and melt it
meltrates_adjust <- melt(data.frame(sdate, data_without_sdate), id.vars = "sdate")

# Convert 'variable' to factor
meltrates_adjust$variable <- as.factor(meltrates_adjust$variable)

# Convert 'value' to numeric
meltrates_adjust$value <- as.numeric(meltrates_adjust$value)

# Create the plot with scale_color_discrete and custom x-axis settings
rates_adjust <- ggplot(meltrates_adjust, aes(x = sdate, y = value, colour = variable, group = variable)) +
  geom_line() +
  labs(x = "Date", y = "rates:Basis Points (bp), volume $billion", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(150, 250, by = 25), limits = c(150, 250)) + 
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 month") +  # Specify date format and breaks
  theme_minimal()
print(rates_adjust)
```


```{r, covid , echo=FALSE}
#3. covid   11/1/2019	    3/16/2020   924  1032
rates=2
vol=3
targetup = 3
targetdown = 4
p1=6
p1=7
p1=8
p1=9
qcovidE<-my_envepisodes$adjust
max(qcovidE[,rates]) #160
min(qcovidE[,rates])  #25
```
(see Figure \@ref(fig:EFFR during covid period 11/1/2019-3/16/2020-plot)
```{r, fig.cap= "EFFR during covid period  11/1/2019-3/16/2020"}
# Assuming 'sdate' is your Date variable
sdate <- as.Date(qcovidE[, 1])
data_without_sdate <- qcovidE[, -1]

# Create a data frame including 'sdate' and melt it
meltrates_covid <- melt(data.frame(sdate, data_without_sdate), id.vars = "sdate")

# Convert 'variable' to factor
meltrates_covid$variable <- as.factor(meltrates_covid$variable)

# Convert 'value' to numeric
meltrates_covid$value <- as.numeric(meltrates_covid$value)

# Create the plot with scale_color_discrete and custom x-axis settings
rates_covid <- ggplot(meltrates_covid, aes(x = sdate, y = value, colour = variable, group = variable)) +
  geom_line() +
  labs(x = "Date", y = "rates:Basis Points (bp), volume $billion", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 175, by = 25), limits = c(0, 175)) + 
  scale_x_date(date_labels = "%Y-%m-%d", date_breaks = "1 month") +  # Specify date format and breaks
  theme_minimal()
print(rates_covid)
```


```{r, zlb , echo=FALSE}
#4. zlb    3/17/2020- 3/16/2022     1032-1516
rates=2
vol=3
targetup = 3
targetdown = 4
p1=6
p1=7
p1=8
p1=9
zlbE<-my_envepisodes$zlb
max(zlbE[,rates]) #25
min(zlbE[,rates]) # 4
```

(see Figure \@ref(fig:EFFR during zero lower bound (Zlb) period 3/17/2020-3/16/2022
```{r, fig.cap= "EFFR during zero lower bound (zlb) period 3/17/2020-3/16/2022"}
sdate <- qzlbE[, 1]
data_without_sdate <- qzlbE[, -1]

# Create a data frame including 'sdate' and melt it
meltrates_zlb<- melt(data.frame(sdate, data_without_sdate), id.vars = "sdate")

#
rates_zlb <- ggplot(meltrates_zlb,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date", y = "rates:Basis Points (bp), volume $billion", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 15, by = 5), limits = c(0, 15)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(rates_zlb)
```

```{r, inflation , echo=FALSE}
#5. inflation  03/17/2022 - 12/14/2023 1517-1960  Correct with new end point
rates=2
vol=3
targetup = 3
targetdown = 4
p1=6
p1=7
p1=8
p1=9
qinflationE<-my_envepisodes$inflationE
max(qinflationE[,rates]) #533
min(qinflationE[,rates])  #33
```
(see Figure \@ref(fig:EFFR during inflation period 3/17/2022-12/14/2023-plot)
```{r , fig.cap= "EFFR during inflation period 03/17/2022-12/14/2023"}
sdate <- qinflationE[, 1]
data_without_sdate <- qinflationE[, -1]

# Create a data frame including 'sdate' and melt it
meltrates_inflation <- melt(data.frame(sdate, data_without_sdate), id.vars = "sdate")

rates_inflation <- ggplot(meltrates_inflation,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date", y = "rates:Basis Points (bp), volume $billion", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 500, by = 50), limits = c(0, 500)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(rates_inflation)
```

Quantiles, are these necessary or can be included in plots above ----------------------
```{r ,  EFFR and percentiles, echo=FALSE, fig.cap="Overnight rates EFFR percentiles Normalcy"}  
normE2 = subset(normE, select = -c(VolumeEFFR, Percentile01_EFFR, Percentile99_EFFR) ) 
quantEnorm <- melt(normE2,id="sdate")
quantE_norm <- ggplot(quantEnorm ,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date",  y = "Basis Points (bp)", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 350, by = 25), limits = c(0, 350)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(quantE_norm)
```

```{r , echo=FALSE, fig.cap="Overnight rates EFFR percentiles adjustment period"}
qadjE2 = subset(qadjE, select = -c(VolumeEFFR, Percentile01_EFFR, Percentile99_EFFR) ) 
quantEadj <- melt(qadjE2,id="sdate")
quantE_adj <- ggplot(quantEadj ,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date",  y = "Basis Points (bp)", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(125, 250, by = 25), limits = c(125, 250)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(quantE_adj)
```

```{r, EFFR percentiles covid, echo=FALSE, fig.cap="Overnight rates EFFR percentiles during covid"}
qcovidE2 = subset(qcovidE, select = -c(VolumeEFFR, Percentile01_EFFR, Percentile99_EFFR) ) 
quantEcovid <- melt(qcovidE2,id="sdate")
quantE_covid <- ggplot(quantEcovid ,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date",  y = "Basis Points (bp)", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 200, by = 25), limits = c(0, 200)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(quantE_covid)
```

```{r, EFFR percentiles zlb, echo=FALSE, fig.cap="Overnight rates EFFR percentiles zero lower bound"}
qzlbE2 = subset(qzlbE, select = -c(VolumeEFFR, Percentile01_EFFR, Percentile99_EFFR) ) 
quantEzlb <- melt(qzlbE2,id="sdate")
quantE_zlb <- ggplot(quantEzlb ,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date",  y = "Basis Points (bp)", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 15, by = 5), limits = c(0, 15)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(quantE_zlb)
```

```{r, EFFR percentiles inflation, echo=FALSE, fig.cap="Overnight rates EFFR percentiles inflation"}
qinflationE2 = subset(qinflationE, select = -c(VolumeEFFR, Percentile01_EFFR, Percentile99_EFFR) ) 
quantEinflation <- melt(qinflationE2,id="sdate")
quantE_inflation <- ggplot(quantEinflation ,aes(x=sdate,y=value,colour=variable,group=variable)) + 
  geom_line() +
  labs(x="Date",  y = "Basis Points (bp)", color = "Rate", linetype = "Rate") +  
  scale_y_continuous(breaks = seq(0, 350, by = 25), limits = c(0, 350)) + 
  theme_minimal()
#theme(axis.title.x=element_blank(), axis.text.x= element_text(size=8,vjust =.5),plot.tag = element_text(size=8,hjust = 0),plot.tag.position = c(0.5,-.01))
print(quantE_inflation)
```

### Volatile rates during episodes
   

```{r, vol measures, echo=FALSE}
my_color_palette <- c("EFFR" = "darkblue", "OBFR" = "darkgreen", "TGCR" = "darkred", "BGCR" = "darkcyan", "SOFR" = "darkorange")
```

```{r, plots norm log percent change, echo=FALSE}
# Norm episode
df <- as.data.frame(norm_measure1)
bgn<-begn[1]
edn<-endn[1]-1
df$sdate <- sdate[bgn:edn]

# Reshape the data to long format
df_long <- gather(df, key = "Variable", value = "Value", -sdate)

# Create a scatter plot using ggplot2
ggplot(df_long, aes(x = sdate, y = Value, color = Variable)) +
  geom_point() +
  labs(title = "Log percent change in rates during normalcy period 2016-2023",
       x = "Date",
       y = "Value",
       color = "Variable") +
  theme_minimal()
```
```{r, plots adjust log percent change, echo=FALSE}
df <- as.data.frame(adjust_measure1)
bgn<-begn[2]
edn<-endn[2]-1
df$sdate <- sdate[bgn:edn]
```

```{r, plots covid log percent change, echo=FALSE}
df <- as.data.frame(covid_measure1)
bgn<-begn[3]
edn<-endn[3]-1
df$sdate <- sdate[bgn:edn]
```

```{r, plots zlb log percent change, echo=FALSE}
df <- as.data.frame(zlb_measure1)
bgn<-begn[4]
edn<-endn[4]-1
df$sdate <- sdate[bgn:edn]
```

```{r, plots inflation log percent change, echo=FALSE}
df <- as.data.frame(inflation_measure1)
bgn<-begn[5]
edn<-endn[5]-1
df$sdate <- sdate[bgn:edn]
```

[FINISH OR DELETE]

 
# Dispersion, clustering
Dispersion measures hypothesis 2
Bertolini et al () note the tendency of all rates to respond revert quickly to the policy rate FFR after to shocks from pilicy changes in the FFR, IOR, (and events?). For the objective of this paper, to understand Fed tolerance for volatility of the FFR, I examine relation between volatility, disperion among key overnight tates. and any deviations of the FFR within its target range. A related question to address with the Fed's preference for offsetting volatility when managing the FFR within FOMC targets.

Duffie and Krishnamurthy () and Alonso provide evidence that there exists significant dispersion of the money market rates from the policy rate that suggest problems for transmission of FONC to the broader economy. The effectiveness of monetary policy to transmit the desired stance of monetary policy to financial markets and the real economy economy depends on efficient transmission of rate changes to other short rates in wholesale and consumer funding market. 

Duffie and Krishnamurthy () and Gara Afonso, Kyungmin Kim, Antoine Martin, Ed Nosal, Simon Potter, and Sam Schulhofer-Wohl () provide direct evidence of of dispersion among overnight policy and wholesale money market rates from the policy rate and deviationof the FFR from the FOMC target range.  

Duffie and Krishnamurthy warn dispersion in rates hampers the transmission of FONC policy to the broader economy.  Dispersion across money market interest rates is a primary indicator of the level of efficiency in how other key short-term interest rates reflect Fed policy changes. They argue that the current setting of U.S.-dollar money markets limits the transmission of Fed policy to the economy.


## The Duffie Krishnamurthy dispersion index
The Duffie Krishnamurthy () index of dispersion of overnight rate,  the weighted mean absolute deviation of the cross-sectional adjusted rates. The measure of dispersion of overnight rates is the absolute deviation of the cross rates of overnight reference and money market rates. This index indicates the level of passthrough inefficiency, the passthrough effectiveness of the Federal Reserve’s monetary policy, the transmission of policy to the economy through other short term rates.
\cite{DuffieKrishnamurthi} 

Their index of rate dispersion $DK_t$ for day t in U.S. short-term money markets is the weighted mean absolute deviation of the cross-sectional distribution of overnight-equivalent rates on that day. $DK_t$ is constructed from overnight equivalent rates $\hat{r}_{i,t}$. Rates are adjusted for term and credit spreads after adjusting for premia associated with credit risk and term structure.frictions associated with imperfect #competition, regulation, infrastructure, and other forms of institutional segmentation within money markets. 

Duffie Krishnamurthy dispersion index $D_t$ at day t is the weighted mean absolute deviation of the cross-sectional adjusted rates distribution on day t  (Figure ~\ref{figure:dispersionDK}) (Table %~\ref{table:dispersionDK}) ). Rates $y_{i,t(m)}$ denote the rate at time t on instrument i, maturing in m days. Since our rate data are daily, I drop the maturity m, so that rates are $y_{t,i}$. First adjust the rate to remove term-structure effects, obtaining the associated "overnight-equivalent” rate as
$\hat{y_{i,t}} = y_{i,t(m)} − (OISt(m) − OISt(1)), (4.1)$ is the volume-weighted mean rate.
$v_{i,t} is the estimated outstanding amount of this instrument on day t (dollars).The Duffie and Krishnamurthy index of dispersion of overnight rate is the weighted mean absolute deviation of the cross-sectional adjusted rates.Dispersion of rates imply rates follow different distributions.

[CORRECT EQUATION]
 
 \begin{align*}
 nrates<-ncol(rrbp)
 D_t =  \frac{1}{v_{i,t}} \left( \sum_{i=1}^{nrates} \times abs{\hat{y}_{i,t}− \bar{y}_t} \right)(4.2)
 \bar{y_t} = \left(\frac{\sum_{i}^{n} v_{i,t} \times \cdot \hat{y}_{i,t}} {\sum_{i}^{n} v_{i,t}}
 \right)
 \end{align*}

<!--
```{r, Duffie Krishnamurthy dispersion code,  echo=FALSE}
ncol<-ncol(rrbp) # 5 if sdate ow 4 (and no OBFR)
T <-nrow(rrbp)
vtot<-0 ;
dk <- matrix(0, nrow = T, ncol = ncol)
dkindex <- numeric(T)
mrate<-0 
meanr<-0 
print(dim(vold))
# Initialize vectors to store results
meanr<- colMeans(rrbp[,2:ncol])
# EFFR  OBFR  TGCR  BGCR  SOFR 
colsumvold <- rowSums(vold[, 1:4], na.rm = TRUE)
  T<-nrow(rrbp)
  # Calculate dk(t)
  for (t in 1:T) {
  for (i in 1:ncol-1) {
    #dk[t, i] <- (1 / colsumvold[t]) * vold[t,i]*abs(rrbp[t,i] - meanr[i])
  }  # if sdate is in rrbp and vold
}
  # Calculate dkindex(t)
  for (t in 1:T) {
  dkindex[t] <- sum(dk[t, 1:ncol-1])
  }

# Convert the "sdate" column to a Date format
```
```{r, dkindexdf,echo=FALSE}
dkindexdf <- data.frame(
     sdate = spread_no_na$sdate,
     dkindex,
     TargetDe = spread_no_na$TargetDe,
     TargetUe = spread_no_na$TargetUe,
     EFFR = spread_no_na$EFFR,
     #OBFR = spread_no_na$OBFR,
     TGCR = spread_no_na$TGCR,
     BGCR = spread_no_na$BGCR,
     OFR = spread_no_na$SOFR
 )
str(dkindexdf)
```


```{r, Duffie Krishnamurthy index plot, echo=FALSE}
# Load the dplyr package
library(dplyr)
dkindexdf$group <- rep(c("Duffie-Krishnamurthy index", "Lower target FFR", "Upper target FFR", "EFFR", "TGCR", "BGCR", "SOFR"), length.out = nrow(dkindexdf))

# Create a named vector for colors
colors <- c("Duffie-Krishnamurthy index" = "black",
            "Lower target FFR" = "blue",
            "Upper target FFR" = "green",
            "EFFR" = "black",
            "TGCR" = "green",
            "BGCR" = "orange",
            "SOFR" = "red")

ggdkindex <- ggplot(dkindexdf, aes(x = sdate, y = dkindex, color = group)) +
  geom_point(shape = 16, size = 1) +
  labs(x = "", y = "basis points", color = "Lines") +
  scale_color_manual(name = "Legend Title", values = colors) +
  theme_minimal()
print(ggdkindex)
```
--> 

Their results show dispersion ranged between 4 and 7 basis points between the end of 2012 and December 17, 2015. Dispersion increased by over 10 basis points, immediately after the first passthrough event in the current Fed monetary policy setting, when the interest rate paid by the Fed on excess reserves (IOER) was increased from 25 to 50 basis basis points on December 17, 2015.

In our sample from 2016 through 2022, the Duffie Krishnamurthy diversity index registers low dispersion, under 4 basis points, among the five overnight rates  EFFR, OBFR, TGCR, BGCR, and SOHR during 2016 and the first quarter of 2017 before jumping to around 5 during the last 2 weeks of March 2017 
%(Figure \ref{fig:dispersionDK}.

Their measure of dispersion rises to over 50 basis points during December 2018 to mid April 2019. The index triples to 151.14 basis on April 19, 2019 and remains at this high level, reaching a high 254.83 points during the spike in repo rates September 19, 2019. The index tapers off after of the dash for cash on March 17, 2020 after spiking 27.5 basis points. Throughout 2020 and 2021, the index was under 1 point. The level of dispersion rose steadily during March 2022, reaching over 200 basis points during the last part of December 2022 as the FOMC raised the target range for the  Federal Funds rate to fight inflation. The index rose  from 14.57 April 15, 2022 to 209.25 on December 29, 2022.

[add Dispersion 2023]

1. ?? The dispersion index of the five daily rates from 2016 to 2023
#(Figure \ref{figures:})


<!--

Table 1  WHAT IS THIS? WHERE?
provides the results of a time-series regression of dispersion on various explanatory
variables: the Markit CDX index (as a measure of crisis severity); the target federal
funds rate (which, in the post-2008 period, is set at the middle of the Fed’s target
range for FFER); an end-of-quarter dummy; and a post-2014 dummy. The first of
these dummies captures the effect of the quarter-end window-dressing of bank balance
sheets, described in Section 6. The second dummy picks up the change in dispersion
since 2014, after which several significant new bank regu
-->


## Gara Afonso, Kyungmin Kim, Antoine Martin, Ed Nosal, Simon Potter, and Sam Schulhofer-Wohl () measure istance of the Federal Funds rates from FOMC targets
Gara Afonso, Kyungmin Kim, Antoine Martin, Ed Nosal, Simon Potter, and Sam Schulhofer-Wohl () propose  dispersion index  $D_t$  as a measure of the difference of value weighted daily EFFR from the actual policy target, the upper and lower target rates specified by the FOMC (the EFFR, the effective fed funds rate published by the Federal Reserve Bank of New York). (Figure).

The daily value of $D_t$ ,the deviations between the value weighted daily fed funds rate and weighted fed funds rate, the actual policy target (the EFFR, the effective fed funds rate published by the Federal Reserve Bank of New York) FOMC  (Figure), During  2016-2022 managing the policy rate remained within its target range (\ref{fig:dispersionEffrGara}). ADD OBSERVATIONS

They Calculate $D_t$  based on the conditions that 
- the actual rate is not higher than the upper FOMC target 
   If FFR > upper target TU, $D_t = rho_bar_t - rho_max_t$

- the actual rate is not lower than the lower FOMC target 
FFR < lower target TD $D_t <- rho_bar_t - rho_min_t$

Otherwise $D_t =0$.


\begin{equation*}
T <-nrow(rrbp)
rho_bar_t<- 0
rho_max_t<- 0
rho_min_t<- 0
D<- 0
      if (rho_max_t < rho_bar_t) {
        D_t <- rho_bar_t - rho_max_t
      } else if (rho_bar_t < rho_min_t) {
        D_t <- rho_bar_t - rho_min_t
      }
\end{equation*}



<!--
This index has a negative bias, relative to dispersion across all actual money-market
transactions rates, because it does not incorporate the heterogeneity of individual transaction rates within each segment of money markets.
Figure 4.2 shows the variation of the dispersion index Dt over the sample period. As
shown, dispersion increases in the run-up to the covid pandemic, punctuated by the major disturbance in repo markets and the March 2020 dash for cash events, peaking in March 2020. Dispersion
decreased [when]  but leveled off to a higher value during the zero lower bound period but lower that the covid period. The index rises with the successive  Fed rate increases to tame inflation

3. If the Fed implicitly responds to volatility in the FFR through information on wholesale money market rates, our model of volatility of the EFFR should include thise index





```{r, Gara index, echo-FALSE}
# Initialize 'g' vector
T=nrow(rrbp)
g <- array(0,T)

# Loop through 't' values

for (t in 1:T) {
  if (!is.na(spread_no_na$TargetUe[t]) && !is.na(rrbp[t, 2])) {
    if (spread_no_na$TargetUe[t] < rrbp[t, 2]) {
      # Upper target
      g[t] <- rrbp[t, 2] - spread_no_na$TargetUe[t]
    } else if (!is.na(spread_no_na$TargetDe[t]) && rrbp[t, 2] < spread_no_na$TargetDe[t]) {
      # Lower target
      g[t] <- rrbp[t, 3] - spread_no_na$TargetDe[t]
    } else {
      # Handle other cases (if neither condition is met)
      # You can assign a default value to 'g[t]' or handle it as needed
    }
  } else {
    # Handle cases where 'spread$TargetUe_EFFR[t]' or 'rrbp[t, 2]' is NA
    # You can assign a default value to 'g[t]' or handle it as needed
  }
}
```



```{r,gara index data frame, echo=FALSE}
garaindexdf <- data.frame(
  sdate = spread_no_na$sdate,
  g,
  TargetDe = spread_no_na$TargetDe,
  TargetUe = spread_no_na$TargetUe,
  EFFR = spread_no_na$EFFR,
  OBFR = spread_no_na$OBFR,
  TGCR = spread_no_na$TGCR,
  BGCR = spread_no_na$BGCR,
  SOFR = spread_no_na$SOFR
)
```

```{r, galonso index plot, echo=FALSE}
garaindexdf$sdate <- as.Date(garaindexdf$sdate)

# Create a variable for group names
garaindexdf$group <- rep(c("Alonso index", "Lower target FFR", "Upper target FFR", "EFFR", "TGCR", "BGCR", "SOFR"), length.out = nrow(dkindexdf))

# Create a named vector for colors
colors <- c("Alonso index" = "black",
            "Lower target FFR" = "blue",
            "Upper target FFR" = "green",
            "EFFR" = "black",
            "TGCR" = "green",
            "BGCR" = "orange",
            "SOFR" = "red")

ggara <- ggplot(dkindexdf, aes(x = sdate, y = dkindex, color = group)) +
  geom_point(shape = 16, size = 1) +
  labs(x = "Date", y = "Basis points (bp)", color = "Lines") +
  scale_color_manual(name = "Legend Title", values = colors) +
  theme_minimal()

print(ggara)
```

(Figure ~\ref{figure:dispersionGara})   (Table %~\ref{table:dispersionGara}) ) 

The daily value of $D_t$, the deviations between the value weighted daily 
fed funds rate (FFR) and the FOMC target, show the EFFR remained within the target ranges from 2017-2023.
FOMC targets were not established until [2018?]


The EFFR stays within the bounds of the FOMC target rates (left panel of Figure ~\ref{igure:}) and 50 percent of the data, the data within the 25th and 75th percentile (right panel)


The SOFR stays within the bounds of the FOMC target rates (left panel of Figure ~\ref{igure:}) and the 25th and 75th percentile (right panel)


## Target rates and quintiles, BOXPLOTS EFFR, SOFR
 The behavior of the EFFR and SOFR offer another view of dispersion. Boxplots here? 
Box plots, quintile plots?

```{r, boxplot data episode norm, echo=FALSE}
#Access the data frame stored in the environment
box_data <- my_environmentbox$norm
str(box_data)
```

```{r, fig.cap= "Daily rates normalcy 2016-2019", echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.pos='H', eval=TRUE, cache=FALSE, dev='png', out.width='80%', results='hide'}
box_data <- my_environmentbox$norm
str(box_data)

my_envepisodes$sdatenorm <- sdatenorm
my_envepisodes$normE <- normE
my_envepisodes$normT <- normT
my_envepisodes$normB <- normB
my_envepisodes$normS <- normS

sdatenorm<-my_envepisodes$sdatenorm 
Estatsnorm <- colMeans(normE[bgn:edn,2:ncol(qnormE)], na.rm = TRUE)
#Ostatsnorm <- colMeans(normO[bgn:edn,,2:ncol(qnormO)], na.rm = TRUE)
Tstatsnorm <- colMeans(qnormT[bgn:edn,2:ncol(qnormT)], na.rm = TRUE)
Bstatsnorm <- colMeans(qnormB[bgn:edn,2:ncol(qnormB)], na.rm = TRUE)
Sstatsnorm <- colMeans(qnormS[bgn:edn,2:ncol(qnormS)], na.rm = TRUE)

boxplot_normalcy <- data.frame(
  Group = structure(1:4, .Label = c("EFFR", "TGCR", "BGCR", "SOFR"), class = "factor"),
  y0 = c(Estatsnorm[5], Tstatsnorm[3], Bstatsnorm[3], Sstatsnorm[3]),
  y25 = c(Estatsnorm[6], Tstatsnorm[4], Bstatsnorm[4], Sstatsnorm[4]),
  y50 = c(Estatsnorm[1], Tstatsnorm[1] , Bstatsnorm[1] , Sstatsnorm[1]) ,
  y75 = c(Estatsnorm[7], Tstatsnorm[5], Bstatsnorm[5], Sstatsnorm[5]),
  y100 = c(Estatsnorm[8], Tstatsnorm[6], Bstatsnorm[6], Sstatsnorm[6]))


bnormalcy <- ggplot(boxplot_normalcy, aes(x = Group,
                                          ymin = y0,
                                          ymax = y100,
                                          lower = y25,
                                          middle = y50,
                                          upper = y75,
                                          fill = Group)) +
             geom_boxplot(stat = "identity")

print(bnormalcy)
```


  
```{r, boxplot data episode adjust, echo=FALSE}
#Access the data frame stored in the environment
box_data <- my_environmentbox$adjust
str(box_data)
```
  
#```{r, adjust episode quantile dataframes, echo=FALSE,  cache=FALSE,results='hide'}
```{r, fig.cap= "Daily rates adjust 2019", echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=8, eval=TRUE, cache=FALSE, dev='png', out.width='80%', , results='hide', fig.pos='H'} 
   boxplot_adjust<- data.frame(
    Group = structure(1:4, .Label = c("EFFR", "TGCR", "BGCR", "SOFR"), class = "factor"),
    y0 = c(Estatsadj[5], Tstatsadj[3], Bstatsadj[3], Sstatsadj[3]),
    y25 = c(Estatsadj[6], Tstatsadj[4], Bstatsadj[4], Sstatsadj[4]),
    y50 = c(Estatsadj[1], Tstatsadj[1] , Bstatsadj[1] , Sstatsadj[1]) ,
    y75 = c(Estatsadj[7], Tstatsadj[5], Bstatsadj[5], Sstatsadj[5]),
    y100 = c(Estatsadj[8], Tstatsadj[6], Bstatsadj[6], Sstatsadj[6]))

badjust<-ggplot(boxplot_adjust , aes(x=Group,
                                           ymin = y0,
                                           ymax = y100,
                                           lower = y25,
                                           middle = y50,
                                           upper = y75,
                                           fill = Group)) +
    geom_boxplot(stat = "identity")   
  print(badjust)
```
```{r, boxplot data episode covid, echo=FALSE}
#Access the data frame stored in the environment
box_data <- my_environmentbox$covid
str(box_data)
``` 
  
#```{r, covid episode quantile dataframes, echo=FALSE,  cache=FALSE,results='hide'}
```{r, fig.cap= "Daily rates covid 2016-2019", echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=8, eval=TRUE, cache=FALSE, dev='png', out.width='80%', , results='hide', fig.pos='H'} 
  boxplot_covid <- data.frame(
    Group = structure(1:4, .Label = c("EFFR", "TGCR", "BGCR", "SOFR"), class = "factor"),
    y0 = c(Estatscovid[5], Tstatscovid[3], Bstatscovid[3], Sstatscovid[3]),
    y25 = c(Estatscovid[6], Tstatscovid[4], Bstatscovid[4], Sstatscovid[4]),
    y50 = c(Estatscovid[1], Tstatscovid[1] , Bstatscovid[1] , Sstatscovid[1]) ,
    y75 = c(Estatscovid[7], Tstatscovid[5], Bstatscovid[5], Sstatscovid[5]),
    y100 = c(Estatscovid[8], Tstatscovid[6], Bstatscovid[6], Sstatscovid[6]))

bcovid<-ggplot(boxplot_covid, aes(x=Group,
                                           ymin = y0,
                                           ymax = y100,
                                           lower = y25,
                                           middle = y50,
                                           upper = y75,
                                           fill = Group)) +
    geom_boxplot(stat = "identity")   
  print(bcovid)
```
```{r, boxplot data episode zlb, echo=FALSE}
#Access the data frame stored in the environment
box_data <- my_environmentbox$zlb
str(box_data)
```
 
#```{r, zlb episode quantile dataframes, echo=FALSE,  cache=FALSE,results='hide'}
```{r, fig.cap= "Daily rates zlb 2016-2019", echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=8, eval=TRUE, cache=FALSE, dev='png', out.width='80%', , results='hide', fig.pos='H'} 

  boxplot_zlb <- data.frame(
    Group = structure(1:4, .Label = c("EFFR", "TGCR", "BGCR", "SOFR"), class = "factor"),
    y0 = c(Estatszlb[5], Tstatszlb[3], Bstatszlb[3], Sstatszlb[3]),
    y25 = c(Estatszlb[6], Tstatszlb[4], Bstatszlb[4], Sstatszlb[4]),
    y50 = c(Estatszlb[1], Tstatszlb[1] , Bstatszlb[1] , Sstatszlb[1]) ,
    y75 = c(Estatszlb[7], Tstatszlb[5], Bstatszlb[5], Sstatszlb[5]),
    y100 = c(Estatszlb[8], Tstatszlb[6], Bstatszlb[6], Sstatszlb[6]))

 bzlb<-ggplot(boxplot_zlb , aes(x=Group,
                                           ymin = y0,
                                           ymax = y100,
                                           lower = y25,
                                           middle = y50,
                                           upper = y75,
                                           fill = Group)) +
    geom_boxplot(stat = "identity")   
  print(bzlb)
```
```{r, boxplot data episode inflation, echo=FALSE}
#Access the data frame stored in the environment
box_data <- my_environmentbox$inflation
str(box_data)
```
  
#```{r, inflation episode quantile dataframes, echo=FALSE,  cache=FALSE,results='hide'}
```{r, fig.cap= "Daily rates nflation 2022-2023", echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=8, eval=TRUE, cache=FALSE, dev='png', out.width='80%', , results='hide', fig.pos='H'}
 boxplot_inflation <- data.frame(
  Group = structure(1:4, .Label = c("EFFR", "TGCR", "BGCR", "SOFR"), class = "factor"),
  y0 = c(Estatsinflation[5], Tstatsinflation[3], Bstatsinflation[3], Sstatsinflation[3]),
  y25 = c(Estatsinflation[6], Tstatsinflation[4], Bstatsinflation[4], Sstatsinflation[4]),
  y50 = c(Estatsinflation[1], Tstatsinflation[1] , Bstatsinflation[1] , Sstatsinflation[1]) ,
  y75 = c(Estatsinflation[7], Tstatsinflation[5], Bstatsinflation[5], Sstatsinflation[5]),
  y100 = c(Estatsinflation[8], Tstatsinflation[6], Bstatsinflation[6], Sstatsinflation[6]))

binflation<-ggplot(boxplot_inflation , aes(x=Group,
                                       ymin = y0,
                                       ymax = y100,
                                       lower = y25,
                                       middle = y50,
                                       upper = y75,
                                       fill = Group)) +
    geom_boxplot(stat = "identity")   
  print(binflation)
```

# Transaction volumes
More volatile than rates.

The increases in SOFR volumes dwarf the EFFR  increased 068  %268 in 2019 and 878 in 2022. SOFR volumes fell 682 in 2020 and 292 in 2021. Daily SOFR trading volumes, $\$1.1$ trillion
dwarf the Federal funds market, $\$62.5$ billion.
(2020? Gibbs CME webinar on repo).


# Reserves
Total reserves in the U. S. banking system now exceed  $\$$3.081237 trillion (Totresns). At the beginning of 2009, reserves were $\$$800 billion, compared to approximately $\$$10 billion pre-crisis. Reserves continued to increase until late 2014. Fed’s quantitative easing programs raised reserves to  a peak of $\$$2.8 trillion in September 2014, reaching $\$$4.275 at then end of 2021 In November 2022 through late 2017, reserves started to decline,  reaching a low of $\$$1.4 trillion in early September 2019 (Table \ref{table:weeklyreserves}, Figure \ref{fig:weeklyreserves}) under the Federal Open Market Committee policy of “balance sheet normalization” aggregate reserves, 

During the initial years of the sample from 3/02/2016 until 2023 reserves were around $\$$200 million until 9/28/2016 when reserves fell to $\$$180 million. Then returned to $\$$200 million on 2/15/2017 until6/24/2018, reaching a low of $\$$160 million on 1/4/2017 (Table ~\ref{table:statsweeklyratesannual}).

<!--
From 7/28/2017 to 6/24/2018, volume weighted median overnight money market rates are higher and normalized reserves are low,  0.19 or $\$$190 milli
on  the week of 7/26/2017 (Figure \ref{figure:vmrates}). All rates reach a maximum at the end of June or first week of July 2018. The EFFR and OBFR reach their maximum value 179.56  and 387.74 basis points respectively during the week 6/24/2018.  The money market rates TGCR, BGCR, and SOFR reach their maxima the following  week 7/1/2018,  Reserves per deposits fell to 0.16 or $\$$160 million on 6/27/2018. All rates then decline, reaching their lowest level 4/5/2020 just as reserves begin to increase to 0.18 or or $\$$180 million . The EFFR reaches a low of 1.74 basis points. Reserves increase from  4/5/2020 to 9/15/2021, 0.24 or $\$$240 million then begin to decline.  From 3/13/2022 to the present, all rates rise steeply with the successive increases in the Fed policy rate response to inflation. The EFFR rises on 3/13/2022 from 1,74 to	27.36 basis points. Reserves decline slightly to 0.17 or $\$$170 million at the end of the sample 12/29/2022 (revise data)
-->

The reasons for changes in the level of reserves in the banking system include:
1) the Federal Reserve trades securities with banks in order  to manage the Federal Funds rate (FFR) within the FOMC target range. These temporaty operations are the focus of this paper.  Purchases through repurchase agreements increase the size of the Federal Reserve balance sheet. Sales through reverse repurchase agreements decrease reserves.
Through temporary open market operations (TOMO),The Federal Reserve changes the level of aggregate reserves short-term through repurchase and reverse repurchase agreements iThe trading desk at the NY Fed conducts all repo and reverse repo operations, including small value exercises. https://www.newyorkfed.org/markets/desk-operations/. 


2)The US Treasury transfers funds between reserves and non-reserve accounts at the FederalReserve
example \url{https://www.moneyandbanking.com/commentary/2020/11/8/the-case-of-the-treasury-account-at-the-federal-reserve}
\ref{figure} Figure \@ref(fig:FedLiabilties).

These repo operations temporarily add or drain reserves available to the banking system and influence day-to-day trading in the Federal Funds market. The Federal Reserve responds to volatility in the federal funds market by adjusting the reserve supply to keep the federal funds rate within its target range through repo and reverse repo operations each day In a repurchase or repo transaction, the Federal Reserve purchases securities to banks. Aggregate reserves increase when the Fed credits the Federal Reserve accounts of the selling banks.

In a repo transaction, the Desk purchases securities from a counterparty subject to an agreement
to resell the securities at a later date. Each repo transaction is economically similar to a loan collateralized by securities, and temporarily increases the supply of reserve balances in the banking system. The repo rate. interest” paid on that loan, is the difference between the original price and the
second, higher price. In a reverse repo transaction, the Desk sells securities to a counterparty subject to an agreement to repurchase the securities at a later date. Reverse repo transactions temporarily reduce the supply of reserve balances in the banking system.

\url{https://libertystreeteconomics.newyorkfed.org/2022/01/how-the-feds-overnight-reverse-repo-facility-works/}
When the Federal Reserve lends to counter parties in a reverse repo transaction, the Fed sale of securities lower aggregate reserves as funds are debited from depositor accounts. These operations are quick and are put in place within a matter of days. For example, in September 2019, the federal funds rate spiked up on September 16 and 17. On September 18-21 ... and dash for cash March 2020. Counterparties that lend to the Fed include primary dealers, hedge funds, insurance firms, money markets and financial firms with large pools of cash . CHECK


```{r, Fed liabilities, echo=FALSE}
#recreate from FRED 
#Composition of Federal Reserve liabilities (Percent of total), 2007-2020
#Note: All data are for the annual averages of weekly data, except for 2020, which is for end-October. #Source: FRED and Federal Reserve H.4.1.
```

Reserves also can change when the US Treasury transfers funds between reserves and non-reserve accounts at the Federal Reserve. The Treasury keeps an account at the Fed  or deposits in Treasury Tax and Loan (TT&L) accounts at commercial banks to provide a buffer to avoid payment issues. A Treasury deposit reduces reserves in the banking system. A decrease in government deposits adds reserves.Prior to 2008, when reserve levels were very low, a change of $1 billion could move interbank lending rates substantially. To avoid unpleasant surprises, the Treasury and the Fed agreed to target an account level of $5 billion at the end of every business day. If the government thought they would miss, officials would alert the Fed staff in the morning so that they could use open-market operations to adjust. 

When the target was $5 billion and the Fed’s liabilities were a relatively modest $750 billion, Treasury deposits accounted for less than 1 percent of the total. In the decade starting in 2008, Treasury balances rose to around $400 billion, accounting for an average of 5% of Fed liabilities. Since June 2020, Treasury deposits at the Fed now account 23 percent of total liabilities. 


[Reverse repo facility]
Counterparties that lend to the Fed include money markets and dealers. 

The FOMC planned reduction of holdings of securities,’balance sheet normalization’, began October 2017, by holding them to maturity, rather than by selling them back in financial markets. This reduction of reserves engendered a disruption in repurchase (repo) markets in September 2019. To resolve this crisis, In October 2019, the Fed resumed purchasing massive amounts of debt securities.

The quantitative easing asset purchases, QE1, QE2, and QE3, challenged the Federal Reserve bank’s ability to control short-term interest rates. As a result, the Fed and other central banks changed their overall frameworks for controlling short-term interest rates. Pre-crisis, these frameworks were typically based on adjusting the scarcity value of a limited supply of central bank deposits (reserves), but the substantial increase in liquidity resulting from asset purchases in response to the crisis made other techniques necessary.

<!--
(Hypothesis 1 continued see rmd 7/35/2023
With weekly data on reserves, rates are higher when reserves are low, and the converse when reserves are high or 'ample' (Tables \ref{tab;e:weeklyreserves and \ref{table:statsweeklyratesannual}}). Daily reserve data are available only to Federal Reserve staff, NYFed staff recommended this analysis use  the weekly series of reserves Reserve Balances with Federal Reserve Banks: Week Average (WRESBAL) $\$S$ billions normalized by deposits of commercial banks (DPSACBW027SBOG) $\$S$ billions to control for the growth of the banking industry. Both weekly series ending on Wednesday.
-->

<!--
DUPLICATE for review purposes
The level of reserves in the banking system can change either because
1) the Federal Reserve buys or sells assets from banks, which changes the size of the Federal Reserve
balance sheet
2) because funds are transferred between reserves and non-reserve accounts at the Federal
3) The Federal Reserve responds to volatility in the federal funds market by adjusting the reserve
supply to keep the federal funds rate within its target range through repo and reverse repo operations each day
The Federal Reserve changes the level of aggregate reserves by trading securities with banks.
Federal Reserve purchases add assets to its balance sheet and issues reserves by crediting the Federal

Temporary open market operations (TOMO), short-term repurchase and reverse repurchase agreements,
support the Fed’s policy objectives for the desired range of the effective Federal Funds rate.
Short-term repurchase and reverse repurchase agreements are designed to temporarily add or drain
reserves available to the banking system and influence day-to-day trading in the Federal Funds market.

Reserve accounts of the selling banks. (reverse repo, Fed is lender?)
In a repo transaction, the Federal Reserve sells securities to banks. Aggregate reserves decline when the Fed debits the Federal Reserve accounts of the selling banks. (Fed repo as borrower)?


In a repo transaction, the Desk purchases securities from a counterparty subject to an agreement to resell the securities at a later date, temporarily increasing the supply of reserve balances in the banking system and lowering the EFFR.

In a repo transaction, the Desk purchases securities from a counterparty subject to an agreement
to resell the securities at a later date. Each repo transaction is economically similar to a loan collateralized
by securities, and temporarily increases the supply of reserve balances in the banking system.
Conversely, in a reverse repo transaction, the Desk sells securities to a counterparty subject to an
agreement to repurchase the securities at a later date. Reverse repo transactions temporarily reduce the
supply of reserve balances in the banking system.
The repo rate. interest” paid on that loan., is the difference between the original price and the
second, higher price, Participants include


When the Federal Reserve lends to counter parties in a reverse repo transaction, the Fed sale of securities lower aggregate reserves as funds are debited from depositor accounts. These operations are quick and are put in place within a matter of days. For example, in September 2019, the federal funds rate spiked up on September 16 and 17. On September 18-21 ... and dash for cash March 2020. Counterparties that lend to the Fed include primary dealers, hedge funds, insurance firms, money markets and financial firms with large pools of cash . CHECK

In a repo transaction, the Federal Reserve sells securities to banks. Aggregate reserves decline when the Fed debits the Federal Reserve accounts of the selling banks. (Fed repo as borrower)?
The trading desk at the NY Fed dpnducts all repo and reverse repo operations, including small value exercises. https://www.newyorkfed.org/markets/desk-operations/

In a repo transaction, the Desk purchases securities from a counterparty subject to an agreement to resell the securities at a later date, temporarily increasing the supply of reserve balances in the banking system and lowering the EFFR.

Each repo transaction is economically similar to a loan collateralized by securities,interest” paid on that loan, is the difference between the original price and the second, higher price. In a reverse repo transaction, the Desk sells securities to a counterparty subject to an agreement to repurchase the securities at a later date.  temporarily decreases the supply of reserve balances in the banking system and raising the FFR
-->




